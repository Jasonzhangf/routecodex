# rcc-llmswitch-core（RouteCodex LLM Switch 核心）

本包是 RouteCodex V2 的“工具治理唯一入口 + 协议转换核心”，实现严格的“前半段/后半段”双向流水线：

- 前半段（Front-Half，Conversion）：按端点最小映射为 OpenAI Chat 标准形状（不做工具治理、不兜底）。
- 后半段（Back-Half，Chat Pipeline）：统一工具治理、参数修复、MCP 两步暴露、流/非流一致化与最终响应组装。

遵循 AGENTS.md 的 9 大架构原则：职责单一、最小兼容、Fail Fast、无兜底、配置驱动、统一出口/入口。

版本提示：从 0.2.95 起，Responses→Chat 的映射严格“只做形状转换”，不再注入兜底文本或工具；所有治理只在后半段进行。

## 总览

```
入站请求（任意端点）
  ├─ Chat (/v1/chat)        ┐
  ├─ Responses (/v1/responses) ├─ 前半段 Conversion → 规范化为 Chat 标准 JSON（非流）
  └─ Anthropic (/v1/messages) ┘   - 仅做字段/形状映射；不做工具治理/文本收割
                                     - 统一关闭上游直通，前半段需要时将 SSE 合成为非流 JSON

                ▼
         Chat Back-Half（唯一治理入口）
           - 工具治理（canonicalize/repair/去重/ID配对）
           - MCP 两步暴露（列表→读取/模板）
           - reasoning/think 标准化（按端点策略保留或过滤）
           - finalize：确保 tool_calls / tool role、finish_reason、content 形状一致

                ▼
          Provider（HTTP 通信，仅转发）

                ▼
         出站响应（统一从 Chat 反向映射）
           - Chat：直接输出标准 OpenAI Chat 形状
           - Responses：从 Chat 还原 required_action / output / items
           - Anthropic：映射为 Anthropic 支持的形状
```

要点：
- “后半段唯一治理点”：三端（Chat/Responses/Anthropic）最终都走同一套 Chat 后半段。
- “前半段最小映射”：只做协议字段/工具形状转换，不做文本工具收割/兜底/治理。
- “流式一致”：默认不上游直通（upstream SSE OFF），前半段将流合成为非流 JSON，再走后半段，保证一致。

## 流水线节点与职能（入站→出站）

1) HTTP Server 入站（端点路由）
- 接收 /v1/chat、/v1/responses、/v1/messages 的原始请求体
- 写入 http-request 快照（可选：.parsed 摘要）

2) 前半段 Conversion（本包 v2/conversion/...）
- Chat：校验/轻量规范（保持 OpenAI Chat 标准）
- Responses：instructions + input 映射为 Chat.messages；function_call/tool_result → assistant.tool_calls / role='tool'（只形状，不治理）
- Anthropic：Claude 消息/工具映射为 OpenAI Chat（仅形状）
- SSE：如入站为 SSE，先在前半段合成为非流 JSON（默认），确保后续统一路径

3) 后半段 Chat Pipeline（本包 v2/conversion/openai-chat/...）
- request-shape：
  - 统一 messages.content 为 string
  - 删除不支持字段（如 stream）
- request-tools-stage：
  - canonicalizeChatResponseTools：不变式（content=null，finish_reason=tool_calls）
  - JSON/JSON5 风格参数修复，失败回退 "{}"
  - 工具 ID 生成/去重
  - MCP 两步暴露（仅在后半段）
- provider 调用（Provider 层）：HTTP 转发 + 快照；不做工具处理
- response-shape：
  - 统一 Chat 响应形状、finish_reason、content
  - reasoning/think 清理或保留（按端点策略）
- response-tools-stage：
  - 工具结果配对（role='tool' 与 tool_calls.id 对齐）
  - Responses 反向桥接（仅映射，不治理）：required_action + items/output 还原

4) 出站响应
- Chat：OpenAI Chat
- Responses：OpenAI Responses
- Anthropic：Anthropic Messages

## 三端前半段：具体滤波器与映射（伪代码）

### Chat 前半段
```
function chatFrontHalf(payload):
  assert(Array.isArray(payload.messages))
  drop(payload.stream)          // 统一非流
  ensureOpenAIChatShape(payload)
  return payload
```

### Responses 前半段（严格不兜底）
```
function responsesFrontHalf(payload):
  ctx = captureResponsesContext(payload)
  tools = normalizeTools(payload.tools)
  msgs = []
  if ctx.instructions: msgs.push({role:'system', content:trim(ctx.instructions)})

  for entry in payload.input:
    switch(entry.type):
      case 'function_call'|'tool_call':
        name = entry.name || entry.function?.name
        args = parseArguments(entry.arguments || entry.function?.arguments)
        msgs.push({role:'assistant', tool_calls:[{id:genId(entry), type:'function', function:{name, arguments:stringify(args)}}]})
      case 'function_call_output'|'tool_result'|'tool_message':
        id = entry.tool_call_id || entry.call_id || entry.tool_use_id || entry.id
        out = normalizeToolOutput(entry.output)
        msgs.push({role:'tool', tool_call_id:id, content:String(out ?? '')})
      default:
        // 优先 entry.message.content[]
        if entry.message?.content: text = collectText(entry.message.content)
        else if entry.content:      text = collectText(entry.content)
        else if entry.text:         text = entry.text
        if text: msgs.push({role:normalizeRole(entry.role), content:text})

  return { model, messages: msgs, tools: tools, tool_choice: payload.tool_choice }
```

注意：不注入“伪 user”，不做文本工具收割/治理，严格只做形状转换。

### Anthropic 前半段
```
function anthropicFrontHalf(payload):
  // Claude → OpenAI Chat 映射
  for m in payload.messages:
    map role/parts → Chat message
  map tools → OpenAI function tools
  drop(stream)
  return chatPayload
```

## 后半段：主要处理点与职责

- request-shape（v2/conversion/openai-chat/request-shape.ts）
  - 统一 messages.content 为 string
  - 删除不支持字段（如 stream）
- request-tools-stage（v2/conversion/openai-chat/request-tools-stage.ts）
  - canonicalizeChatResponseTools：不变式（content=null，finish_reason=tool_calls）
  - jsonish.repairArgumentsToString：把任意形态 arguments 修复为安全 JSON 字符串
  - 工具 ID 生成/去重
  - MCP 两步暴露（仅在后半段）
- response-shape（v2/conversion/openai-chat/response-shape.ts）
  - Chat 响应标准化；think/推理文本按端点策略保留/清理
- response-tools-stage（v2/conversion/openai-chat/response-tools-stage.ts）
  - role='tool' 与 tool_calls.id 配对
  - Responses 反向桥接（仅映射，不治理）：required_action + items/output 还原

## 流式（SSE）策略
- 默认不上游直通（provider 配置未显式允许时）。
- 前半段把 SSE 合成为非流 JSON；后半段统一处理，再需要时用本包 streaming 模块合成 Responses SSE。

## 快照与排错
- 快照目录：`~/.routecodex/codex-samples/<endpoint>/`
  - `*_http-request.json` / `*_http-request.parsed.json`
  - `*_pipeline.llmswitch.request.post.json`（进入后半段前的 Chat 形状）
  - `*_pipeline.provider.request.pre.json`（上游请求体，顶层仅 Chat 字段）
- 常见 1214 根因：
  - 缺失用户消息（Responses 输入未包含 user 文本，且前半段不兜底）
  - 顶层出现 data/metadata/stream 等额外键（应移除“添加逻辑”，而非末端清理）

## 设计原则与边界
- 工具治理唯一在后半段；前半段绝不进行文本工具收割/参数修复
- 兼容层只做 provider 特定最小映射；Provider 只做 HTTP 通信
- Fail Fast：形状不合规直接报错，不做隐藏兜底

## 版本与构建
- 构建：`npm run build`
- 打包：`npm pack`
- 发布：`npm publish`
