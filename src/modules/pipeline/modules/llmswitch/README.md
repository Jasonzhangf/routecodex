# LLMSwitch æ¨¡å—

LLMSwitch æ¨¡å—æä¾›å¤šåè®®è½¬æ¢åŠŸèƒ½ï¼Œå°†ä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹APIåè®®è¿›è¡Œç›¸äº’è½¬æ¢ï¼Œæ”¯æŒ OpenAIã€Anthropicã€Responses ç­‰å¤šç§åè®®æ ¼å¼ã€‚

## ğŸ¯ æ¨¡å—æ¦‚è¿°

LLMSwitch æ¨¡å—æ˜¯æµæ°´çº¿æ¶æ„çš„ç¬¬ 1 å±‚ï¼ˆåè®®è½¬æ¢å±‚ï¼‰ï¼Œè´Ÿè´£å¤„ç†è¿›å…¥æµæ°´çº¿çš„ç¬¬ä¸€ä¸ªåè®®è½¬æ¢æ­¥éª¤ã€‚å®ƒåˆ†æä¼ å…¥è¯·æ±‚çš„åè®®ç±»å‹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç›®æ ‡ä¾›åº”å•†æ‰€æœŸæœ›çš„åè®®æ ¼å¼ã€‚

## ğŸ”„ æ”¯æŒçš„åè®®è½¬æ¢

### ğŸ”§ OpenAI è§„èŒƒåŒ–è½¬æ¢å™¨
- **å®ç°æ–‡ä»¶**: `openai-normalizer.ts` / `llmswitch-openai-openai.ts`
- **åŠŸèƒ½**: OpenAI åè®®è§„èŒƒåŒ–ï¼Œä¿æŒè¯·æ±‚ç»“æ„ä¸€è‡´
- **ç‰¹æ€§**:
  - å®Œæ•´çš„ OpenAI åè®®æ”¯æŒ
  - è¯·æ±‚/å“åº”å…ƒæ•°æ®æ·»åŠ 
  - æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•ä¿¡æ¯
  - åè®®éªŒè¯å’Œæ ‡å‡†åŒ–
  - é”™è¯¯ä¸Šä¸‹æ–‡å¢å¼º

### ğŸ¤– Anthropic-OpenAI åŒå‘è½¬æ¢å™¨
- **å®ç°æ–‡ä»¶**: `llmswitch-anthropic-openai.ts`
- **åŠŸèƒ½**: Anthropic åè®®ä¸ OpenAI åè®®äº’è½¬
- **ç‰¹æ€§**:
  - æ¶ˆæ¯æ ¼å¼è½¬æ¢
  - å·¥å…·è°ƒç”¨é€‚é…
  - æµå¼å“åº”å¤„ç†
  - æ¨ç†å†…å®¹å¤„ç†
  - å“åº”æ ¼å¼æ ‡å‡†åŒ–

### ğŸ†• Responses-Chat è½¬æ¢å™¨
- **å®ç°æ–‡ä»¶**: `llmswitch-response-chat.ts`
- **åŠŸèƒ½**: OpenAI Responses API ä¸ Chat Completions API äº’è½¬
- **ç‰¹æ€§**:
  - **åŒå‘è½¬æ¢**: Responses â†” Chat æ ¼å¼å®Œå…¨æ”¯æŒ
  - **å·¥å…·è°ƒç”¨**: å®Œæ•´çš„å·¥å…·è°ƒç”¨æ ¼å¼è½¬æ¢
  - **æµå¼äº‹ä»¶**: æ”¯æŒ Responses API çš„æ‰€æœ‰ SSE äº‹ä»¶
  - **å…ƒæ•°æ®ä¿æŒ**: ä¿ç•™åŸå§‹è¯·æ±‚ä¸Šä¸‹æ–‡å’Œåè®®ä¿¡æ¯
  - **æ™ºèƒ½å¤„ç†**: è‡ªåŠ¨å¤„ç† reasoningã€function_call ç­‰ç‰¹æ®Šå†…å®¹

### ğŸ”„ ç»Ÿä¸€åè®®è½¬æ¢å™¨
- **å®ç°æ–‡ä»¶**: `llmswitch-unified.ts`
- **åŠŸèƒ½**: å¤šåè®®æ™ºèƒ½è½¬æ¢å’Œè·¯ç”±
- **ç‰¹æ€§**:
  - è‡ªåŠ¨åè®®æ£€æµ‹
  - æ™ºèƒ½è½¬æ¢ç­–ç•¥é€‰æ‹©
  - å¤šåè®®æ”¯æŒ
  - ç»Ÿä¸€é”™è¯¯å¤„ç†

## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½

### ğŸ“Š åè®®æ£€æµ‹ä¸è·¯ç”±
```typescript
// è‡ªåŠ¨åè®®æ£€æµ‹
private detectProtocol(request: any): 'openai' | 'anthropic' | 'responses' | 'unknown' {
  if (request.input && Array.isArray(request.input)) {
    return 'responses';
  } else if (request.messages) {
    return 'openai';
  } else if (this.hasAnthropicFormat(request)) {
    return 'anthropic';
  }
  return 'unknown';
}
```

### ğŸ”„ Responses è½¬æ¢ç¤ºä¾‹
```typescript
// Responses â†’ Chat è½¬æ¢
const responsesToChat = new ResponsesToChatLLMSwitch(config, dependencies);

// è¾“å…¥ï¼šResponses API æ ¼å¼
const responsesRequest = {
  model: 'gpt-4',
  instructions: 'You are a helpful assistant.',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Hello!' }]
    }
  ],
  tools: [/* å·¥å…·å®šä¹‰ */],
  stream: true
};

// è¾“å‡ºï¼šChat Completions æ ¼å¼
const chatRequest = await responsesToChat.processIncoming(responsesRequest);
// {
//   model: 'gpt-4',
//   messages: [
//     { role: 'system', content: 'You are a helpful assistant.' },
//     { role: 'user', content: 'Hello!' }
//   ],
//   tools: [/* è½¬æ¢åçš„å·¥å…·å®šä¹‰ */],
//   stream: true,
//   _metadata: {
//     switchType: 'llmswitch-response-chat',
//     entryProtocol: 'responses',
//     targetProtocol: 'openai'
//   }
// }
```

### ğŸ“‹ å…ƒæ•°æ®å¢å¼º
```typescript
// è¯·æ±‚å…ƒæ•°æ®æå–
private extractRequestMetadata(request: any, protocol: string): Record<string, any> {
  return {
    timestamp: Date.now(),
    protocol,
    entryProtocol: protocol,
    targetProtocol: this.getTargetProtocol(protocol),
    hasModel: !!request.model,
    hasTools: !!request.tools,
    hasStream: !!request.stream,
    messageCount: this.getMessageCount(request),
    toolCount: request.tools?.length || 0,
    requestType: this.inferRequestType(request, protocol)
  };
}
```

### ğŸ›¡ï¸ åè®®éªŒè¯
```typescript
// åè®®éªŒè¯
private validateProtocol(request: any, protocol: string): void {
  switch (protocol) {
    case 'openai':
      this.validateOpenAIProtocol(request);
      break;
    case 'anthropic':
      this.validateAnthropicProtocol(request);
      break;
    case 'responses':
      this.validateResponsesProtocol(request);
      break;
    default:
      throw new Error(`Unsupported protocol: ${protocol}`);
  }
}
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/modules/pipeline/modules/llmswitch/
â”œâ”€â”€ openai-normalizer.ts              # OpenAI è§„èŒƒåŒ–å®ç°
â”œâ”€â”€ llmswitch-openai-openai.ts        # OpenAI â†’ OpenAI è½¬æ¢å™¨
â”œâ”€â”€ llmswitch-anthropic-openai.ts    # Anthropic â†” OpenAI è½¬æ¢å™¨
â”œâ”€â”€ llmswitch-response-chat.ts        # Responses â†” Chat è½¬æ¢å™¨ â­
â”œâ”€â”€ llmswitch-unified.ts              # ç»Ÿä¸€åè®®è½¬æ¢å™¨
â”œâ”€â”€ anthropic-openai-converter.ts    # Anthropic è½¬æ¢å™¨å·¥å…·
â”œâ”€â”€ anthropic-openai-config.ts        # Anthropic è½¬æ¢é…ç½®
â””â”€â”€ README.md                         # æœ¬æ–‡æ¡£
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### Responses API è½¬æ¢
```typescript
import { ResponsesToChatLLMSwitch } from './llmswitch-response-chat.js';

const responsesSwitch = new ResponsesToChatLLMSwitch({
  type: 'llmswitch-response-chat',
  config: {
    enableValidation: true,
    enableMetadata: true,
    preserveReasoning: true
  }
}, dependencies);

await responsesSwitch.initialize();

// å¤„ç† Responses API è¯·æ±‚
const chatRequest = await responsesSwitch.processIncoming({
  model: 'gpt-4',
  instructions: 'You are a helpful assistant.',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Calculate 15 * 25' }]
    }
  ],
  tools: [
    {
      type: 'function',
      name: 'calculate',
      description: 'Perform mathematical calculations',
      parameters: {
        type: 'object',
        properties: {
          expression: { type: 'string' }
        }
      }
    }
  ]
});
```

### åœ¨æµæ°´çº¿é…ç½®ä¸­ä½¿ç”¨
```typescript
const pipelineConfig = {
  modules: {
    llmSwitch: {
      type: 'llmswitch-response-chat',  // Responses æ”¯æŒ
      config: {
        enableValidation: true,
        enablePerformanceTracking: true,
        preserveOriginalContext: true
      }
    }
  }
};
```

### é…ç½®ç¤ºä¾‹
```json
{
  "virtualrouter": {
    "inputProtocol": "responses",
    "outputProtocol": "openai",
    "providers": {
      "lmstudio": {
        "type": "lmstudio",
        "baseURL": "http://localhost:1234",
        "apiKey": "your-api-key",
        "models": {
          "gpt-4": {
            "compatibility": {
              "type": "responses-chat-switch"
            }
          }
        }
      }
    },
    "routing": {
      "default": ["lmstudio.gpt-4"]
    }
  }
}
```

## âš™ï¸ é…ç½®é€‰é¡¹

### Responses-Chat è½¬æ¢é…ç½®
```typescript
interface ResponsesChatConfig {
  enableValidation?: boolean;           // å¯ç”¨åè®®éªŒè¯
  enableMetadata?: boolean;             // å¯ç”¨å…ƒæ•°æ®å¢å¼º
  preserveReasoning?: boolean;          // ä¿ç•™æ¨ç†å†…å®¹
  enableToolMapping?: boolean;          // å¯ç”¨å·¥å…·æ˜ å°„
  maxLogEntries?: number;               // æœ€å¤§æ—¥å¿—æ¡ç›®æ•°
  streamingChunkSize?: number;          // æµå¼å“åº”å—å¤§å°
}
```

### OpenAI é€ä¼ é…ç½®
```typescript
interface OpenAIPassthroughConfig {
  enableValidation?: boolean;           // å¯ç”¨åè®®éªŒè¯
  enableMetadata?: boolean;             // å¯ç”¨å…ƒæ•°æ®å¢å¼º
  enablePerformanceTracking?: boolean;  // å¯ç”¨æ€§èƒ½è·Ÿè¸ª
  maxLogEntries?: number;               // æœ€å¤§æ—¥å¿—æ¡ç›®æ•°
}
```

### Anthropic-OpenAI è½¬æ¢é…ç½®
```typescript
interface AnthropicOpenAIConfig {
  direction: 'anthropic-to-openai' | 'openai-to-anthropic';
  enableTools?: boolean;                 // å¯ç”¨å·¥å…·è½¬æ¢
  enableStreaming?: boolean;             // å¯ç”¨æµå¼è½¬æ¢
  preserveReasoning?: boolean;           // ä¿ç•™æ¨ç†å†…å®¹
  modelMappings?: Record<string, string>; // æ¨¡å‹æ˜ å°„
}
```

## ğŸ”„ æ”¯æŒçš„è½¬æ¢æ˜ å°„

### Responses â†” Chat è½¬æ¢
| Responses å­—æ®µ | Chat å­—æ®µ | è¯´æ˜ |
|----------------|------------|------|
| `instructions` | `messages[0].content` (system role) | ç³»ç»ŸæŒ‡ä»¤ |
| `input[].content[]` | `messages[].content` | æ¶ˆæ¯å†…å®¹ |
| `tools[]` | `tools[]` | å·¥å…·å®šä¹‰ |
| `tool_choice` | `tool_choice` | å·¥å…·é€‰æ‹© |
| `max_output_tokens` | `max_tokens` | æœ€å¤§ä»¤ç‰Œæ•° |
| `stream` | `stream` | æµå¼æ§åˆ¶ |

### å·¥å…·è°ƒç”¨è½¬æ¢
```typescript
// Responses æ ¼å¼å·¥å…·è°ƒç”¨
{
  "type": "function_call",
  "name": "calculate",
  "arguments": "{\"expression\":\"15*25\"}",
  "call_id": "call_123"
}

// è½¬æ¢ä¸º Chat æ ¼å¼
{
  "tool_calls": [{
    "id": "call_123",
    "type": "function",
    "function": {
      "name": "calculate",
      "arguments": "{\"expression\":\"15*25\"}"
    }
  }]
}
```

## ğŸ›ï¸ è¯·æ±‚ç±»å‹æ¨æ–­

### æ”¯æŒçš„è¯·æ±‚ç±»å‹
```typescript
type RequestType =
  | 'chat'           // èŠå¤©å®Œæˆ (OpenAI)
  | 'messages'       // æ¶ˆæ¯æ ¼å¼ (Anthropic)
  | 'responses'      // Responses API
  | 'completion'     // æ–‡æœ¬å®Œæˆ
  | 'embedding'      // æ–‡æœ¬åµŒå…¥
  | 'tool'           // å·¥å…·è°ƒç”¨
  | 'unknown';       // æœªçŸ¥ç±»å‹
```

### åè®®è‡ªåŠ¨æ£€æµ‹
```typescript
private detectProtocol(request: any): ProtocolType {
  // Responses API æ£€æµ‹
  if (request.input && Array.isArray(request.input)) {
    return 'responses';
  }

  // OpenAI Chat æ£€æµ‹
  if (request.messages && Array.isArray(request.messages)) {
    return 'openai';
  }

  // Anthropic Messages æ£€æµ‹
  if (this.hasAnthropicFormat(request)) {
    return 'anthropic';
  }

  return 'unknown';
}
```

## ğŸ“Š æ€§èƒ½è·Ÿè¸ª

### æ€§èƒ½å…ƒæ•°æ®
```typescript
private addPerformanceMetadata(data: any, operation: string): any {
  return {
    ...data,
    _performance: {
      ...(data._performance || {}),
      [operation]: {
        timestamp: Date.now(),
        operation,
        moduleId: this.id,
        protocol: data._metadata?.originalProtocol
      }
    }
  };
}
```

### è½¬æ¢æ€§èƒ½ç›‘æ§
```typescript
// è½¬æ¢æ€§èƒ½ç»Ÿè®¡
const conversionStats = {
  conversionTime: endTime - startTime,
  inputSize: JSON.stringify(request).length,
  outputSize: JSON.stringify(transformed).length,
  protocol: detectedProtocol,
  hasTools: !!request.tools,
  messageCount: this.getMessageCount(request)
};
```

## ğŸš¨ é”™è¯¯å¤„ç†

### åè®®éªŒè¯é”™è¯¯
```typescript
// Responses API éªŒè¯
if (protocol === 'responses') {
  if (!request.input || !Array.isArray(request.input)) {
    throw new Error('Invalid Responses protocol: input must be an array');
  }
}

// å·¥å…·æ ¼å¼éªŒè¯
if (request.tools && !this.validateTools(request.tools)) {
  throw new Error('Invalid tool format in request');
}
```

### è½¬æ¢é”™è¯¯å¤„ç†
```typescript
// è½¬æ¢é”™è¯¯è®°å½•å’Œæ¢å¤
try {
  const transformed = await this.transformRequest(request, protocol);
} catch (error) {
  this.logger.logModule(this.id, 'transform-error', {
    error: error instanceof Error ? error.message : String(error),
    protocol,
    requestType: this.inferRequestType(request, protocol)
  });

  // å°è¯•é™çº§å¤„ç†
  return this.handleTransformError(request, error);
}
```

## ğŸ” è°ƒè¯•æ”¯æŒ

### è¯¦ç»†æ—¥å¿—è®°å½•
```typescript
// è¯·æ±‚è½¬æ¢æ—¥å¿—
this.logger.logTransformation(this.id, 'responses-to-chat', request, transformed);

// å“åº”è½¬æ¢æ—¥å¿—
this.logger.logTransformation(this.id, 'chat-to-responses', response, converted);

// æµå¼äº‹ä»¶æ—¥å¿—
this.logger.logModule(this.id, 'stream-event', {
  eventType: event.type,
  itemId: event.data.item_id,
  sequenceNumber: event.data.sequence_number
});
```

### è°ƒè¯•ä¿¡æ¯
```typescript
// å®Œæ•´çš„è°ƒè¯•ä¸Šä¸‹æ–‡
const debugInfo = {
  sessionId: request._metadata?.sessionId,
  moduleId: this.id,
  operationId: 'llmswitch_transform',
  timestamp: Date.now(),
  type: 'transform',
  position: 'middle',
  data: {
    original: request,
    transformed: transformed,
    metadata: transformed._metadata,
    protocol: detectedProtocol,
    conversionStats
  }
};
```

## ğŸŒ API ç«¯ç‚¹æ”¯æŒ

### æ”¯æŒçš„ç«¯ç‚¹
- **`/v1/chat/completions`** - OpenAI Chat Completions API
- **`/v1/responses`** - OpenAI Responses API â­
- **`/v1/messages`** - Anthropic Messages API
- **`/v1/completions`** - OpenAI Completions API

### ç«¯ç‚¹æ˜ å°„
```typescript
const endpointMappings = {
  '/v1/responses': {
    entryProtocol: 'responses',
    switchType: 'llmswitch-response-chat',
    targetProtocol: 'openai'
  },
  '/v1/chat/completions': {
    entryProtocol: 'openai',
    switchType: 'llmswitch-openai-openai',
    targetProtocol: 'openai'
  },
  '/v1/messages': {
    entryProtocol: 'anthropic',
    switchType: 'llmswitch-anthropic-openai',
    targetProtocol: 'openai'
  }
};
```

## ğŸ”§ æ‰©å±•æ€§

### æ·»åŠ æ–°çš„ LLMSwitch å®ç°
```typescript
class NewProtocolLLMSwitch implements LLMSwitchModule {
  readonly type = 'llmswitch-new-protocol';
  readonly protocol = 'new-protocol';

  async processIncoming(request: any): Promise<any> {
    const context = this.captureRequestContext(request);
    const transformed = this.transformRequest(request, context);

    return {
      ...transformed,
      _metadata: {
        switchType: this.type,
        timestamp: Date.now(),
        entryProtocol: this.protocol,
        targetProtocol: 'openai',
        ...context
      }
    };
  }

  async processOutgoing(response: any): Promise<any> {
    const context = this.extractResponseContext(response);
    return this.transformResponse(response, context);
  }
}
```

### è‡ªå®šä¹‰åè®®è½¬æ¢å™¨
```typescript
class CustomProtocolConverter {
  async convertRequest(request: any, targetProtocol: string): Promise<any> {
    // è‡ªå®šä¹‰è¯·æ±‚è½¬æ¢é€»è¾‘
    switch (targetProtocol) {
      case 'openai':
        return this.convertToOpenAI(request);
      case 'anthropic':
        return this.convertToAnthropic(request);
      case 'responses':
        return this.convertToResponses(request);
      default:
        throw new Error(`Unsupported target protocol: ${targetProtocol}`);
    }
  }
}
```

## ğŸ“ˆ ç‰ˆæœ¬ä¿¡æ¯

- **å½“å‰ç‰ˆæœ¬**: 2.0.0
- **æ–°å¢ç‰¹æ€§**: Responses API å®Œæ•´æ”¯æŒ
- **å…¼å®¹æ€§**: RouteCodex Pipeline >= 2.0.0
- **TypeScript**: >= 5.0.0
- **Node.js**: >= 18.0.0

## ğŸ”— ä¾èµ–å…³ç³»

- **rcc-debugcenter**: è°ƒè¯•ä¸­å¿ƒé›†æˆ
- **PipelineDebugLogger**: æ¨¡å—æ—¥å¿—è®°å½•
- **ErrorHandlingCenter**: é”™è¯¯å¤„ç†é›†æˆ
- **DebugEventBus**: äº‹ä»¶æ€»çº¿é€šä¿¡
- **BaseModule**: åŸºç¡€æ¨¡å—æ¥å£

## âœ¨ æ–°ç‰¹æ€§ (v2.0.0)

### ğŸ†• Responses API æ”¯æŒ
- å®Œæ•´çš„ Responses â†” Chat æ ¼å¼è½¬æ¢
- æ”¯æŒæ‰€æœ‰ Responses API å­—æ®µå’ŒåŠŸèƒ½
- å·¥å…·è°ƒç”¨å®Œæ•´æ”¯æŒ
- æµå¼äº‹ä»¶å¤„ç†

### ğŸ”§ å¢å¼ºçš„åè®®æ£€æµ‹
- è‡ªåŠ¨æ£€æµ‹è¾“å…¥åè®®ç±»å‹
- æ™ºèƒ½è½¬æ¢ç­–ç•¥é€‰æ‹©
- é”™è¯¯æ¢å¤æœºåˆ¶

### ğŸ“Š æ”¹è¿›çš„è°ƒè¯•åŠŸèƒ½
- è¯¦ç»†çš„è½¬æ¢æ—¥å¿—
- æ€§èƒ½ç»Ÿè®¡
- åè®®è½¬æ¢å¯è§†åŒ–

## ğŸš€ æ›´æ–°æ—¥å¿—

### v2.0.0 (2025-10-17)
- âœ¨ æ–°å¢ `llmswitch-response-chat` è½¬æ¢å™¨
- ğŸ”„ å®Œæ•´çš„ Responses API æ”¯æŒ
- ğŸ“Š æ”¹è¿›çš„æ€§èƒ½è·Ÿè¸ªå’Œè°ƒè¯•åŠŸèƒ½
- ğŸ›¡ï¸ å¢å¼ºçš„åè®®éªŒè¯å’Œé”™è¯¯å¤„ç†
- ğŸ“š å®Œæ•´çš„æ–‡æ¡£æ›´æ–°

### v1.5.0 (2025-01-22)
- ğŸ”§ å®Œå–„ Anthropic-OpenAI è½¬æ¢
- ğŸ“Š æ–°å¢æ€§èƒ½è·Ÿè¸ªåŠŸèƒ½
- ğŸ›¡ï¸ æ”¹è¿›é”™è¯¯å¤„ç†æœºåˆ¶
- ğŸ“š å®Œå–„æ–‡æ¡£å’Œè°ƒè¯•æ”¯æŒè¯´æ˜

## ğŸ”® æœªæ¥è®¡åˆ’

### v2.1.0 è®¡åˆ’
- ğŸ¤– Google Gemini åè®®æ”¯æŒ
- ğŸ”„ å®æ—¶æµå¼åè®®è½¬æ¢
- ğŸ“Š åè®®è½¬æ¢æ€§èƒ½ä¼˜åŒ–
- ğŸ§ª æ›´å¤šçš„åè®®æµ‹è¯•è¦†ç›–

### é•¿æœŸè§„åˆ’
- ğŸŒ æ›´å¤šåè®®æ”¯æŒ (Cohere, Mistral ç­‰)
- ğŸ”„ åè®®ç‰ˆæœ¬ç®¡ç†
- ğŸ§  æ™ºèƒ½åè®®è½¬æ¢ç­–ç•¥
- ğŸ“Š åè®®è½¬æ¢åˆ†æå’ŒæŠ¥å‘Š

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹è°ƒè¯•æ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯
2. æ£€æŸ¥åè®®æ ¼å¼æ˜¯å¦ç¬¦åˆè§„èŒƒ
3. éªŒè¯é…ç½®æ–‡ä»¶è®¾ç½®
4. å‚è€ƒæœ¬æ–‡æ¡£çš„ä½¿ç”¨ç¤ºä¾‹

---

**æœ€åæ›´æ–°**: 2025-10-17 - æ–°å¢ Responses API æ”¯æŒæ–‡æ¡£