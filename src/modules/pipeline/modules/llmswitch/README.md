# LLMSwitch æ¨¡å—

LLMSwitch æ¨¡å—æä¾›åè®®è½¬æ¢åŠŸèƒ½ï¼Œå°†ä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹APIåè®®è¿›è¡Œç›¸äº’è½¬æ¢ï¼Œæ”¯æŒ OpenAIã€Anthropic Claudeã€Responses API ç­‰å¤šç§åè®®çš„åŒå‘è½¬æ¢ã€‚

## ğŸ¯ æ¨¡å—æ¦‚è¿°

LLMSwitch æ¨¡å—æ˜¯æµæ°´çº¿æ¶æ„çš„ç¬¬ 1 å±‚ï¼ˆåè®®è½¬æ¢å±‚ï¼‰ï¼Œè´Ÿè´£å¤„ç†è¿›å…¥æµæ°´çº¿çš„ç¬¬ä¸€ä¸ªåè®®è½¬æ¢æ­¥éª¤ã€‚å®ƒåˆ†æä¼ å…¥è¯·æ±‚çš„åè®®ç±»å‹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç›®æ ‡ä¾›åº”å•†æ‰€æœŸæœ›çš„åè®®æ ¼å¼ã€‚

### ğŸ“‹ æ ¸å¿ƒèŒè´£
- **åè®®è¯†åˆ«**: è‡ªåŠ¨æ£€æµ‹è¯·æ±‚åè®®ç±»å‹ï¼ˆOpenAI Chatã€Responsesã€Anthropicï¼‰
- **åŒå‘è½¬æ¢**: æ”¯æŒå¤šç§åè®®ä¹‹é—´çš„åŒå‘è½¬æ¢
- **æ ¼å¼è§„èŒƒåŒ–**: ç¡®ä¿è¯·æ±‚æ ¼å¼ç¬¦åˆç›®æ ‡åè®®è¦æ±‚
- **å…ƒæ•°æ®æ³¨å…¥**: æ·»åŠ è½¬æ¢è¿½è¸ªå’Œè°ƒè¯•ä¿¡æ¯
- **å·¥å…·è°ƒç”¨é€‚é…**: å¤„ç†ä¸åŒåè®®çš„å·¥å…·è°ƒç”¨æ ¼å¼å·®å¼‚

## ğŸ”„ æ”¯æŒçš„åè®®è½¬æ¢

### ğŸ”§ OpenAI â†’ OpenAI è§„èŒƒåŒ–
- **å®ç°æ¥æº**: rcc-llmswitch-coreï¼ˆåŒ…å†…å®ç°ï¼‰
- **å¯¼å…¥è·¯å¾„**: `rcc-llmswitch-core/llmswitch/openai-normalizer`
- **ç±»å‹**: `llmswitch-openai-openai`
- **åè®®**: `openai` â†’ `openai`
- **åŠŸèƒ½**: OpenAI åè®®è§„èŒƒåŒ–å’ŒéªŒè¯
- **ç‰¹æ€§**:
  - ä¸¥æ ¼çš„ Chat Completions æ ¼å¼éªŒè¯
  - å·¥å…·è°ƒç”¨å‚æ•°æ ‡å‡†åŒ–ï¼ˆJSON å­—ç¬¦ä¸²éªŒè¯ï¼‰
  - å‡½æ•°åç§°ä¸å·¥å…·å£°æ˜åŒ¹é…éªŒè¯
  - æ¶ˆæ¯æ ¼å¼è§„èŒƒåŒ–
  - åŸºäº `rcc-llmswitch-core` çš„è½¬æ¢å¼•æ“
  - è¯·æ±‚/å“åº”å…ƒæ•°æ®æ·»åŠ 
  - è°ƒè¯•å’Œæ€§èƒ½ç›‘æ§

### ğŸ”„ Anthropic â†” OpenAI åŒå‘è½¬æ¢
- **å®ç°æ¥æº**: rcc-llmswitch-coreï¼ˆåŒ…å†…å®ç°ï¼‰
- **å¯¼å…¥è·¯å¾„**: `rcc-llmswitch-core/llmswitch/anthropic-openai-converter`
- **ç±»å‹**: `llmswitch-anthropic-openai`
- **åè®®**: `anthropic` â†” `openai`
- **åŠŸèƒ½**: Anthropic Claude API ä¸ OpenAI Chat API äº’è½¬
- **ç‰¹æ€§**:
  - åŸºäºé…ç½®é©±åŠ¨çš„è½¬æ¢æ˜ å°„
  - æ”¯æŒè¯·æ±‚å’Œå“åº”åŒå‘è½¬æ¢
  - æ™ºèƒ½åè®®æ£€æµ‹å’Œè·¯ç”±
  - å·¥å…·è°ƒç”¨æ ¼å¼é€‚é…
  - æµå¼å“åº”æ”¯æŒ
  - æ¨¡å‹å‚æ•°æ˜ å°„ï¼ˆtemperature, max_tokens ç­‰ï¼‰
  - è½¬æ¢ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆæŒ‰ requestId è®°å½•å…¥å£åè®®ï¼‰
  - ä¸¥æ ¼æ¨¡å¼å’Œä¿¡ä»»æ¨¡å¼ï¼ˆtrustSchemaï¼‰

### ğŸŒ Responses â†’ Chat è½¬æ¢
- **å®ç°æ¥æº**: rcc-llmswitch-coreï¼ˆåŒ…å†…å®ç°ï¼‰
- **å¯¼å…¥è·¯å¾„**: `rcc-llmswitch-core/llmswitch/llmswitch-response-chat`
- **ç±»å‹**: `llmswitch-response-chat`
- **åè®®**: `openai-responses` â†’ `openai`
- **åŠŸèƒ½**: OpenAI Responses API è½¬æ¢ä¸º Chat Completions æ ¼å¼
- **ç‰¹æ€§**:
  - åŸºäº `rcc-llmswitch-core/conversion` çš„æ ‡å‡†åŒ–è½¬æ¢
  - è¯·æ±‚ä¸Šä¸‹æ–‡æ•è·å’Œç®¡ç†
  - å·¥å…·è°ƒç”¨æ ¼å¼è½¬æ¢
  - å“åº” ID æå–å’Œè¿½è¸ª
  - è‡ªåŠ¨æ¨¡å¼æ£€æµ‹ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä½³è½¬æ¢ç­–ç•¥ï¼‰

### ğŸ”„ Responses Passthrough
- **å®ç°æ¥æº**: rcc-llmswitch-coreï¼ˆåŒ…å†…å®ç°ï¼‰
- **å¯¼å…¥è·¯å¾„**: `rcc-llmswitch-core/llmswitch/llmswitch-responses-passthrough`
- **ç±»å‹**: `llmswitch-responses-passthrough`
- **åè®®**: `openai-responses` â†’ `openai-responses`
- **åŠŸèƒ½**: Responses API ç›´æ¥é€ä¼ ï¼Œæœ€å°è½¬æ¢å¼€é”€
- **ç‰¹æ€§**:
  - åŸºæœ¬çš„å¯¹è±¡å½¢çŠ¶éªŒè¯
  - å…ƒæ•°æ®æ ‡è®°å’Œæ³¨å…¥
  - æœ€å°æ€§èƒ½å¼€é”€
  - é€‚ç”¨äºåŸç”Ÿ Responses API æ”¯æŒ

### ğŸ› ï¸ è½¬æ¢è·¯ç”±å™¨
- **å®ç°æ–‡ä»¶**: `llmswitch-conversion-router.ts`
- **ç±»å‹**: `llmswitch-conversion-router`
- **åè®®**: æ™ºèƒ½è·¯ç”±åˆ°å…¶ä»– LLMSwitch å®ç°
- **åŠŸèƒ½**: æ ¹æ®è¯·æ±‚ç±»å‹å’Œé…ç½®æ™ºèƒ½è·¯ç”±åˆ°é€‚å½“çš„è½¬æ¢å™¨
- **ç‰¹æ€§**:
  - åŠ¨æ€è½¬æ¢å™¨é€‰æ‹©
  - é…ç½®é©±åŠ¨çš„è·¯ç”±è§„åˆ™
  - å¤šåè®®æ”¯æŒ
  - å›é€€æœºåˆ¶

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/modules/pipeline/modules/llmswitch/
â”œâ”€â”€ conversion/                     # ï¼ˆå¦‚éœ€è¦†ç›–çš„ï¼‰è½¬æ¢è§„åˆ™/é…ç½®
â”‚   â”œâ”€â”€ anthropic-openai-config.ts
â”‚   â””â”€â”€ ...
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ (æ ¸å¿ƒå®ç°ç”± rcc-llmswitch-core æä¾›)
â”œâ”€â”€ llmswitch-conversion-router.ts   # è·¯ç”±å™¨ï¼ˆå¦‚ä¿ç•™ï¼‰
â””â”€â”€ README.md                        # æœ¬æ–‡æ¡£
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### OpenAI è§„èŒƒåŒ–ä½¿ç”¨
```typescript
import { OpenAINormalizerLLMSwitch } from 'rcc-llmswitch-core/llmswitch/openai-normalizer';

const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: {
    enableValidation: true,
    enableMetadata: true
  }
}, dependencies);

await llmSwitch.initialize();

// å¤„ç† OpenAI è¯·æ±‚ï¼Œç¡®ä¿æ ¼å¼è§„èŒƒ
const normalizedRequest = await llmSwitch.processIncoming({
  model: 'gpt-4',
  messages: [
    { role: 'user', content: 'Hello!' }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'calculate',
        description: 'Perform calculations',
        parameters: {
          type: 'object',
          properties: {
            expression: { type: 'string' }
          }
        }
      }
    }
  ]
});

// ç»“æœ: æ ¼å¼è§„èŒƒåŒ–çš„è¯·æ±‚ï¼ŒåŒ…å«å·¥å…·è°ƒç”¨éªŒè¯
```

### Anthropic-OpenAI åŒå‘è½¬æ¢
```typescript
import { AnthropicOpenAIConverter } from 'rcc-llmswitch-core/llmswitch/anthropic-openai-converter';

const converter = new AnthropicOpenAIConverter({
  type: 'llmswitch-anthropic-openai',
  config: {
    enableStreaming: true,
    enableTools: true,
    trustSchema: true,
    conversionMappings: {
      // è‡ªå®šä¹‰è½¬æ¢æ˜ å°„
      requestMappings: { /* ... */ },
      responseMappings: { /* ... */ }
    }
  }
}, dependencies);

await converter.initialize();

// Anthropic æ ¼å¼è¯·æ±‚
const anthropicRequest = {
  model: 'claude-3-sonnet',
  messages: [
    { role: 'user', content: 'Hello!' }
  ],
  max_tokens: 1000,
  tools: [ /* Anthropic å·¥å…·æ ¼å¼ */ ]
};

// è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢ä¸º OpenAI æ ¼å¼
const openAIRequest = await converter.processIncoming(anthropicRequest);
```

### Responses API è½¬æ¢
```typescript
import { ResponsesToChatLLMSwitch } from 'rcc-llmswitch-core/llmswitch/llmswitch-response-chat';

## æ„å»ºé¡ºåºï¼ˆé‡è¦ï¼‰

æ¶‰åŠ `sharedmodule/` ä¸‹çš„ä¿®æ”¹ï¼Œè¯·éµå¾ªâ€œå…ˆæ¨¡å—ã€åæ•´åŒ…â€çš„æ„å»ºé¡ºåºï¼š

- æ„å»ºå…±äº«æ¨¡å—ï¼š`npm run --workspace sharedmodule/llmswitch-core build`
- æ„å»ºæ ¹åŒ…ï¼š`npm run build`

ç¡®ä¿ core æ”¹åŠ¨ä¼˜å…ˆç”Ÿæ•ˆï¼Œé¿å…å¼•ç”¨æ—§æ„ä»¶å¯¼è‡´çš„ä¸ä¸€è‡´ã€‚

const responsesConverter = new ResponsesToChatLLMSwitch({
  type: 'llmswitch-response-chat',
  config: {
    // é…ç½®é€‰é¡¹
  }
}, dependencies);

await responsesConverter.initialize();

// Responses API æ ¼å¼è¯·æ±‚
const responsesRequest = {
  model: 'gpt-4-turbo',
  input: [
    { role: 'user', content: 'Hello!' }
  ],
  tools: [ /* Responses å·¥å…·æ ¼å¼ */ ]
};

// è½¬æ¢ä¸º Chat Completions æ ¼å¼
const chatRequest = await responsesConverter.processIncoming(responsesRequest);
```

## âš™ï¸ é…ç½®é€‰é¡¹

### OpenAI è§„èŒƒåŒ–é…ç½®
```typescript
interface OpenAINormalizerConfig {
  enableValidation?: boolean;        // å¯ç”¨ä¸¥æ ¼éªŒè¯
  enableMetadata?: boolean;          // å¯ç”¨å…ƒæ•°æ®å¢å¼º
  maxLogEntries?: number;           // æœ€å¤§æ—¥å¿—æ¡ç›®æ•°
}
```

### Anthropic-OpenAI è½¬æ¢é…ç½®
```typescript
interface AnthropicOpenAIConfig {
  enableStreaming?: boolean;         // å¯ç”¨æµå¼è½¬æ¢
  enableTools?: boolean;            // å¯ç”¨å·¥å…·è½¬æ¢
  trustSchema?: boolean;            // ä¿¡ä»»æ¨¡å¼ï¼ˆä¸é‡å‘½åå·¥å…·ï¼‰
  conversionMappings?: {            // è‡ªå®šä¹‰è½¬æ¢æ˜ å°„
    requestMappings?: any;
    responseMappings?: any;
  };
}
```

### Responses è½¬æ¢é…ç½®
```typescript
interface ResponsesChatConfig {
  autoMode?: boolean;              // è‡ªåŠ¨æ¨¡å¼æ£€æµ‹
  preserveResponsesFormat?: boolean; // ä¿æŒ Responses æ ¼å¼
}
```

## ğŸ”„ è½¬æ¢æµç¨‹

### åè®®æ£€æµ‹å’Œè·¯ç”±
```typescript
// è‡ªåŠ¨åè®®æ£€æµ‹
function detectProtocol(request: any): 'openai' | 'anthropic' | 'responses' {
  if (request.messages) return 'openai';
  if (request.input) return 'responses';
  if (request.anthropic_version) return 'anthropic';
  return 'openai'; // é»˜è®¤
}

// è½¬æ¢è·¯ç”±
const converter = selectConverter(detectProtocol(request), targetProtocol);
const converted = await converter.processIncoming(request);
```

### å…ƒæ•°æ®æ³¨å…¥
```typescript
// æ‰€æœ‰è½¬æ¢éƒ½ä¼šæ³¨å…¥ç»Ÿä¸€çš„å…ƒæ•°æ®
const enhancedRequest = {
  ...convertedRequest,
  _metadata: {
    switchType: 'llmswitch-xxx',
    timestamp: Date.now(),
    entryProtocol: 'detected-protocol',
    targetProtocol: 'target-protocol',
    requestId: 'generated-or-preserved-id',
    conversionContext: { /* è½¬æ¢ä¸Šä¸‹æ–‡ */ }
  }
};
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†

### åè®®éªŒè¯é”™è¯¯
```typescript
// OpenAI æ ¼å¼éªŒè¯
if (!request.messages && !request.prompt) {
  throw new Error('Invalid OpenAI protocol: missing messages or prompt');
}

// å·¥å…·è°ƒç”¨éªŒè¯
if (request.tool_calls) {
  for (const toolCall of request.tool_calls) {
    if (toolCall.function && typeof toolCall.function.arguments !== 'string') {
      throw new Error('Tool function.arguments must be a JSON string');
    }
  }
}
```

### è½¬æ¢é”™è¯¯å¤„ç†
```typescript
// è½¬æ¢å¤±è´¥æ—¶çš„å¤„ç†
try {
  const converted = await this.convertRequest(request);
} catch (error) {
  this.logger.logModule(this.id, 'conversion-error', {
    error: error.message,
    entryProtocol: this.detectedProtocol,
    targetProtocol: this.targetProtocol
  });
  throw new Error(`Protocol conversion failed: ${error.message}`);
}
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### è½¬æ¢æ€§èƒ½è¿½è¸ª
```typescript
// æ€§èƒ½å…ƒæ•°æ®
const performanceMetadata = {
  conversionTime: Date.now() - startTime,
  entryProtocol: detectedProtocol,
  targetProtocol: targetProtocol,
  conversionRules: appliedRules.length,
  hasTools: !!request.tools,
  messageCount: request.messages?.length || 0
};
```

### è½¬æ¢ç»Ÿè®¡
```typescript
// è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
const stats = await llmSwitch.getConversionStats();
console.log({
  totalConversions: stats.totalConversions,
  successRate: stats.successRate,
  averageConversionTime: stats.averageTime,
  protocolDistribution: stats.protocolDistribution
});
```

## ğŸŒ API åè®®æ”¯æŒ

### OpenAI Chat Completions API
- **ç«¯ç‚¹**: `/v1/chat/completions`
- **è¯·æ±‚æ ¼å¼**: `{ messages, model, tools, tool_calls, stream }`
- **å“åº”æ ¼å¼**: `{ choices, usage, id, created }`
- **ç‰¹æ€§**: å·¥å…·è°ƒç”¨ã€æµå¼å“åº”ã€å¤šè½®å¯¹è¯

### OpenAI Responses API
- **ç«¯ç‚¹**: `/v1/responses`
- **è¯·æ±‚æ ¼å¼**: `{ input, model, tools, stream }`
- **å“åº”æ ¼å¼**: `{ output, usage, id, created }`
- **ç‰¹æ€§**: æ–°ä¸€ä»£ APIã€ç®€åŒ–æ ¼å¼ã€åŸç”Ÿå·¥å…·æ”¯æŒ

### Anthropic Claude API
- **ç«¯ç‚¹**: `/v1/messages`
- **è¯·æ±‚æ ¼å¼**: `{ messages, model, max_tokens, tools }`
- **å“åº”æ ¼å¼**: `{ content, usage, id, created }`
- **ç‰¹æ€§**: ç³»ç»Ÿæç¤ºã€å·¥å…·ä½¿ç”¨ã€æ€è€ƒå†…å®¹

## ğŸ”§ æ‰©å±•æ€§

### æ·»åŠ æ–°çš„åè®®è½¬æ¢
```typescript
class NewProtocolConverter implements LLMSwitchModule {
  readonly type = 'llmswitch-new-protocol';
  readonly protocol = 'new-protocol';

  async processIncoming(request: SharedPipelineRequest): Promise<SharedPipelineRequest> {
    // æ£€æµ‹åè®®
    const detectedProtocol = this.detectProtocol(request.data);

    // æ‰§è¡Œè½¬æ¢
    const converted = await this.convertProtocol(request.data, detectedProtocol, 'target');

    // æ³¨å…¥å…ƒæ•°æ®
    return {
      ...request,
      data: {
        ...converted,
        _metadata: {
          switchType: this.type,
          timestamp: Date.now(),
          entryProtocol: detectedProtocol,
          targetProtocol: 'target'
        }
      }
    };
  }

  private detectProtocol(data: any): string {
    // å®ç°åè®®æ£€æµ‹é€»è¾‘
  }

  private async convertProtocol(data: any, from: string, to: string): Promise<any> {
    // å®ç°åè®®è½¬æ¢é€»è¾‘
  }
}
```

### è‡ªå®šä¹‰è½¬æ¢è§„åˆ™
```typescript
// åœ¨ anthropic-openai-config.ts ä¸­æ·»åŠ è‡ªå®šä¹‰æ˜ å°„
const customMappings = {
  requestMappings: [
    {
      sourcePath: 'max_tokens',
      targetPath: 'max_tokens',
      transform: 'direct'
    },
    {
      sourcePath: 'temperature',
      targetPath: 'temperature',
      transform: 'mapping',
      mapping: {
        0: 0,
        1: 1,
        2: 2  // Anthropic 0-2 æ˜ å°„åˆ° OpenAI 0-2
      }
    }
  ],
  responseMappings: [
    // å“åº”æ˜ å°„è§„åˆ™
  ]
};
```

## ğŸ“ˆ ç‰ˆæœ¬ä¿¡æ¯

- **å½“å‰ç‰ˆæœ¬**: 3.0.0
- **æ–°å¢ç‰¹æ€§**:
  - Responses API æ”¯æŒ
  - Anthropic åŒå‘è½¬æ¢
  - æ™ºèƒ½è½¬æ¢è·¯ç”±
  - åŸºäº `rcc-llmswitch-core` çš„æ ‡å‡†åŒ–è½¬æ¢
- **å…¼å®¹æ€§**: RouteCodex Pipeline >= 3.0.0
- **TypeScript**: >= 5.0.0
- **Node.js**: >= 18.0.0

## ğŸ”— ä¾èµ–å…³ç³»

- **rcc-llmswitch-core**: æ ¸å¿ƒè½¬æ¢å¼•æ“å’Œå·¥å…·å‡½æ•°
- **PipelineDebugLogger**: æ¨¡å—æ—¥å¿—è®°å½•
- **BaseModule**: åŸºç¡€æ¨¡å—æ¥å£
- **SharedPipelineRequest/Response**: å…±äº«æ•°æ®ä¼ è¾“å¯¹è±¡

## ğŸš¨ å·²çŸ¥é™åˆ¶

### å½“å‰é™åˆ¶
1. **åè®®ç‰ˆæœ¬æ”¯æŒ** - ä¸»è¦æ”¯æŒ API v1 ç‰ˆæœ¬
2. **å®æ—¶è½¬æ¢** - æµå¼åè®®è½¬æ¢å­˜åœ¨å»¶è¿Ÿ
3. **å¤æ‚å·¥å…·é“¾** - å¤šæ­¥éª¤å·¥å…·è°ƒç”¨è½¬æ¢å¯èƒ½ä¸å®Œæ•´
4. **é”™è¯¯æ¢å¤** - è½¬æ¢å¤±è´¥åçš„å›é€€æœºåˆ¶æœ‰é™

### æœªæ¥è®¡åˆ’
1. **æ›´å¤šåè®®æ”¯æŒ** - Google Geminiã€Cohere ç­‰
2. **å®æ—¶æµå¼è½¬æ¢** - é›¶å»¶è¿Ÿæµå¼åè®®è½¬æ¢
3. **æ™ºèƒ½åè®®æ£€æµ‹** - åŸºäºå†…å®¹ç‰¹å¾çš„è‡ªåŠ¨åè®®è¯†åˆ«
4. **è½¬æ¢è§„åˆ™å­¦ä¹ ** - åŸºäºä½¿ç”¨æ¨¡å¼çš„æ™ºèƒ½ä¼˜åŒ–

## ğŸ”„ æ›´æ–°æ—¥å¿—

### v3.0.0 (2025-10-24)
- âœ¨ æ–°å¢ Responses API å®Œæ•´æ”¯æŒ
- âœ¨ æ–°å¢ Anthropic â†” OpenAI åŒå‘è½¬æ¢
- âœ¨ æ–°å¢æ™ºèƒ½è½¬æ¢è·¯ç”±å™¨
- ğŸ”„ åŸºäº `rcc-llmswitch-core` çš„æ ‡å‡†åŒ–é‡æ„
- ğŸ›¡ï¸ å¢å¼ºçš„å·¥å…·è°ƒç”¨éªŒè¯å’Œè½¬æ¢
- ğŸ“Š å®Œå–„çš„æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•æ”¯æŒ

### v2.0.0 (2025-01-22)
- ğŸ”§ OpenAI è§„èŒƒåŒ–åŠŸèƒ½å¢å¼º
- ğŸ“Š æ€§èƒ½ç›‘æ§åŠŸèƒ½å®Œå–„
- ğŸ›¡ï¸ é”™è¯¯å¤„ç†æœºåˆ¶ä¼˜åŒ–

### v1.0.0 (2025-01-15)
- ğŸ¯ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- ğŸ”„ åŸºç¡€çš„ OpenAI é€ä¼ åŠŸèƒ½
- ğŸ“Š ç®€å•çš„å…ƒæ•°æ®æ³¨å…¥

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æ£€æŸ¥åè®®æ ¼å¼æ˜¯å¦ç¬¦åˆå¯¹åº” API è§„èŒƒ
2. éªŒè¯è½¬æ¢é…ç½®æ˜¯å¦æ­£ç¡®
3. æŸ¥çœ‹è½¬æ¢æ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯
4. æ£€æŸ¥ç›®æ ‡åè®®çš„å®˜æ–¹æ–‡æ¡£

---

**æœ€åæ›´æ–°**: 2025-10-24 - å…¨é¢æ›´æ–° LLMSwitch æ¨¡å—æ–‡æ¡£ï¼Œæ–°å¢ Responses API å’Œ Anthropic æ”¯æŒ

LLMSwitch æ¨¡å—æä¾›å¤šåè®®è½¬æ¢åŠŸèƒ½ï¼Œå°†ä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹APIåè®®è¿›è¡Œç›¸äº’è½¬æ¢ï¼Œæ”¯æŒ OpenAIã€Anthropicã€Responses ç­‰å¤šç§åè®®æ ¼å¼ã€‚

## ğŸ¯ æ¨¡å—æ¦‚è¿°

LLMSwitch æ¨¡å—æ˜¯æµæ°´çº¿æ¶æ„çš„ç¬¬ 1 å±‚ï¼ˆåè®®è½¬æ¢å±‚ï¼‰ï¼Œè´Ÿè´£å¤„ç†è¿›å…¥æµæ°´çº¿çš„ç¬¬ä¸€ä¸ªåè®®è½¬æ¢æ­¥éª¤ã€‚å®ƒåˆ†æä¼ å…¥è¯·æ±‚çš„åè®®ç±»å‹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç›®æ ‡ä¾›åº”å•†æ‰€æœŸæœ›çš„åè®®æ ¼å¼ã€‚

## ğŸ”„ æ”¯æŒçš„åè®®è½¬æ¢

### ğŸ”§ OpenAI è§„èŒƒåŒ–è½¬æ¢å™¨
- å®ç°æ¥æº: rcc-llmswitch-coreï¼ˆåŒ…å†…å®ç°ï¼‰
- å¯¼å…¥è·¯å¾„: `rcc-llmswitch-core/llmswitch/openai-normalizer`
- **åŠŸèƒ½**: OpenAI åè®®è§„èŒƒåŒ–ï¼Œä¿æŒè¯·æ±‚ç»“æ„ä¸€è‡´
- **ç‰¹æ€§**:
  - å®Œæ•´çš„ OpenAI åè®®æ”¯æŒ
  - è¯·æ±‚/å“åº”å…ƒæ•°æ®æ·»åŠ 
  - æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•ä¿¡æ¯
  - åè®®éªŒè¯å’Œæ ‡å‡†åŒ–
  - é”™è¯¯ä¸Šä¸‹æ–‡å¢å¼º

### ğŸ¤– Anthropic-OpenAI åŒå‘è½¬æ¢å™¨
- å®ç°æ¥æº: rcc-llmswitch-coreï¼ˆåŒ…å†…å®ç°ï¼‰
- å¯¼å…¥è·¯å¾„: `rcc-llmswitch-core/llmswitch/anthropic-openai-converter`
- **åŠŸèƒ½**: Anthropic åè®®ä¸ OpenAI åè®®äº’è½¬
- **ç‰¹æ€§**:
  - æ¶ˆæ¯æ ¼å¼è½¬æ¢
  - å·¥å…·è°ƒç”¨é€‚é…
  - æµå¼å“åº”å¤„ç†
  - æ¨ç†å†…å®¹å¤„ç†
  - å“åº”æ ¼å¼æ ‡å‡†åŒ–

### ğŸ†• Responses-Chat è½¬æ¢å™¨ï¼ˆç»ç”± core codecsï¼‰
- å®ç°æ¥æº: rcc-llmswitch-coreï¼ˆåŒ…å†…å®ç°ï¼‰
- å¯¼å…¥è·¯å¾„: `rcc-llmswitch-core/llmswitch/llmswitch-response-chat`
- **åŠŸèƒ½**: OpenAI Responses API ä¸ Chat Completions API äº’è½¬
- **ç‰¹æ€§**:
  - **åŒå‘è½¬æ¢**: Responses â†” Chat æ ¼å¼å®Œå…¨æ”¯æŒ
  - **å·¥å…·è°ƒç”¨**: å®Œæ•´çš„å·¥å…·è°ƒç”¨æ ¼å¼è½¬æ¢
  - **æµå¼äº‹ä»¶**: æ”¯æŒ Responses API çš„æ‰€æœ‰ SSE äº‹ä»¶
  - **å…ƒæ•°æ®ä¿æŒ**: ä¿ç•™åŸå§‹è¯·æ±‚ä¸Šä¸‹æ–‡å’Œåè®®ä¿¡æ¯
  - **æ™ºèƒ½å¤„ç†**: è‡ªåŠ¨å¤„ç† reasoningã€function_call ç­‰ç‰¹æ®Šå†…å®¹
- **ç»Ÿä¸€å…¥å£**: åœ¨æœ€æ–°æ¶æ„ä¸‹ï¼Œæ‰€æœ‰æµæ°´çº¿å®ä¾‹éƒ½æŒ‚è½½ `llmswitch-conversion-router`ï¼Œå¹¶ä¾é  `entryEndpoint` è‡ªåŠ¨åŒ¹é…å¯¹åº” codecï¼ˆOpenAI / Anthropic / Responsesï¼‰ï¼Œæ— éœ€é¢å¤–çš„æ‰‹å·¥é…ç½®ã€‚
- **æ ¸å¿ƒå®ç°æ”¶æ•›**: å…·ä½“çš„è½¬æ¢é€»è¾‘ï¼ˆResponsesâ†”Chatã€OpenAI è§„èŒƒåŒ–ç­‰ï¼‰å·²è¿ç§»åˆ° `@routecodex/llmswitch-core`ï¼Œæ­¤å¤„é€‚é…å™¨ä»…åšå§”æ´¾ï¼Œé¿å…é‡å¤å®ç°ã€‚

### â›” ç»Ÿä¸€åè®®è½¬æ¢å™¨
è¯¥å®ç°å·²ç§»é™¤ã€‚ç»Ÿä¸€è·¯ç”±ç”± `llmswitch-conversion-router` + `rcc-llmswitch-core` çš„ `switch-orchestrator` + `codecs/*` æä¾›ï¼Œè¯·ä½¿ç”¨ conversion-router ä½œä¸ºå…¥å£ã€‚

## ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½

### ğŸ“Š åè®®æ£€æµ‹ä¸è·¯ç”±
```typescript
// è‡ªåŠ¨åè®®æ£€æµ‹
private detectProtocol(request: any): 'openai' | 'anthropic' | 'responses' | 'unknown' {
  if (request.input && Array.isArray(request.input)) {
    return 'responses';
  } else if (request.messages) {
    return 'openai';
  } else if (this.hasAnthropicFormat(request)) {
    return 'anthropic';
  }
  return 'unknown';
}
```

### ğŸ”„ Responses è½¬æ¢ç¤ºä¾‹
```typescript
// Responses â†’ Chat è½¬æ¢
const responsesToChat = new ResponsesToChatLLMSwitch(config, dependencies);

// è¾“å…¥ï¼šResponses API æ ¼å¼
const responsesRequest = {
  model: 'gpt-4',
  instructions: 'You are a helpful assistant.',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Hello!' }]
    }
  ],
  tools: [/* å·¥å…·å®šä¹‰ */],
  stream: true
};

// è¾“å‡ºï¼šChat Completions æ ¼å¼
const chatRequest = await responsesToChat.processIncoming(responsesRequest);
// {
//   model: 'gpt-4',
//   messages: [
//     { role: 'system', content: 'You are a helpful assistant.' },
//     { role: 'user', content: 'Hello!' }
//   ],
//   tools: [/* è½¬æ¢åçš„å·¥å…·å®šä¹‰ */],
//   stream: true,
//   _metadata: {
//     switchType: 'llmswitch-response-chat',
//     entryProtocol: 'responses',
//     targetProtocol: 'openai'
//   }
// }
```

### ğŸ“‹ å…ƒæ•°æ®å¢å¼º
```typescript
// è¯·æ±‚å…ƒæ•°æ®æå–
private extractRequestMetadata(request: any, protocol: string): Record<string, any> {
  return {
    timestamp: Date.now(),
    protocol,
    entryProtocol: protocol,
    targetProtocol: this.getTargetProtocol(protocol),
    hasModel: !!request.model,
    hasTools: !!request.tools,
    hasStream: !!request.stream,
    messageCount: this.getMessageCount(request),
    toolCount: request.tools?.length || 0,
    requestType: this.inferRequestType(request, protocol)
  };
}
```

### ğŸ›¡ï¸ åè®®éªŒè¯
```typescript
// åè®®éªŒè¯
private validateProtocol(request: any, protocol: string): void {
  switch (protocol) {
    case 'openai':
      this.validateOpenAIProtocol(request);
      break;
    case 'anthropic':
      this.validateAnthropicProtocol(request);
      break;
    case 'responses':
      this.validateResponsesProtocol(request);
      break;
    default:
      throw new Error(`Unsupported protocol: ${protocol}`);
  }
}
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/modules/pipeline/modules/llmswitch/
â”œâ”€â”€ (å…¼å®¹ä¿ç•™) openai-normalizer.ts   # æ—§æœ¬åœ°å®ç°ï¼ˆå·²ç”± core æä¾›ç»Ÿä¸€å®ç°ï¼‰
â”œâ”€â”€ (æ ¸å¿ƒå®ç°ç”± rcc-llmswitch-core æä¾›)
â”œâ”€â”€ anthropic-openai-config.ts        # ï¼ˆå¦‚éœ€è¦†ç›–ï¼‰Anthropic è½¬æ¢é…ç½®
â””â”€â”€ README.md                         # æœ¬æ–‡æ¡£
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### Responses API è½¬æ¢
```typescript
import { ResponsesToChatLLMSwitch } from 'rcc-llmswitch-core/llmswitch/llmswitch-response-chat';

const responsesSwitch = new ResponsesToChatLLMSwitch({
  type: 'llmswitch-response-chat',
  config: {
    enableValidation: true,
    enableMetadata: true,
    preserveReasoning: true
  }
}, dependencies);

await responsesSwitch.initialize();

// å¤„ç† Responses API è¯·æ±‚
const chatRequest = await responsesSwitch.processIncoming({
  model: 'gpt-4',
  instructions: 'You are a helpful assistant.',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Calculate 15 * 25' }]
    }
  ],
  tools: [
    {
      type: 'function',
      name: 'calculate',
      description: 'Perform mathematical calculations',
      parameters: {
        type: 'object',
        properties: {
          expression: { type: 'string' }
        }
      }
    }
  ]
});
```

### åœ¨æµæ°´çº¿é…ç½®ä¸­ä½¿ç”¨ï¼ˆé€šè¿‡ conversion-routerï¼‰
```typescript
const pipelineConfig = {
  modules: {
    llmSwitch: {
      type: 'llmswitch-conversion-router',  // ç»Ÿä¸€å…¥å£ï¼ˆæ ¹æ® entryEndpoint è‡ªåŠ¨é€‰æ‹© codecï¼‰
      config: {
        // ç”±ä¸»åŒ…åœ¨è¿è¡Œæ—¶æä¾›ï¼š
        // baseDir æŒ‡å‘åŒ…æ ¹ï¼ˆåŒ…å« config/ï¼‰ï¼ŒprofilesPath ç›¸å¯¹è¯¥ç›®å½•
        baseDir: "<auto>",
        profilesPath: "config/conversion/llmswitch-profiles.json"
      }
    }
  }
};
```

### é…ç½®ç¤ºä¾‹
```json
{
  "virtualrouter": {
    "inputProtocol": "responses",
    "outputProtocol": "openai",
    "providers": {
      "lmstudio": {
        "type": "lmstudio",
        "baseURL": "http://localhost:1234",
        "apiKey": "your-api-key",
        "models": {
          "gpt-4": {
            "compatibility": {
              "type": "responses-chat-switch"
            }
          }
        }
      }
    },
    "routing": {
      "default": ["lmstudio.gpt-4"]
    }
  }
}
```

## âš™ï¸ é…ç½®é€‰é¡¹

### Responses-Chat è½¬æ¢é…ç½®
```typescript
interface ResponsesChatConfig {
  enableValidation?: boolean;           // å¯ç”¨åè®®éªŒè¯
  enableMetadata?: boolean;             // å¯ç”¨å…ƒæ•°æ®å¢å¼º
  preserveReasoning?: boolean;          // ä¿ç•™æ¨ç†å†…å®¹
  enableToolMapping?: boolean;          // å¯ç”¨å·¥å…·æ˜ å°„
  maxLogEntries?: number;               // æœ€å¤§æ—¥å¿—æ¡ç›®æ•°
  streamingChunkSize?: number;          // æµå¼å“åº”å—å¤§å°
}
```

### OpenAI é€ä¼ é…ç½®
```typescript
interface OpenAIPassthroughConfig {
  enableValidation?: boolean;           // å¯ç”¨åè®®éªŒè¯
  enableMetadata?: boolean;             // å¯ç”¨å…ƒæ•°æ®å¢å¼º
  enablePerformanceTracking?: boolean;  // å¯ç”¨æ€§èƒ½è·Ÿè¸ª
  maxLogEntries?: number;               // æœ€å¤§æ—¥å¿—æ¡ç›®æ•°
}
```

### Anthropic-OpenAI è½¬æ¢é…ç½®
```typescript
interface AnthropicOpenAIConfig {
  direction: 'anthropic-to-openai' | 'openai-to-anthropic';
  enableTools?: boolean;                 // å¯ç”¨å·¥å…·è½¬æ¢
  enableStreaming?: boolean;             // å¯ç”¨æµå¼è½¬æ¢
  preserveReasoning?: boolean;           // ä¿ç•™æ¨ç†å†…å®¹
  modelMappings?: Record<string, string>; // æ¨¡å‹æ˜ å°„
}
```

## ğŸ”„ æ”¯æŒçš„è½¬æ¢æ˜ å°„

### Responses â†” Chat è½¬æ¢
| Responses å­—æ®µ | Chat å­—æ®µ | è¯´æ˜ |
|----------------|------------|------|
| `instructions` | `messages[0].content` (system role) | ç³»ç»ŸæŒ‡ä»¤ |
| `input[].content[]` | `messages[].content` | æ¶ˆæ¯å†…å®¹ |
| `tools[]` | `tools[]` | å·¥å…·å®šä¹‰ |
| `tool_choice` | `tool_choice` | å·¥å…·é€‰æ‹© |
| `max_output_tokens` | `max_tokens` | æœ€å¤§ä»¤ç‰Œæ•° |
| `stream` | `stream` | æµå¼æ§åˆ¶ |

### å·¥å…·è°ƒç”¨è½¬æ¢
```typescript
// Responses æ ¼å¼å·¥å…·è°ƒç”¨
{
  "type": "function_call",
  "name": "calculate",
  "arguments": "{\"expression\":\"15*25\"}",
  "call_id": "call_123"
}

// è½¬æ¢ä¸º Chat æ ¼å¼
{
  "tool_calls": [{
    "id": "call_123",
    "type": "function",
    "function": {
      "name": "calculate",
      "arguments": "{\"expression\":\"15*25\"}"
    }
  }]
}
```

## ğŸ›ï¸ è¯·æ±‚ç±»å‹æ¨æ–­

### æ”¯æŒçš„è¯·æ±‚ç±»å‹
```typescript
type RequestType =
  | 'chat'           // èŠå¤©å®Œæˆ (OpenAI)
  | 'messages'       // æ¶ˆæ¯æ ¼å¼ (Anthropic)
  | 'responses'      // Responses API
  | 'completion'     // æ–‡æœ¬å®Œæˆ
  | 'embedding'      // æ–‡æœ¬åµŒå…¥
  | 'tool'           // å·¥å…·è°ƒç”¨
  | 'unknown';       // æœªçŸ¥ç±»å‹
```

### åè®®è‡ªåŠ¨æ£€æµ‹
```typescript
private detectProtocol(request: any): ProtocolType {
  // Responses API æ£€æµ‹
  if (request.input && Array.isArray(request.input)) {
    return 'responses';
  }

  // OpenAI Chat æ£€æµ‹
  if (request.messages && Array.isArray(request.messages)) {
    return 'openai';
  }

  // Anthropic Messages æ£€æµ‹
  if (this.hasAnthropicFormat(request)) {
    return 'anthropic';
  }

  return 'unknown';
}
```

## ğŸ“Š æ€§èƒ½è·Ÿè¸ª

### æ€§èƒ½å…ƒæ•°æ®
```typescript
private addPerformanceMetadata(data: any, operation: string): any {
  return {
    ...data,
    _performance: {
      ...(data._performance || {}),
      [operation]: {
        timestamp: Date.now(),
        operation,
        moduleId: this.id,
        protocol: data._metadata?.originalProtocol
      }
    }
  };
}
```

### è½¬æ¢æ€§èƒ½ç›‘æ§
```typescript
// è½¬æ¢æ€§èƒ½ç»Ÿè®¡
const conversionStats = {
  conversionTime: endTime - startTime,
  inputSize: JSON.stringify(request).length,
  outputSize: JSON.stringify(transformed).length,
  protocol: detectedProtocol,
  hasTools: !!request.tools,
  messageCount: this.getMessageCount(request)
};
```

## ğŸš¨ é”™è¯¯å¤„ç†

### åè®®éªŒè¯é”™è¯¯
```typescript
// Responses API éªŒè¯
if (protocol === 'responses') {
  if (!request.input || !Array.isArray(request.input)) {
    throw new Error('Invalid Responses protocol: input must be an array');
  }
}

// å·¥å…·æ ¼å¼éªŒè¯
if (request.tools && !this.validateTools(request.tools)) {
  throw new Error('Invalid tool format in request');
}
```

### è½¬æ¢é”™è¯¯å¤„ç†
```typescript
// è½¬æ¢é”™è¯¯è®°å½•å’Œæ¢å¤
try {
  const transformed = await this.transformRequest(request, protocol);
} catch (error) {
  this.logger.logModule(this.id, 'transform-error', {
    error: error instanceof Error ? error.message : String(error),
    protocol,
    requestType: this.inferRequestType(request, protocol)
  });

  // å°è¯•é™çº§å¤„ç†
  return this.handleTransformError(request, error);
}
```

## ğŸ” è°ƒè¯•æ”¯æŒ

### è¯¦ç»†æ—¥å¿—è®°å½•
```typescript
// è¯·æ±‚è½¬æ¢æ—¥å¿—
this.logger.logTransformation(this.id, 'responses-to-chat', request, transformed);

// å“åº”è½¬æ¢æ—¥å¿—
this.logger.logTransformation(this.id, 'chat-to-responses', response, converted);

// æµå¼äº‹ä»¶æ—¥å¿—
this.logger.logModule(this.id, 'stream-event', {
  eventType: event.type,
  itemId: event.data.item_id,
  sequenceNumber: event.data.sequence_number
});
```

### è°ƒè¯•ä¿¡æ¯
```typescript
// å®Œæ•´çš„è°ƒè¯•ä¸Šä¸‹æ–‡
const debugInfo = {
  sessionId: request._metadata?.sessionId,
  moduleId: this.id,
  operationId: 'llmswitch_transform',
  timestamp: Date.now(),
  type: 'transform',
  position: 'middle',
  data: {
    original: request,
    transformed: transformed,
    metadata: transformed._metadata,
    protocol: detectedProtocol,
    conversionStats
  }
};
```

## ğŸŒ API ç«¯ç‚¹æ”¯æŒ

### æ”¯æŒçš„ç«¯ç‚¹
- **`/v1/chat/completions`** - OpenAI Chat Completions API
- **`/v1/responses`** - OpenAI Responses API â­
- **`/v1/messages`** - Anthropic Messages API
- **`/v1/completions`** - OpenAI Completions API

### ç«¯ç‚¹æ˜ å°„
```typescript
const endpointMappings = {
  '/v1/responses': {
    entryProtocol: 'responses',
    switchType: 'llmswitch-response-chat',
    targetProtocol: 'openai'
  },
  '/v1/chat/completions': {
    entryProtocol: 'openai',
    switchType: 'llmswitch-openai-openai',
    targetProtocol: 'openai'
  },
  '/v1/messages': {
    entryProtocol: 'anthropic',
    switchType: 'llmswitch-anthropic-openai',
    targetProtocol: 'openai'
  }
};
```

## ğŸ”§ æ‰©å±•æ€§

### æ·»åŠ æ–°çš„ LLMSwitch å®ç°
```typescript
class NewProtocolLLMSwitch implements LLMSwitchModule {
  readonly type = 'llmswitch-new-protocol';
  readonly protocol = 'new-protocol';

  async processIncoming(request: any): Promise<any> {
    const context = this.captureRequestContext(request);
    const transformed = this.transformRequest(request, context);

    return {
      ...transformed,
      _metadata: {
        switchType: this.type,
        timestamp: Date.now(),
        entryProtocol: this.protocol,
        targetProtocol: 'openai',
        ...context
      }
    };
  }

  async processOutgoing(response: any): Promise<any> {
    const context = this.extractResponseContext(response);
    return this.transformResponse(response, context);
  }
}
```

### è‡ªå®šä¹‰åè®®è½¬æ¢å™¨
```typescript
class CustomProtocolConverter {
  async convertRequest(request: any, targetProtocol: string): Promise<any> {
    // è‡ªå®šä¹‰è¯·æ±‚è½¬æ¢é€»è¾‘
    switch (targetProtocol) {
      case 'openai':
        return this.convertToOpenAI(request);
      case 'anthropic':
        return this.convertToAnthropic(request);
      case 'responses':
        return this.convertToResponses(request);
      default:
        throw new Error(`Unsupported target protocol: ${targetProtocol}`);
    }
  }
}
```

## ğŸ“ˆ ç‰ˆæœ¬ä¿¡æ¯

- **å½“å‰ç‰ˆæœ¬**: 2.0.0
- **æ–°å¢ç‰¹æ€§**: Responses API å®Œæ•´æ”¯æŒ
- **å…¼å®¹æ€§**: RouteCodex Pipeline >= 2.0.0
- **TypeScript**: >= 5.0.0
- **Node.js**: >= 18.0.0

## ğŸ”— ä¾èµ–å…³ç³»

- **rcc-debugcenter**: è°ƒè¯•ä¸­å¿ƒé›†æˆ
- **PipelineDebugLogger**: æ¨¡å—æ—¥å¿—è®°å½•
- **ErrorHandlingCenter**: é”™è¯¯å¤„ç†é›†æˆ
- **DebugEventBus**: äº‹ä»¶æ€»çº¿é€šä¿¡
- **BaseModule**: åŸºç¡€æ¨¡å—æ¥å£

## âœ¨ æ–°ç‰¹æ€§ (v2.0.0)

### ğŸ†• Responses API æ”¯æŒ
- å®Œæ•´çš„ Responses â†” Chat æ ¼å¼è½¬æ¢
- æ”¯æŒæ‰€æœ‰ Responses API å­—æ®µå’ŒåŠŸèƒ½
- å·¥å…·è°ƒç”¨å®Œæ•´æ”¯æŒ
- æµå¼äº‹ä»¶å¤„ç†

### ğŸ”§ å¢å¼ºçš„åè®®æ£€æµ‹
- è‡ªåŠ¨æ£€æµ‹è¾“å…¥åè®®ç±»å‹
- æ™ºèƒ½è½¬æ¢ç­–ç•¥é€‰æ‹©
- é”™è¯¯æ¢å¤æœºåˆ¶

### ğŸ“Š æ”¹è¿›çš„è°ƒè¯•åŠŸèƒ½
- è¯¦ç»†çš„è½¬æ¢æ—¥å¿—
- æ€§èƒ½ç»Ÿè®¡
- åè®®è½¬æ¢å¯è§†åŒ–

## ğŸš€ æ›´æ–°æ—¥å¿—

### v2.0.0 (2025-10-17)
- âœ¨ æ–°å¢ `llmswitch-response-chat` è½¬æ¢å™¨
- ğŸ”„ å®Œæ•´çš„ Responses API æ”¯æŒ
- ğŸ“Š æ”¹è¿›çš„æ€§èƒ½è·Ÿè¸ªå’Œè°ƒè¯•åŠŸèƒ½
- ğŸ›¡ï¸ å¢å¼ºçš„åè®®éªŒè¯å’Œé”™è¯¯å¤„ç†
- ğŸ“š å®Œæ•´çš„æ–‡æ¡£æ›´æ–°

### v1.5.0 (2025-01-22)
- ğŸ”§ å®Œå–„ Anthropic-OpenAI è½¬æ¢
- ğŸ“Š æ–°å¢æ€§èƒ½è·Ÿè¸ªåŠŸèƒ½
- ğŸ›¡ï¸ æ”¹è¿›é”™è¯¯å¤„ç†æœºåˆ¶
- ğŸ“š å®Œå–„æ–‡æ¡£å’Œè°ƒè¯•æ”¯æŒè¯´æ˜

## ğŸ”® æœªæ¥è®¡åˆ’

### v2.1.0 è®¡åˆ’
- ğŸ¤– Google Gemini åè®®æ”¯æŒ
- ğŸ”„ å®æ—¶æµå¼åè®®è½¬æ¢
- ğŸ“Š åè®®è½¬æ¢æ€§èƒ½ä¼˜åŒ–
- ğŸ§ª æ›´å¤šçš„åè®®æµ‹è¯•è¦†ç›–

### é•¿æœŸè§„åˆ’
- ğŸŒ æ›´å¤šåè®®æ”¯æŒ (Cohere, Mistral ç­‰)
- ğŸ”„ åè®®ç‰ˆæœ¬ç®¡ç†
- ğŸ§  æ™ºèƒ½åè®®è½¬æ¢ç­–ç•¥
- ğŸ“Š åè®®è½¬æ¢åˆ†æå’ŒæŠ¥å‘Š

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹è°ƒè¯•æ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯
2. æ£€æŸ¥åè®®æ ¼å¼æ˜¯å¦ç¬¦åˆè§„èŒƒ
3. éªŒè¯é…ç½®æ–‡ä»¶è®¾ç½®
4. å‚è€ƒæœ¬æ–‡æ¡£çš„ä½¿ç”¨ç¤ºä¾‹

---

**æœ€åæ›´æ–°**: 2025-10-17 - æ–°å¢ Responses API æ”¯æŒæ–‡æ¡£
