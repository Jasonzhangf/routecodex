# LLMSwitch æ¨¡å—

LLMSwitch æ¨¡å—æä¾›åè®®è½¬æ¢åŠŸèƒ½ï¼Œå°†ä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹APIåè®®è¿›è¡Œç›¸äº’è½¬æ¢ï¼Œç›®å‰ä¸»è¦ä¸“æ³¨äº OpenAI åè®®çš„é€ä¼ å’Œè½¬æ¢ã€‚

## æ¨¡å—æ¦‚è¿°

LLMSwitch æ¨¡å—æ˜¯æµæ°´çº¿æ¶æ„çš„ç¬¬ 0 å±‚ï¼ˆå…¥å£å±‚ï¼‰ï¼Œè´Ÿè´£å¤„ç†è¿›å…¥æµæ°´çº¿çš„ç¬¬ä¸€ä¸ªåè®®è½¬æ¢æ­¥éª¤ã€‚å®ƒåˆ†æä¼ å…¥è¯·æ±‚çš„åè®®ç±»å‹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç›®æ ‡ä¾›åº”å•†æ‰€æœŸæœ›çš„åè®®æ ¼å¼ã€‚

## æ”¯æŒçš„åè®®è½¬æ¢

### ğŸ”§ OpenAI â†’ OpenAI è§„èŒƒåŒ–
- **å®ç°æ–‡ä»¶**: `llmswitch-openai-openai.ts`
- **åŠŸèƒ½**: OpenAI åè®®è§„èŒƒåŒ–ï¼Œä¿æŒè¯·æ±‚ç»“æ„ä¸€è‡´
- **ç‰¹æ€§**:
  - å®Œæ•´çš„ OpenAI åè®®æ”¯æŒ
  - è¯·æ±‚/å“åº”å…ƒæ•°æ®æ·»åŠ 
  - æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•ä¿¡æ¯
  - åè®®éªŒè¯å’Œæ ‡å‡†åŒ–
  - é”™è¯¯ä¸Šä¸‹æ–‡å¢å¼º

### ğŸ”„ Anthropic-OpenAI è½¬æ¢å™¨
- **å®ç°æ–‡ä»¶**: `anthropic-openai-converter.ts`
- **åŠŸèƒ½**: Anthropic åè®®ä¸ OpenAI åè®®äº’è½¬
- **ç‰¹æ€§**:
  - æ¶ˆæ¯æ ¼å¼è½¬æ¢
  - å‚æ•°æ˜ å°„
  - å·¥å…·è°ƒç”¨é€‚é…
  - å“åº”æ ¼å¼æ ‡å‡†åŒ–

## æ ¸å¿ƒåŠŸèƒ½

### ğŸ¯ åè®®é€ä¼ 
```typescript
// OpenAI è§„èŒƒåŒ–å®ç°
class OpenAINormalizerLLMSwitch implements LLMSwitchModule {
  async processIncoming(request: any): Promise<any> {
    // æ·»åŠ å…ƒæ•°æ®ä½†ä¿æŒåè®®ä¸å˜
    return {
      ...request,
      _metadata: {
        switchType: 'llmswitch-openai-openai',
        timestamp: Date.now(),
        originalProtocol: 'openai',
        targetProtocol: 'openai'
      }
    };
  }
}
```

### ğŸ“Š å…ƒæ•°æ®å¢å¼º
```typescript
// è¯·æ±‚å…ƒæ•°æ®æå–
private extractRequestMetadata(request: any): Record<string, any> {
  return {
    timestamp: Date.now(),
    hasModel: !!request.model,
    hasMessages: !!request.messages,
    hasTools: !!request.tools,
    hasStream: !!request.stream,
    messageCount: request.messages?.length || 0,
    toolCount: request.tools?.length || 0,
    requestType: this.inferRequestType(request)
  };
}
```

### ğŸ›¡ï¸ åè®®éªŒè¯
```typescript
// åè®®éªŒè¯
private validateProtocol(request: any): void {
  if (!request.messages && !request.prompt) {
    throw new Error('Invalid OpenAI protocol: missing messages or prompt');
  }
  
  if (request.messages && !Array.isArray(request.messages)) {
    throw new Error('Invalid OpenAI protocol: messages must be an array');
  }
}
```

## æ–‡ä»¶ç»“æ„

```
src/modules/pipeline/modules/llm-switch/
â”œâ”€â”€ llmswitch-openai-openai.ts    # OpenAI â†’ OpenAI è§„èŒƒåŒ–å®ç°
â”œâ”€â”€ anthropic-openai-converter.ts # Anthropic â†’ OpenAI è½¬æ¢å™¨
â”œâ”€â”€ anthropic-openai-config.ts    # è½¬æ¢é…ç½®
â””â”€â”€ README.md                     # æœ¬æ–‡æ¡£
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨
```typescript
import { OpenAINormalizerLLMSwitch } from './llmswitch-openai-openai.js';

const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: {
    enableValidation: true,
    enableMetadata: true
  }
}, dependencies);

await llmSwitch.initialize();

// å¤„ç† OpenAI è¯·æ±‚
const enhancedRequest = await llmSwitch.processIncoming({
  model: 'gpt-4',
  messages: [
    { role: 'user', content: 'Hello!' }
  ],
  tools: [/* å·¥å…·å®šä¹‰ */],
  stream: false
});

// ç»“æœåŒ…å«å¢å¼ºçš„å…ƒæ•°æ®
console.log(enhancedRequest._metadata);
// {
//   switchType: 'llmswitch-openai-openai',
//   timestamp: 1643723400000,
//   originalProtocol: 'openai',
//   targetProtocol: 'openai'
// }
```

### åœ¨æµæ°´çº¿ä¸­ä½¿ç”¨
```typescript
const pipelineConfig = {
  modules: {
    llmSwitch: {
      type: 'llmswitch-openai-openai',
      config: {
        enableValidation: true,
        enablePerformanceTracking: true
      }
    }
  }
};

// è¯·æ±‚å¢å¼º
const enhancedRequest = await llmSwitch.processIncoming(request);
// åŒ…å«å®Œæ•´çš„è°ƒè¯•å’Œæ€§èƒ½å…ƒæ•°æ®
```

### åè®®è½¬æ¢ç¤ºä¾‹
```typescript
// Anthropic åˆ° OpenAI è½¬æ¢
import { AnthropicOpenAIConverter } from './anthropic-openai-converter.js';

const converter = new AnthropicOpenAIConverter({
  type: 'llmswitch-anthropic-openai',
  config: {
    direction: 'anthropic-to-openai',
    enableTools: true
  }
}, dependencies);

await converter.initialize();

// Anthropic æ ¼å¼è¯·æ±‚
const anthropicRequest = {
  model: 'claude-3-sonnet',
  messages: [
    { role: 'user', content: 'Hello!' }
  ],
  max_tokens: 1000
};

// è½¬æ¢ä¸º OpenAI æ ¼å¼
const openAIRequest = await converter.transformRequest(anthropicRequest);
```

## é…ç½®é€‰é¡¹

### OpenAI é€ä¼ é…ç½®
```typescript
interface OpenAIPassthroughConfig {
  enableValidation?: boolean;        // å¯ç”¨åè®®éªŒè¯
  enableMetadata?: boolean;          // å¯ç”¨å…ƒæ•°æ®å¢å¼º
  enablePerformanceTracking?: boolean; // å¯ç”¨æ€§èƒ½è·Ÿè¸ª
  maxLogEntries?: number;            // æœ€å¤§æ—¥å¿—æ¡ç›®æ•°
}
```

### Anthropic-OpenAI è½¬æ¢é…ç½®
```typescript
interface AnthropicOpenAIConfig {
  direction: 'anthropic-to-openai' | 'openai-to-anthropic'; // è½¬æ¢æ–¹å‘
  enableTools?: boolean;             // å¯ç”¨å·¥å…·è½¬æ¢
  enableStreaming?: boolean;         // å¯ç”¨æµå¼è½¬æ¢
  modelMappings?: Record<string, string>; // æ¨¡å‹æ˜ å°„
}
```

### è¯·æ±‚æ ¼å¼æ£€æµ‹é…ç½®
```typescript
interface RequestFormatDetectorConfig {
  confidenceThreshold?: number;      // ç½®ä¿¡åº¦é˜ˆå€¼
  supportedFormats?: string[];       // æ”¯æŒçš„æ ¼å¼åˆ—è¡¨
  enableValidation?: boolean;        // å¯ç”¨æ ¼å¼éªŒè¯
}
```

## è¯·æ±‚ç±»å‹æ¨æ–­

### æ”¯æŒçš„è¯·æ±‚ç±»å‹
```typescript
type RequestType = 
  | 'chat'           // èŠå¤©å®Œæˆ
  | 'completion'     // æ–‡æœ¬å®Œæˆ
  | 'embedding'      // æ–‡æœ¬åµŒå…¥
  | 'tool'           // å·¥å…·è°ƒç”¨
  | 'moderation'     // å†…å®¹å®¡æ ¸
  | 'unknown';       // æœªçŸ¥ç±»å‹
```

### ç±»å‹æ¨æ–­é€»è¾‘
```typescript
private inferRequestType(request: any): RequestType {
  if (request.messages) {
    return 'chat';
  } else if (request.prompt) {
    return 'completion';
  } else if (request.input) {
    return 'embedding';
  } else if (request.tools) {
    return 'tool';
  }
  return 'unknown';
}
```

## æ€§èƒ½è·Ÿè¸ª

### æ€§èƒ½å…ƒæ•°æ®
```typescript
private addPerformanceMetadata(data: any, operation: string): any {
  return {
    ...data,
    _performance: {
      ...(data._performance || {}),
      [operation]: {
        timestamp: Date.now(),
        operation,
        moduleId: this.id
      }
    }
  };
}
```

### å“åº”æ€§èƒ½è·Ÿè¸ª
```typescript
// å“åº”æ€§èƒ½æ•°æ®
const responseMetadata = {
  hasChoices: !!response.choices,
  hasUsage: !!response.usage,
  choiceCount: response.choices?.length || 0,
  usage: response.usage ? {
    promptTokens: response.usage.prompt_tokens,
    completionTokens: response.usage.completion_tokens,
    totalTokens: response.usage.total_tokens
  } : null
};
```

## é”™è¯¯å¤„ç†

### åè®®éªŒè¯é”™è¯¯
```typescript
// åè®®éªŒè¯å¤±è´¥
if (!request.messages && !request.prompt) {
  throw new Error('Invalid OpenAI protocol: missing messages or prompt');
}

// æ¶ˆæ¯æ ¼å¼é”™è¯¯
if (request.messages && !Array.isArray(request.messages)) {
  throw new Error('Invalid OpenAI protocol: messages must be an array');
}
```

### è½¬æ¢é”™è¯¯å¤„ç†
```typescript
// è½¬æ¢é”™è¯¯è®°å½•
try {
  const transformed = await this.transformRequest(request);
} catch (error) {
  this.logger.logModule(this.id, 'transform-error', {
    error: error instanceof Error ? error.message : String(error),
    requestType: this.inferRequestType(request)
  });
  throw error;
}
```

## è°ƒè¯•æ”¯æŒ

### è¯¦ç»†æ—¥å¿—è®°å½•
```typescript
// è¯·æ±‚è½¬æ¢æ—¥å¿—
this.logger.logTransformation(this.id, 'llmswitch-request-transform', request, transformed);

// å“åº”è½¬æ¢æ—¥å¿—
this.logger.logTransformation(this.id, 'llmswitch-response-transform', response, transformed);

// é”™è¯¯æ—¥å¿—
this.logger.logModule(this.id, 'request-transform-error', { error, request });
```

### è°ƒè¯•ä¿¡æ¯åŒ…å«
```typescript
// å®Œæ•´çš„è°ƒè¯•ä¸Šä¸‹æ–‡
const debugInfo = {
  sessionId: request._metadata?.sessionId,
  moduleId: this.id,
  operationId: 'llmswitch_transform',
  timestamp: Date.now(),
  type: 'transform',
  position: 'middle',
  data: {
    original: request,
    transformed: transformed,
    metadata: transformed._metadata
  }
};
```

## æ‰©å±•æ€§

### æ·»åŠ æ–°çš„ LLMSwitch å®ç°
```typescript
class NewLLMSwitch implements LLMSwitchModule {
  readonly type = 'new-protocol';
  
  async processIncoming(request: any): Promise<any> {
    // å®ç°æ–°çš„åè®®è½¬æ¢é€»è¾‘
    return {
      ...request,
      _metadata: {
        switchType: this.type,
        timestamp: Date.now(),
        originalProtocol: 'original',
        targetProtocol: 'target'
      }
    };
  }
  
  async processOutgoing(response: any): Promise<any> {
    // å®ç°å“åº”è½¬æ¢é€»è¾‘
    return response;
  }
}
```

### è‡ªå®šä¹‰åè®®è½¬æ¢
```typescript
// æ³¨å†Œæ–°çš„è½¬æ¢å™¨
class CustomProtocolConverter {
  async convertRequest(request: any): Promise<any> {
    // è‡ªå®šä¹‰è¯·æ±‚è½¬æ¢é€»è¾‘
  }
  
  async convertResponse(response: any): Promise<any> {
    // è‡ªå®šä¹‰å“åº”è½¬æ¢é€»è¾‘
  }
}
```

## å·²çŸ¥é™åˆ¶

### âŒ å½“å‰é™åˆ¶
1. **åè®®æ”¯æŒæœ‰é™** - ä¸»è¦æ”¯æŒ OpenAI åè®®é€ä¼ 
2. **æ— å®æ—¶è½¬æ¢** - ä¸æ”¯æŒå®æ—¶æµå¼åè®®è½¬æ¢
3. **æ— å¤šåè®®æ··åˆ** - ä¸æ”¯æŒåŒä¸€è¯·æ±‚ä¸­çš„å¤šåè®®æ··åˆ
4. **æ— åè®®ç‰ˆæœ¬æ£€æµ‹** - ä¸æ£€æµ‹åè®®ç‰ˆæœ¬å·®å¼‚

### ğŸ”„ æœªæ¥è®¡åˆ’
1. **å¤šåè®®æ”¯æŒ** - æ·»åŠ  Anthropicã€Google ç­‰åè®®æ”¯æŒ
2. **å®æ—¶è½¬æ¢** - æ”¯æŒæµå¼æ•°æ®çš„å®æ—¶åè®®è½¬æ¢
3. **åè®®ç‰ˆæœ¬ç®¡ç†** - æ”¯æŒä¸åŒç‰ˆæœ¬çš„åè®®è½¬æ¢
4. **æ™ºèƒ½åè®®æ£€æµ‹** - è‡ªåŠ¨æ£€æµ‹å’Œé€‰æ‹©æœ€ä½³è½¬æ¢ç­–ç•¥

## ç‰ˆæœ¬ä¿¡æ¯

- **å½“å‰ç‰ˆæœ¬**: 1.0.0
- **å…¼å®¹æ€§**: RouteCodex Pipeline >= 1.0.0
- **TypeScript**: >= 5.0.0
- **Node.js**: >= 18.0.0

## ä¾èµ–å…³ç³»

- **rcc-debugcenter**: è°ƒè¯•ä¸­å¿ƒé›†æˆ
- **PipelineDebugLogger**: æ¨¡å—æ—¥å¿—è®°å½•
- **ErrorHandlingCenter**: é”™è¯¯å¤„ç†é›†æˆ
- **DebugEventBus**: äº‹ä»¶æ€»çº¿é€šä¿¡

## æœ€åæ›´æ–°

2025-01-22 - å®Œå–„åè®®è½¬æ¢æ–‡æ¡£å’Œè°ƒè¯•æ”¯æŒè¯´æ˜
