# RouteCodex - RouteCodex Claude Code Router

[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)
[![Architecture](https://img.shields.io/badge/Architecture-4--layer-purple.svg)](./ARCHITECTURE_DOCUMENTATION.md)

A sophisticated AI service routing and transformation system that provides seamless integration with multiple AI providers through a clean, modular architecture.

Note: Node.js engines updated — requires Node >=20 and <26. Global installs on Node 24 are supported.

Release 0.41.1
- Fix: Anthropic streaming compliance for tool_use. RouteCodex now emits standard SSE events for tool inputs so Claude Code correctly accumulates parameters during streaming tool calls.

Important: Anthropic tool-calls (since 0.4.0)
- By default we trust tool schemas (trustSchema=true). When converting OpenAI `tool_calls` to Anthropic `tool_use`, we preserve `function.name` and `arguments` as-is without renaming tool names or remapping argument fields. This aligns with Claude Code Router and guarantees correct tool flow (emitting `stop_reason=tool_use` on `message_delta`).

## ✅ GLM‑4.6 Works With Codex CLI

RouteCodex now supports running GLM‑4.6 with Codex CLI through the OpenAI‑compatible endpoint. Highlights:

- Wire API: `chat` (Codex requires chat protocol)
- Tool calls: reconstructed in Compatibility (no tool text leakage)
- Thinking: enabled for GLM‑4.6; private <think>…</think> is stripped from output while preserving model reasoning behavior
- SSE: router emits `delta.tool_calls` when needed; content is suppressed to keep tool calls at the end

Quick setup

1) Start RouteCodex (default port 5520)

```bash
npm run dev
# OpenAI endpoint: http://localhost:5520/v1/openai
```

Singleton server (avoid port conflicts)

- Background (recommended): `npm run start:bg` uses `scripts/run-bg.sh`, which now checks the target port (default 5520 or `~/.routecodex/config.json:port`).
  - If another instance is listening, the command exits with code 9 and a hint to use `--replace`.
  - To force replace: `bash scripts/run-bg.sh --replace -- 'node dist/index.js'` (or set `ROUTECODEX_PORT`/`--port <n>` when not using default).

- Foreground (timed): `npm run start:fg` uses `scripts/run-fg-gtimeout.sh <timeout> -- ...`, which also supports `--replace` and `--port`.
  - Example: `bash scripts/run-fg-gtimeout.sh 12 --replace -- 'node dist/index.js'`.

This prevents accidental coexistence of a dev worktree server and a globally installed server on the same port.

2) Ensure user config includes GLM provider (auto‑generated by default at `~/.routecodex/config.json`):

```json
{
  "virtualrouter": {
    "providers": {
      "glm": {
        "type": "glm",
        "baseURL": "https://open.bigmodel.cn/api/coding/paas/v4",
        "apiKey": ["<YOUR_GLM_KEY>"],
        "models": {
          "glm-4.6": {
            "compatibility": { "type": "glm-compatibility", "config": { "thinking": { "enabled": true, "payload": { "type": "enabled" } } } }
          }
        }
      }
    },
    "routing": { "default": ["glm.glm-4.6"] }
  },
  "port": 5520
}
```

3) Configure Codex profile (`~/.codex/config.toml`):

```toml
[model_providers.rc]
name = "glm"
base_url = "http://localhost:5520/v1/openai"
wire_api = "chat"
api_key = "<YOUR_GLM_KEY>"

[profiles.rc]
model_provider = "rc"
model = "glm-4.6"
```

4) Test with Codex

```bash
codex -p rc "你好，列出当前目录 (触发工具调用)"
```

Notes
- Compatibility converts `<invoke …>`, `<function …>`, `<function_call>{…}</function_call>` and `[tool_call:name]{…}` into OpenAI `tool_calls`.
- Private reasoning blocks `<think>…</think>` are removed from the visible content to keep OpenAI/Codex semantics intact.

## 🧭 Responses Module (Design Overview)

We decouple “conversion” from “streaming” and drive both via JSON configuration.

- Conversion (Requests/Responses)
  - Requests: Convert OpenAI Responses input (nested `input[]` with `message` + `content[{type,text}]`) into Chat messages. Expansion is recursive and controlled by JSON mapping (no hard-coding).
  - Responses: Convert provider Chat payloads back to OpenAI Responses JSON (text, tools, usage). llmswitch preferred; fallback mapper based on JSON.

- Streaming (SSE)
  - SSE only reads normalized Responses objects and emits `response.*` events. Options for message lifecycle, required_action, and heartbeat are configurable.

- Where to configure
  - Module switches: `config/modules.json` → `responses` section (useLlmswitch, fallback, provider non-stream, SSE lifecycle/required_action/heartbeat)
  - Field mappings: `config/responses-conversion.json` (wrapper/type/text/blocks keys; allowed content types; text extraction paths; tool args paths)
  - Env overrides: `ROUTECODEX_RESP_*` variables

- Files
  - `src/server/config/responses-config.ts` — load module config + mappings
  - `src/server/conversion/responses-converter.ts` — conversions (Responses↔Chat)
  - `src/server/handlers/responses.ts` — handler: calls converter, then streams SSE based on config

- Debug artifacts
  - `~/.routecodex/codex-samples/anth-replay/raw-request_req_<RID>.json`
  - `~/.routecodex/codex-samples/anth-replay/pre-pipeline_req_<RID>.json`
  - `~/.routecodex/codex-samples/anth-replay/sse-events-req_<RID>.log`

### 🔬 Responses SSE Implementation Details

This router aligns its SSE output with the OpenAI Responses API event model. Key details:

- Event order (tool‑first typical path)
  1) `response.created` → 2) `response.in_progress`
  3) `response.output_item.added`(reasoning, `output_index=0`)
  4) `response.reasoning_summary_part.added`(summary_index=0)
  5) multiple `response.reasoning_summary_text.delta` (with `obfuscation` placeholder)
  6) `response.reasoning_summary_text.done` → `response.reasoning_summary_part.done`
  7) repeat 4–6 (summary_index=1)
  8) `response.output_item.added`(function_call, `output_index=2`)
  9) `response.content_part.added`(input_json)
  10) multiple `response.function_call_arguments.delta`
  11) `response.function_call_arguments.done`
  12) `response.output_item.done`(function_call)
  13) `response.completed`

- Event order (text‑first): insert message lifecycle between reasoning and function_call
  - `response.output_item.added`(message, `output_index=1`) → `response.content_part.added` → multiple `response.output_text.delta` → `response.output_text.done` → `response.content_part.done` → `response.output_item.done`(message)

- Required fields and indices
  - All events carry `sequence_number`, starting at 0 and incrementing by 1 per event.
  - `created_at` (seconds) is used for `response.{created|in_progress|completed}.response.created_at`.
  - `output_index` mapping is fixed: `0=reasoning`, `1=message`, `2=function_call`; `content_index` is 0.
  - Reasoning:
    - `response.output_item.added`(reasoning) → `item` includes `{ id, type: "reasoning", encrypted_content, summary: [] }`.
    - `response.reasoning_summary_part.added/done` and `response.reasoning_summary_text.delta/done` include `item_id/output_index/summary_index`; deltas include `obfuscation` placeholder.
  - Message:
    - `response.output_item.added`(message) → `item` includes `{ id, type: "message", role: "assistant", status: "in_progress", content: [] }`.
    - `response.content_part.added`(message) → `part: { type: "output_text", annotations: [], logprobs: [], text: "" }`.
    - `response.output_text.delta/done` include `item_id/output_index/content_index/logprobs` (empty arrays are valid).
  - Function call:
    - `response.output_item.added`(function_call) → `item` includes `{ id, type: "function_call", call_id, name, status: "in_progress", arguments: "" }`.
    - `response.content_part.added`(function_call) → `part: { type: "input_json", partial_json: "" }`.
    - `response.function_call_arguments.delta` → `item_id/output_index/delta`; `done` → `item_id/output_index/arguments/name`.
    - `response.output_item.done`(function_call) → `{ status: "completed", arguments, call_id, name }`.
  - `response.completed` → `response` contains `output` array (reasoning/message/function_call in order) and `usage.input_tokens|output_tokens|total_tokens`. No top‑level `usage` or `required_action` in events.

- Message lifecycle emission policy
  - Message lifecycle is emitted only when there is actual text delta; in tool‑first turns, no empty message skeleton is sent. Tool‑first detection recognizes `function_call/tool_call/tool_use`.

- Not emitted
  - `response.required_action` is not emitted in Responses SSE.

- Tool execution
  - The server never executes tools; the client executes tools and decides follow‑up turns.

- Heartbeat
  - Controlled by `responses.sse.heartbeatMs` (or env `ROUTECODEX_RESPONSES_HEARTBEAT_MS`). When >0, the router sends SSE heartbeats to keep connections alive.

- Mapping and configuration
  - Request mapping (Responses→Chat) is driven by `llmswitch-response-chat` and `config/responses-conversion.json`.
  - Response mapping (Chat→Responses) emits `created_at` and `function_call`; the SSE layer replays events as specified above.
  - Implementation references:
    - Event replay: `src/server/handlers/responses.ts`
    - Mapping: `src/modules/pipeline/modules/llmswitch/llmswitch-response-chat.ts`, `src/server/conversion/responses-mapper.ts`
    - Config: `config/responses-conversion.json`, `src/server/config/responses-config.ts`

## 🏗️ 4-Layer Architecture

RouteCodex implements a sophisticated 4-layer pipeline architecture that provides clean separation of concerns and flexible protocol handling:

```
HTTP Request → LLM Switch → Workflow → Compatibility → Provider → AI Service
     ↓             ↓          ↓            ↓           ↓
  Request      Protocol   Flow       Format     Standard     Response
  Analysis     Routing    Control    Conversion   HTTP Server  Processing
```

### Layer 1: LLM Switch (Dynamic Routing + Standardization)
- **Request Analysis**: Analyzes incoming requests to determine optimal routing
- **Protocol Routing**: Routes requests to appropriate processing pipelines
- **OpenAI Standardization**: Enforces strict OpenAI Chat Completions shape before downstream modules
  - `assistant.tool_calls[].function.arguments` must be a JSON string (objects are stringified)
  - `assistant.content` must be a string (default "")
  - `tool.content` must be a string
- **Dynamic Classification**: Supports 7 routing categories (default, longcontext, thinking, background, websearch, vision, coding)

### Layer 2: Workflow (Flow Control)
- **Streaming Control**: Handles streaming/non-streaming request conversion
- **Request Processing**: Manages request flow and processing state
- **Response Handling**: Processes and transforms responses as needed

### Layer 3: Compatibility (Format Transformation - Vendor Differences Only)
- **Protocol Translation**: Converts between different AI service protocols
- **Format Adaptation**: Transforms request/response formats between providers
- **Tool Integration**: Handles tool calling format conversion and execution
- **Configuration-Driven**: Uses JSON configuration for transformation rules
- **Scope Rule**: Compatibility only handles vendor-specific differences. All “standard OpenAI” normalization lives in the LLM Switch.
- **Simple String Format**: Supports easy compatibility specification (e.g., `"compatibility": "passthrough"`)

### Passive Monitoring (Design‑Ready, not enabled by default)
- A passive Monitoring skeleton is included to capture request/response artifacts for OpenAI and Anthropic without changing behavior.
- Records (when enabled in future) are stored under `~/.routecodex/monitor/sessions/<YYYYMMDD>/<protocol>/<reqId>/` and can be used for:
  - Offline routing dry‑run (simulate routing decisions from recorded inputs)
  - Response replay (non‑stream/stream events)
- See `docs/monitoring/Design.md` for details.

### Virtual Router Dry‑Run (Matrix)
- Run a local matrix dry‑run against your config (no provider calls):

```bash
npm run build
node scripts/virtualrouter-dry-run-matrix.mjs --config ~/.routecodex/config/verified_0.46.32/multi-provider.json
```

## 🔧 Compatibility Field Configuration

The compatibility field supports both simple string format and complex object format, providing flexible configuration options for different use cases.

### Simple String Format

For basic usage, you can specify compatibility as a simple string:

```json
{
  "compatibility": "passthrough"
}
```

#### Supported String Values:
- `"passthrough"` - Direct pass-through without transformation (default)
- `"lmstudio"` - LM Studio compatibility mode
- `"qwen"` - Qwen compatibility mode
- `"iflow"` - iFlow compatibility mode
- Multiple providers: `"lmstudio/qwen"` - Will use first available provider

### Complex Object Format

For advanced configuration, use the complex object format:

```json
{
  "compatibility": {
    "type": "lmstudio-compatibility",
    "config": {
      "toolsEnabled": true,
      "customRules": [
        {
          "id": "ensure-standard-tools-format",
          "transform": "mapping",
          "sourcePath": "tools",
          "targetPath": "tools",
          "mapping": {
            "type": "type",
            "function": "function"
          }
        }
      ]
    }
  }
}
```

### Priority Hierarchy

The system follows this priority order when determining compatibility:

1. **User Config Compatibility** (highest priority)
2. **Model-Level Compatibility**
3. **Provider-Level Compatibility**
4. **Auto-Inference** (fallback based on provider type)

> Note on inference: To keep “configuration as the single source of truth”, prefer setting `compatibility` explicitly (default is `passthrough`). Inference may be removed in future revisions.

### Configuration Examples

#### Basic Passthrough Configuration
```json
{
  "server": {
    "port": 5506
  },
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234",
      "apiKey": "your-api-key"
    }
  },
  "compatibility": "passthrough"
}
```

#### Multi-Provider Compatibility
```json
{
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234"
    },
    "qwen": {
      "type": "qwen",
      "baseUrl": "https://chat.qwen.ai"
    }
  },
  "compatibility": "lmstudio/qwen"
}
```

#### Advanced Configuration with Custom Rules
```json
{
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234"
    }
  },
  "compatibility": {
    "type": "lmstudio-compatibility",
    "config": {
      "toolsEnabled": true,
      "customRules": [
        {
          "id": "tool-format-conversion",
          "transform": "mapping",
          "sourcePath": "tools",
          "targetPath": "tools",
          "mapping": {
            "type": "type",
            "function": "function"
          }
        }
      ]
    }
  }
}
```

### Layer 4: Provider (Standard HTTP Server)
- **HTTP Communication**: Manages all HTTP communications with AI services
- **Authentication**: Handles provider authentication and authorization
- **Error Handling**: Manages network errors and provider-specific issues

## ✨ Key Features

### 🔧 Tool Calling Support
- Full OpenAI-compatible tool calling implementation
- Supports multiple AI providers (LM Studio, OpenAI, Qwen, iFlow, Anthropic)
- Automatic tool format conversion and execution
- Real-time tool response processing

### 🔐 OAuth 2.0 Authentication
- Secure OAuth 2.0 Device Flow implementation
- Automatic token management and refresh
- PKCE (Proof Key for Code Exchange) security
- Persistent token storage and recovery

## iFlow OAuth (CLI‑Aligned)

This project’s iFlow provider implements the same end‑to‑end authorization behavior as the official iFlow CLI:

- Flow priority
  - Authorization Code + local callback (opens your default browser)
  - Fallback: Device Code (prints `verification_uri` and `user_code`; also opens browser when possible)

- Endpoints (aligned with iFlow CLI)
  - Authorization: `https://iflow.cn/oauth`
  - Token exchange: `https://iflow.cn/oauth/token`
  - Device code: `https://iflow.cn/oauth/device/code` (auto‑fallback with `/device_code` variant)
  - LLM API (OpenAI‑compatible): `https://apis.iflow.cn/v1`

- Token storage (JSON)
  - File: `~/.iflow/oauth_creds.json`
  - Fields: `access_token`, `refresh_token`, `expires_at`, `token_type`, `scope`, `apiKey`
  - Note: `apiKey` is fetched after OAuth (`/api/oauth/getUserInfo?accessToken=...`) and is used for LLM API calls as the primary credential

- Auth header selection for LLM API
  - Prefer: `Authorization: Bearer <apiKey>`
  - Fallback: `Authorization: Bearer <access_token>`

- Client credentials (optional, CLI‑style)
  - If you have official client credentials, set:
    - `IFLOW_CLIENT_ID`
    - `IFLOW_CLIENT_SECRET`
  - When `client_secret` is present, token exchange uses `Authorization: Basic base64(client_id:client_secret)` (same as CLI)
  - If not provided, PKCE is used where supported; otherwise device flow is used

### Quick Start: iFlow Auth

- One‑shot auth helper (opens browser; writes `~/.iflow/oauth_creds.json`):

```bash
node --import tsx scripts/iflow-auth.ts
```

- Verify token file (should include `apiKey`):

```bash
sed -n '1,200p' ~/.iflow/oauth_creds.json
```

- Direct API sanity check (replace `<MODEL_ID>` with an available model for your account):

```bash
APIKEY=$(node -e 'const fs=require("fs");const p=process.env.HOME+"/.iflow/oauth_creds.json";if(fs.existsSync(p)){const j=JSON.parse(fs.readFileSync(p,"utf8"));if(j.apiKey)process.stdout.write(j.apiKey)}')
curl -sS \
  -H "Authorization: Bearer $APIKEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"<MODEL_ID>","messages":[{"role":"user","content":"用一句中文自我介绍。"}]}' \
  https://apis.iflow.cn/v1/chat/completions
```

### Server Config Examples

- Minimal provider (OAuth stored in default path):

```json
{
  "virtualrouter": {
    "providers": {
      "iflow": {
        "type": "iflow",
        "baseURL": "https://api.iflow.cn/v1",
        "oauth": {
          "deviceCodeUrl": "https://iflow.cn/oauth/device/code",
          "tokenUrl": "https://iflow.cn/oauth/token",
          "tokenFile": "~/.iflow/oauth_creds.json"
        },
        "compatibility": { "type": "iflow-compatibility" },
        "models": { "qwen3-coder": { "maxContext": 128000, "maxTokens": 8192 } }
      }
    }
  }
}
```

- With client credentials (CLI‑style Basic token exchange):

```json
{
  "virtualrouter": {
    "providers": {
      "iflow": {
        "type": "iflow",
        "baseURL": "https://api.iflow.cn/v1",
        "oauth": {
          "deviceCodeUrl": "https://iflow.cn/oauth/device/code",
          "tokenUrl": "https://iflow.cn/oauth/token",
          "tokenFile": "~/.iflow/oauth_creds.json",
          "clientId": "${IFLOW_CLIENT_ID}",
          "clientSecret": "${IFLOW_CLIENT_SECRET}"
        },
        "compatibility": { "type": "iflow-compatibility" }
      }
    }
  }
}
```

### Troubleshooting

- 404 on `iflow.cn/v1/...` or `api.iflow.cn/v1/...`: use `apis.iflow.cn/v1` for LLM API
- 40308 (“APIKEY需要升级…”): regenerate your apiKey in iFlow user settings and retry
- 400 “No provider for model: …”: the model is not enabled for your apiKey/account; choose a model available to your plan
- Device code HTML/portal pages: the provider auto‑retries with alternate path and may switch host to `api.iflow.cn` when necessary


### 🌊 Streaming Support
- Native streaming request/response handling
- Automatic streaming/non-streaming conversion
- Chunked response processing
- Backward compatibility with non-streaming clients

### 🔄 Multi-Protocol Support
- OpenAI-compatible API endpoints (`/v1/chat/completions`)
- OpenAI Responses API support (`/v1/responses`)
- Native provider protocol support
- Automatic protocol translation
- Configuration-driven transformations

### 🎯 Dynamic Routing
- Intelligent request routing based on content analysis
- 7 specialized routing categories
- Performance-optimized pipeline selection
- Custom routing rules support

### 🧪 Advanced Dry-Run System
- **Comprehensive Pipeline Debugging**: Node-level dry-run execution with "pipeline break" debugging
- **Input Simulation**: Intelligent mock data generation for all-nodes dry-run scenarios
- **Bidirectional Pipeline**: Request and response pipeline dry-run with real server response integration
- **Memory Management**: Intelligent resource cleanup and memory leak prevention
- **Error Boundaries**: Multi-level error handling with automatic recovery mechanisms

## 🚀 Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/routecodex.git
cd routecodex

# Install dependencies
npm install

# Build the project
npm run build
```

### Basic Usage

#### LM Studio Provider (API Key)

```javascript
import { OpenAINormalizerLLMSwitch } from './dist/modules/pipeline/modules/llmswitch/openai-normalizer.js';
import { LMStudioCompatibility } from './dist/modules/pipeline/modules/compatibility/lmstudio-compatibility.js';
import { LMStudioProviderSimple } from './dist/modules/pipeline/modules/provider/lmstudio-provider-simple.js';
import { StreamingControlWorkflow } from './dist/modules/pipeline/modules/workflow/streaming-control.js';
import { DebugCenter } from 'rcc-debugcenter';
import { ErrorHandlingCenter } from 'rcc-errorhandling';

// Initialize components
const errorHandlingCenter = new ErrorHandlingCenter();
const debugCenter = new DebugCenter();
await errorHandlingCenter.initialize();

// Create 4-layer pipeline
const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: {
    protocol: 'openai',
    targetFormat: 'lmstudio'
  }
}, { errorHandlingCenter, debugCenter, logger });

const workflow = new StreamingControlWorkflow({
  type: 'streaming-control',
  config: { enableStreaming: true }
}, { errorHandlingCenter, debugCenter, logger });

const compatibility = new LMStudioCompatibility({
  type: 'lmstudio-compatibility',
  config: { toolsEnabled: true }
}, { errorHandlingCenter, debugCenter, logger });

const provider = new LMStudioProviderSimple({
  type: 'lmstudio-http',
  config: {
    baseUrl: 'http://localhost:1234',
    auth: { type: 'apikey', apiKey: 'your-api-key' }
  }
}, { errorHandlingCenter, debugCenter, logger });

// Initialize all modules
await llmSwitch.initialize();
await workflow.initialize();
await compatibility.initialize();
await provider.initialize();

// Process request through 4-layer pipeline
const request = {
  model: 'gpt-oss-20b-mlx',
  messages: [{ role: 'user', content: 'Hello!' }],
  tools: [...] // Optional tools
};

const result = await provider.processIncoming(
  await compatibility.processIncoming(
    await workflow.processIncoming(
      await llmSwitch.processIncoming(request)
    )
  )
);
```

#### Qwen OAuth Provider

```javascript
import { OpenAINormalizerLLMSwitch } from './dist/modules/pipeline/modules/llmswitch/openai-normalizer.js';
import { QwenCompatibility } from './dist/modules/pipeline/modules/compatibility/qwen-compatibility.js';
import { QwenProvider } from './dist/modules/pipeline/modules/provider/qwen-provider.js';
import { StreamingControlWorkflow } from './dist/modules/pipeline/modules/workflow/streaming-control.js';
import { DebugCenter } from 'rcc-debugcenter';
import { ErrorHandlingCenter } from 'rcc-errorhandling';

// Initialize components
const errorHandlingCenter = new ErrorHandlingCenter();
const debugCenter = new DebugCenter();
await errorHandlingCenter.initialize();

// Create 4-layer pipeline with OAuth
const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: { protocol: 'openai', targetFormat: 'qwen' }
}, { errorHandlingCenter, debugCenter, logger });

const workflow = new StreamingControlWorkflow({
  type: 'streaming-control',
  config: { enableStreaming: true }
}, { errorHandlingCenter, debugCenter, logger });

const compatibility = new QwenCompatibility({
  type: 'qwen-compatibility',
  config: { toolsEnabled: true }
}, { errorHandlingCenter, debugCenter, logger });

const provider = new QwenProvider({
  type: 'qwen-provider',
  config: {
    type: 'qwen',
    baseUrl: 'https://chat.qwen.ai',
    oauth: {
      clientId: 'f0304373b74a44d2b584a3fb70ca9e56',
      deviceCodeUrl: 'https://chat.qwen.ai/api/v1/oauth2/device/code',
      tokenUrl: 'https://chat.qwen.ai/api/v1/oauth2/token',
      scopes: ['openid', 'profile', 'email', 'model.completion'],
      tokenFile: './qwen-token.json'
    }
  }
}, { errorHandlingCenter, debugCenter, logger });

// Initialize all modules
await llmSwitch.initialize();
await workflow.initialize();
await compatibility.initialize();
await provider.initialize(); // This will trigger OAuth flow if not authenticated

// Process request - OAuth is handled automatically
const request = {
  model: 'qwen-turbo',
  messages: [{ role: 'user', content: 'Hello! How can you help me?' }]
};

const result = await provider.processIncoming(
  await compatibility.processIncoming(
    await workflow.processIncoming(
      await llmSwitch.processIncoming(request)
    )
  )
);
```

### Configuration

- Set config path via env var:
  - `ROUTECODEX_CONFIG_PATH=~/.routecodex/config/modelscope.json npm start`
- If not set, default path is `~/.routecodex/config.json`
- Single Source Of Truth (SSOT):
  - HTTP port must come from user config only
  - Compatibility must be set in user config (default `passthrough` is recommended first)
  - LLM Switch can be set to `openai-normalizer` to enforce strict OpenAI request shape
  - In `~/.routecodex/config.json`, set top-level `port` and optional host:
    ```json
    {
      "port": 5506,
      "server": { "host": "localhost" }
    }
    ```
  - If no port is provided, the server will fail to start with an explicit error.
- Merged config output: `~/routecodex/config/merged-config.json`

### Helper Notes

Port cleanup is now handled by the built‑in CLI (routecodex start --restart) and server shutdown endpoint. No external helper scripts are required.

### 🔧 Simplified Logging System

RouteCodex now includes a simplified logging system that provides easy-to-use logging configuration without the complexity of the full debug system.

#### Quick Start
```bash
# Enable simplified logging
routecodex simple-log on --level debug --output console

# Check current status
routecodex simple-log status

# Change log level
routecodex simple-log level error

# Set output to file
routecodex simple-log output file

# Disable simplified logging
routecodex simple-log off
```

#### Features
- **One-click enable/disable**: Simple commands to control logging
- **Multiple log levels**: debug, info, warn, error
- **Flexible output**: console, file, or both
- **Persistent configuration**: Settings survive restarts
- **Automatic application**: Config automatically applied when server starts

#### Configuration File
Settings are stored in `~/.routecodex/simple-log-config.json` and automatically loaded on server startup. Default log output directory is `~/routecodex/debug-logs` when file output is enabled.

#### Benefits
- **Reduced complexity**: 788-line complex indexer → 150-line simplified version
- **No memory overhead**: Removed history management and compression
- **Always-on capability**: Once enabled, stays active until explicitly disabled
- **Zero configuration**: Works out of the box with sensible defaults

### Pipeline Scope

- Pipeline uses LLM Switch + Compatibility + Provider.
- LLM Switch `openai-normalizer` performs standard OpenAI normalization.
- Compatibility handles vendor-specific differences; `passthrough` does no changes.

### Tool Calling Example

```javascript
const requestWithTools = {
  model: 'gpt-oss-20b-mlx',
  messages: [
    {
      role: 'system',
      content: 'You are a helpful assistant with access to tools.'
    },
    {
      role: 'user',
      content: 'What is 15 * 25?'
    }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'calculate',
        description: 'Perform mathematical calculations',
        parameters: {
          type: 'object',
          properties: {
            expression: {
              type: 'string',
              description: 'Mathematical expression to evaluate'
            }
          },
          required: ['expression']
        }
      }
    }
  ]
};

// Process through pipeline - tool calls will be automatically handled
const response = await processRequest(requestWithTools);

// Tool calls will be in response.choices[0].message.tool_calls
console.log('Tool calls:', response.choices[0].message.tool_calls);
```

## 📖 Documentation

- [Architecture Documentation](./ARCHITECTURE_DOCUMENTATION.md) - Detailed 4-layer architecture explanation
- [Configuration Guide](./docs/CONFIG_ARCHITECTURE.md) - Configuration options and examples
- [Pipeline Architecture](./docs/pipeline/ARCHITECTURE.md) - Pipeline system details
- [LM Studio Integration](./docs/lmstudio-tool-calling.md) - LM Studio specific setup
- [Dry-Run System](./docs/dry-run/README.md) - Comprehensive dry-run debugging framework
- [Bidirectional Pipeline](./docs/dry-run/bidirectional-pipeline.md) - Advanced bidirectional pipeline features

## 🔧 Configuration

### Basic Configuration

```json
{
  "server": {
    "port": 5506,
    "host": "localhost"
  },
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234",
      "apiKey": "your-api-key"
    }
  },
  "routing": {
    "default": "lmstudio"
  },
  "compatibility": "passthrough",
  "features": {
    "tools": {
      "enabled": true,
      "maxTools": 10
    },
    "streaming": {
      "enabled": true,
      "chunkSize": 1024
    }
  }
}
```

### Advanced Pipeline Configuration

For detailed pipeline configuration, you can specify individual modules:

```json
{
  "pipeline": {
    "llmSwitch": {
      "type": "llmswitch-openai-openai",
      "config": {
        "protocol": "openai",
        "targetFormat": "lmstudio"
      }
    },
    "workflow": {
      "type": "streaming-control",
      "config": {
        "enableStreaming": true,
        "bufferSize": 1024,
        "timeout": 30000
      }
    },
    "compatibility": {
      "type": "lmstudio-compatibility",
      "config": {
        "toolsEnabled": true,
        "customRules": [
          {
            "id": "ensure-standard-tools-format",
            "transform": "mapping",
            "sourcePath": "tools",
            "targetPath": "tools",
            "mapping": {
              "type": "type",
              "function": "function"
            }
          }
        ]
      }
    },
    "provider": {
      "type": "lmstudio-http",
      "config": {
        "type": "lmstudio",
        "baseUrl": "http://localhost:1234",
        "auth": {
          "type": "apikey",
          "apiKey": "your-api-key"
        },
        "timeout": 60000
      }
    }
  }
}
```

## 🧪 Testing

```bash
# Run all tests
npm test

# Run specific test
npm test -- --testNamePattern="tool calling"

# Run integration test with LM Studio
node test-llmswitch-workflow-integration.mjs

# Run dry-run system tests
node test-all-nodes-dry-run.mjs

# Run bidirectional pipeline tests
node test-bidirectional-pipeline-dry-run.mjs

# Run build
npm run build
```

## 🎯 Supported Providers

### ✅ Currently Supported (Fully Operational)
- **LM Studio**: Local AI model hosting with full tool support and comprehensive dry-run analysis
- **Qwen**: Alibaba's language models with OAuth 2.0 authentication and API key fallback
- **iFlow**: AI service provider with OAuth 2.0 + PKCE authentication, Kimi model support
- **OpenAI**: GPT models with function calling
- **Anthropic**: Claude model family

### 🔧 Latest Updates (v0.2.7)
This release enables full operational capability for three major AI providers:

- **LM Studio**: Complete tool calling implementation with dry-run testing framework
- **Qwen**: Working OAuth authentication with automatic token refresh and caching
- **iFlow**: Fixed authentication endpoints and API configuration, Kimi model integration
- **Unified Auth Framework**: EnhancedAuthResolver supporting multiple authentication types
- **Build System**: Resolved TypeScript compilation issues
- **🔧 Offline Logging CLI**: New comprehensive CLI for offline log capture and analysis
  - Module-level and pipeline-level offline logging configuration
  - Automatic log file management with rotation and compression
  - Built-in log analysis with HTML visualization reports
  - Time series analysis for performance monitoring
  - Zero-dependency offline operation

### 🔐 OAuth Authentication Details

#### Qwen OAuth Provider
- **Authentication**: OAuth 2.0 Device Flow
- **Token Management**: Automatic refresh with persistent storage
- **Security**: Standard OAuth 2.0 security model
- **Models**: qwen-turbo, qwen-max, qwen-turbo-latest

#### iFlow OAuth Provider
- **Authentication**: OAuth 2.0 Device Flow with PKCE
- **Token Management**: Automatic refresh with persistent storage
- **Security**: Enhanced PKCE (Proof Key for Code Exchange) protection
- **Models**: iflow-turbo, iflow-pro, iflow-turbo-latest

### 🔄 Planned Support
- Google Gemini
- Cohere
- Custom provider framework

## 📊 Performance

- **Request Processing**: < 50ms overhead per layer
- **Tool Calling**: Native provider optimization
- **Streaming**: Real-time chunk processing
- **Memory Usage**: Efficient module loading and caching

## 🧪 标准OpenAI API测试方法

RouteCodex提供完整的OpenAI兼容API测试，确保与各种AI客户端的完美集成。

### 端到端测试流程

#### 1. 启动服务器
```bash
# 使用ModelScope配置启动
ROUTECODEX_CONFIG_PATH=~/.routecodex/config/modelscope.json npm start

# 服务器将在配置的端口上启动 (默认5506)
# 健康检查: curl http://localhost:5506/health
```

#### 2. 标准测试命令

**基础对话测试**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "Hello, please respond briefly"
      }
    ]
  }'
```

**工具调用测试**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "What is 15 * 25? Use the calculator tool."
      }
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate",
          "description": "Perform mathematical calculations",
          "parameters": {
            "type": "object",
            "properties": {
              "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
              }
            },
            "required": ["expression"]
          }
        }
      }
    ]
  }'
```

**流式响应测试**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "Count from 1 to 5 slowly"
      }
    ],
    "stream": true
  }'
```

#### 3. 验证端点

**服务器状态**:
```bash
# 根端点 - 显示系统信息
curl http://localhost:5506/

# 健康检查
curl http://localhost:5506/health

# 配置信息
curl http://localhost:5506/config
```

#### 4. 完整测试脚本

创建测试脚本 `test-modelscope.sh`:
```bash
#!/bin/bash

# ModelScope API测试脚本
set -e

BASE_URL="http://localhost:5506"
API_KEY="${MS_API_KEY:-YOUR_MODELSCOPE_API_KEY}"
MODEL="Qwen/Qwen3-Coder-480B-A35B-Instruct"

echo "🧪 RouteCodex ModelScope API测试开始..."
echo "📡 测试地址: $BASE_URL"
echo "🔑 使用模型: $MODEL"
echo ""

# 1. 健康检查
echo "1️⃣ 健康检查..."
curl -s "$BASE_URL/health" | jq .

# 2. 基础对话测试
echo ""
echo "2️⃣ 基础对话测试..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"Hello! Please respond briefly.\"
      }
    ]
  }" | jq .

# 3. 代码生成测试
echo ""
echo "3️⃣ 代码生成测试..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"Write a simple Python function to calculate factorial.\"
      }
    ]
  }" | jq .

# 4. 工具调用测试
echo ""
echo "4️⃣ 工具调用测试..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"What is the square root of 144?\"
      }
    ],
    \"tools\": [
      {
        \"type\": \"function\",
        \"function\": {
          \"name\": \"calculate\",
          \"description\": \"Perform mathematical calculations\",
          \"parameters\": {
            \"type\": \"object\",
            \"properties\": {
              \"expression\": {
                \"type\": \"string\",
                \"description\": \"Mathematical expression to evaluate\"
              }
            },
            \"required\": [\"expression\"]
          }
        }
      }
    ]
  }" | jq .

echo ""
echo "✅ 所有测试完成!"
```

#### 5. 测试结果验证

成功的测试响应应该包含:
- **状态码**: 200 OK
- **响应结构**: 标准OpenAI格式
- **处理时间**: 通常 < 2秒
- **Token使用**: 合理的token计数
- **路由信息**: 包含 `_metadata` 字段

#### 6. 配置文件示例

`~/.routecodex/config/modelscope.json`:
```json
{
  "port": 5506,
  "virtualrouter": {
    "inputProtocol": "openai",
    "outputProtocol": "openai",
    "providers": {
      "modelscope": {
        "type": "openai",
        "baseURL": "https://api-inference.modelscope.cn/v1",
        "apiKey": [
          "YOUR_MODELSCOPE_KEY1",
          "YOUR_MODELSCOPE_KEY2",
          "YOUR_MODELSCOPE_KEY3",
          "YOUR_MODELSCOPE_KEY4"
        ],
        "models": {
          "Qwen/Qwen3-Coder-480B-A35B-Instruct": {
            "maxContext": 262144,
            "maxTokens": 262144
          }
        }
      }
    },
    "routing": {
      "default": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ],
      "coding": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ],
      "longcontext": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ]
    }
  }
}
```

#### 7. 常见问题排查

**问题1: 404 Not Found**
```bash
# 检查端点是否正确
curl http://localhost:5506/
# 应该看到可用端点列表，OpenAI端点是 /v1/*
```

**问题2: 认证失败**
```bash
# 检查API密钥格式
# 密钥应该是: ms-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

**问题3: 模型错误**
```bash
# 检查模型名称是否在配置中存在
# 使用配置文件中定义的确切模型名称
```

#### 8. 性能基准

- **响应时间**: 800-1500ms (取决于网络和模型复杂度)
- **Token处理**: 支持262K上下文长度
- **并发能力**: 支持多请求并发处理
- **错误处理**: 完整的错误恢复机制

这套标准测试方法确保RouteCodex与任何OpenAI兼容客户端的完美集成。

## 🆕 Responses API Support ⭐

RouteCodex now provides comprehensive support for OpenAI's **Responses API** through its 4-layer pipeline architecture, enabling seamless integration with the next-generation AI interface.

### 🎯 Key Features
- **🔄 Full Protocol Conversion**: Automatic conversion between Responses and Chat formats
- **🔧 Complete Tool Support**: Full tool calling support in Responses API format
- **📡 Advanced Streaming**: Complete streaming event support with SSE processing
- **🏗️ Pipeline Integration**: Seamless integration with existing 4-layer architecture
- **🧪 Comprehensive Testing**: Extensive test fixtures and regression coverage

### 📡 API Endpoints

**Responses API**: `http://localhost:5506/v1/responses` ⭐
**Chat Completions**: `http://localhost:5506/v1/chat/completions`

### 🏗️ Architecture Integration

The Responses API integrates seamlessly into RouteCodex's 4-layer pipeline:

```
Responses Request → LLM Switch → Workflow → Compatibility → Provider → AI Service
       ↓               ↓          ↓            ↓           ↓
   Protocol       Format     Flow       Format     Standard
   Detection      Conversion  Control    Conversion  HTTP Server
   (responses)   (→ Chat)   (Events)   (→ Chat)   (Chat Format)
```

**Key Components**:
- **LLM Switch**: `llmswitch-response-chat` module handles protocol conversion
- **Workflow**: `responses-streaming-workflow` manages streaming events
- **Compatibility**: Format adaptation for different providers
- **Provider**: Standard HTTP communication with AI services

### ⚙️ Configuration

#### Basic Responses API Configuration

```json
{
  "virtualrouter": {
    "inputProtocol": "responses",
    "outputProtocol": "openai",
    "providers": {
      "lmstudio": {
        "type": "lmstudio",
        "baseURL": "http://localhost:1234",
        "apiKey": "your-api-key",
        "models": {
          "gpt-4": {
            "compatibility": { "type": "responses-chat-switch" }
          }
        }
      }
    },
    "routing": {
      "default": ["lmstudio.gpt-4"]
    }
  }
}
```

#### Advanced Pipeline Configuration

```json
{
  "pipeline": {
    "llmSwitch": {
      "type": "llmswitch-response-chat",
      "config": {
        "enableValidation": true,
        "enableMetadata": true,
        "preserveReasoning": true
      }
    },
    "workflow": {
      "type": "responses-streaming-workflow",
      "config": {
        "enableEventProcessing": true,
        "rebuildCompleteResponse": true
      }
    },
    "compatibility": {
      "type": "passthrough-compatibility",
      "config": {}
    },
    "provider": {
      "type": "lmstudio-http",
      "config": {
        "baseUrl": "http://localhost:1234",
        "auth": { "type": "apikey", "apiKey": "your-api-key" }
      }
    }
  }
}
```

### 🚀 Usage Examples

#### Basic Responses API Request

```bash
curl -X POST http://localhost:5506/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "gpt-4",
    "instructions": "You are a helpful assistant.",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "Hello! How can you help me today?"
          }
        ]
      }
    ],
    "stream": true
  }'
```

#### Responses API with Tool Calling

```bash
curl -X POST http://localhost:5506/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "gpt-4",
    "instructions": "You are a helpful assistant with access to tools.",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "What is 15 * 25? Use the calculator tool."
          }
        ]
      }
    ],
    "tools": [
      {
        "type": "function",
        "name": "calculate",
        "description": "Perform mathematical calculations",
        "parameters": {
          "type": "object",
          "properties": {
            "expression": {
              "type": "string",
              "description": "Mathematical expression to evaluate"
            }
          },
          "required": ["expression"]
        }
      }
    ],
    "stream": true
  }'
```

### 📡 Streaming Events

The Responses API supports all standard streaming events with proper SSE formatting:

| Event Type | Description | Example |
|------------|-------------|---------|
| `response.created` | Response initialization | `{"type": "response.created", "response": {...}}` |
| `response.output_item.added` | New output item | `{"type": "response.output_item.added", "item": {...}}` |
| `response.content_part.added` | Content part added | `{"type": "response.content_part.added", "part": {...}}` |
| `response.output_text.delta` | Text delta | `{"type": "response.output_text.delta", "delta": "Hello"} ⭐` |
| `response.tool_call.delta` | Tool call delta | `{"type": "response.tool_call.delta", "delta": {...}}` |
| `response.output_item.done` | Item completed | `{"type": "response.output_item.done", "item": {...}}` |
| `response.done` | Response complete | `{"type": "response.done", "response": {...}}` |

### 🔄 Protocol Conversion Details

#### Request Flow: Responses → Chat

1. **Input Processing**: Convert `input[]` array to `messages[]` format
2. **Instructions Handling**: Convert `instructions` to system message
3. **Tool Format**: Convert Responses tool format to OpenAI format
4. **Metadata**: Preserve original protocol information

```typescript
// Responses Format Input
{
  "model": "gpt-4",
  "instructions": "You are helpful",
  "input": [
    {
      "type": "message",
      "role": "user",
      "content": [{"type": "input_text", "text": "Hello"}]
    }
  ]
}

// Converted to Chat Format
{
  "model": "gpt-4",
  "messages": [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Hello"}
  ]
}
```

#### Response Flow: Chat → Responses

1. **Message Reconstruction**: Convert Chat messages back to Responses output format
2. **Tool Call Conversion**: Convert OpenAI tool_calls to Responses tool format
3. **Streaming Events**: Generate appropriate SSE events for streaming responses
4. **Metadata Handling**: Preserve conversion context and protocol information

### 🧪 Testing and Validation

#### Test Fixtures

Comprehensive test fixtures are available in `tests/fixtures/`:

- `responses_1760615123370_request.json` - Complete sample request format
- `responses_output_text_events.json` - Streaming event samples
- `responses_tool_call_events.json` - Tool calling event samples

#### Integration Testing

```bash
# Test Responses API with streaming
curl -X POST http://localhost:5506/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer test-key" \
  -d @tests/fixtures/responses_1760615123370_request.json

# Verify streaming events
# Should see proper SSE formatting with event types and data
```

### 🔧 Client Configuration

#### Codex CLI Integration

```toml
[model_providers.rc]
name = "routecodex"
base_url = "http://localhost:5506/v1"
wire_api = "responses"
api_key = "your-api-key"
```

#### OpenAI SDK Integration

```typescript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'http://localhost:5506/v1',
  apiKey: 'your-api-key'
});

// Use Responses API
const response = await client.responses.create({
  model: 'gpt-4',
  instructions: 'You are a helpful assistant.',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Hello!' }]
    }
  ]
});
```

### 🛡️ Error Handling

The Responses API includes comprehensive error handling:

- **Format Validation**: Validate Requests API request format
- **Protocol Errors**: Handle conversion errors gracefully
- **Streaming Errors**: Proper error event streaming
- **Tool Call Errors**: Detailed tool calling error information

### 📈 Performance Features

- **Event Batching**: Efficient processing of streaming events
- **Memory Management**: Intelligent buffering for large responses
- **Async Processing**: Non-blocking event handling
- **Error Recovery**: Automatic retry and fallback mechanisms

### 🚨 Limitations and Considerations

**Current Limitations**:
- Some advanced Responses features may still be in development
- Certain edge cases in tool calling format conversion
- Complex multimodal content handling

**Best Practices**:
- Use standard Responses API format for best compatibility
- Test with provided fixtures before production deployment
- Monitor streaming event processing for performance optimization

### 🔄 Migration from Chat Completions

#### Easy Migration Path

Existing Chat Completions users can easily migrate:

1. **Update Endpoint**: Change from `/v1/chat/completions` to `/v1/responses`
2. **Format Conversion**: Convert `messages[]` to `input[]` array format
3. **Instructions**: Move system messages to `instructions` field
4. **Tools**: Update tool format to Responses specification

#### Migration Example

```typescript
// Before (Chat Completions)
const chatRequest = {
  model: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are helpful' },
    { role: 'user', content: 'Hello!' }
  ],
  tools: [...] // OpenAI tool format
};

// After (Responses API)
const responsesRequest = {
  model: 'gpt-4',
  instructions: 'You are helpful',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Hello!' }]
    }
  ],
  tools: [...] // Responses tool format
};
```

### 📚 Documentation and Resources

- **Module Documentation**: See `src/modules/pipeline/modules/llmswitch/README.md`
- **Architecture Details**: See `docs/llmswitch-responses-plan.md`
- **Test Examples**: See `tests/fixtures/` directory
- **Configuration Guide**: See individual module READMEs

---

**Responses API support represents a major enhancement to RouteCodex**, providing next-generation AI interface capabilities while maintaining full backward compatibility with existing Chat Completions workflows.

## ✅ ModelScope 独立验证通过

- 日期: 2025-09-26
- 配置: `~/.routecodex/config/modelscope.json`
- 端口: 5507（示例环境）
- 验证项:
  - 非流式返回 OpenAI 标准结构（id/object/created/model/choices/usage）
  - 流式（SSE）分片规范（object: chat.completion.chunk，delta.content，有 [DONE]）
  - 严格 JSON 模式：`response_format: { "type": "json_object" }` 内容清洗为纯 JSON 字符串
  - 429 调度：由 PipelineManager 统一处理，多 Key/Pipeline 轮询重试，全部枯竭才返回 429
  - 统一响应头：`x-request-id`、`Cache-Control: no-store`、`Content-Type: application/json; charset=utf-8`

详情见 docs/verification/modelscope-verify.md。

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🙏 Acknowledgments

- **LM Studio**: For providing excellent local AI model hosting
- **OpenAI**: For the standard API specification
- **TypeScript**: For enabling type-safe development
- **Node.js**: For the robust runtime environment

## 📞 Support

For support, please:
1. Check the [documentation](./docs/)
2. Search existing [issues](https://github.com/your-username/routecodex/issues)
3. Create a new issue with detailed information

---

**Built with ❤️ using the 4-layer architecture pattern**
