# RouteCodex - RouteCodex Claude Code Router

[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)
[![Architecture](https://img.shields.io/badge/Architecture-4--layer-purple.svg)](./ARCHITECTURE_DOCUMENTATION.md)

A sophisticated AI service routing and transformation system that provides seamless integration with multiple AI providers through a clean, modular architecture.

Release 0.41.1
- Fix: Anthropic streaming compliance for tool_use. RouteCodex now emits standard SSE events for tool inputs so Claude Code correctly accumulates parameters during streaming tool calls.

Important: Anthropic tool-calls (since 0.4.0)
- By default we trust tool schemas (trustSchema=true). When converting OpenAI `tool_calls` to Anthropic `tool_use`, we preserve `function.name` and `arguments` as-is without renaming tool names or remapping argument fields. This aligns with Claude Code Router and guarantees correct tool flow (emitting `stop_reason=tool_use` on `message_delta`).

## ‚úÖ GLM‚Äë4.6 Works With Codex CLI

RouteCodex now supports running GLM‚Äë4.6 with Codex CLI through the OpenAI‚Äëcompatible endpoint. Highlights:

- Wire API: `chat` (Codex requires chat protocol)
- Tool calls: reconstructed in Compatibility (no tool text leakage)
- Thinking: enabled for GLM‚Äë4.6; private <think>‚Ä¶</think> is stripped from output while preserving model reasoning behavior
- SSE: router emits `delta.tool_calls` when needed; content is suppressed to keep tool calls at the end

Quick setup

1) Start RouteCodex (default port 5520)

```bash
npm run dev
# OpenAI endpoint: http://localhost:5520/v1/openai
```

2) Ensure user config includes GLM provider (auto‚Äëgenerated by default at `~/.routecodex/config.json`):

```json
{
  "virtualrouter": {
    "providers": {
      "glm": {
        "type": "glm",
        "baseURL": "https://open.bigmodel.cn/api/coding/paas/v4",
        "apiKey": ["<YOUR_GLM_KEY>"],
        "models": {
          "glm-4.6": {
            "compatibility": { "type": "glm-compatibility", "config": { "thinking": { "enabled": true, "payload": { "type": "enabled" } } } }
          }
        }
      }
    },
    "routing": { "default": ["glm.glm-4.6"] }
  },
  "port": 5520
}
```

3) Configure Codex profile (`~/.codex/config.toml`):

```toml
[model_providers.rc]
name = "glm"
base_url = "http://localhost:5520/v1/openai"
wire_api = "chat"
api_key = "<YOUR_GLM_KEY>"

[profiles.rc]
model_provider = "rc"
model = "glm-4.6"
```

4) Test with Codex

```bash
codex -p rc "‰Ω†Â•ΩÔºåÂàóÂá∫ÂΩìÂâçÁõÆÂΩï (Ëß¶ÂèëÂ∑•ÂÖ∑Ë∞ÉÁî®)"
```

Notes
- Compatibility converts `<invoke ‚Ä¶>`, `<function ‚Ä¶>`, `<function_call>{‚Ä¶}</function_call>` and `[tool_call:name]{‚Ä¶}` into OpenAI `tool_calls`.
- Private reasoning blocks `<think>‚Ä¶</think>` are removed from the visible content to keep OpenAI/Codex semantics intact.

## üèóÔ∏è 4-Layer Architecture

RouteCodex implements a sophisticated 4-layer pipeline architecture that provides clean separation of concerns and flexible protocol handling:

```
HTTP Request ‚Üí LLM Switch ‚Üí Workflow ‚Üí Compatibility ‚Üí Provider ‚Üí AI Service
     ‚Üì             ‚Üì          ‚Üì            ‚Üì           ‚Üì
  Request      Protocol   Flow       Format     Standard     Response
  Analysis     Routing    Control    Conversion   HTTP Server  Processing
```

### Layer 1: LLM Switch (Dynamic Routing + Standardization)
- **Request Analysis**: Analyzes incoming requests to determine optimal routing
- **Protocol Routing**: Routes requests to appropriate processing pipelines
- **OpenAI Standardization**: Enforces strict OpenAI Chat Completions shape before downstream modules
  - `assistant.tool_calls[].function.arguments` must be a JSON string (objects are stringified)
  - `assistant.content` must be a string (default "")
  - `tool.content` must be a string
- **Dynamic Classification**: Supports 7 routing categories (default, longcontext, thinking, background, websearch, vision, coding)

### Layer 2: Workflow (Flow Control)
- **Streaming Control**: Handles streaming/non-streaming request conversion
- **Request Processing**: Manages request flow and processing state
- **Response Handling**: Processes and transforms responses as needed

### Layer 3: Compatibility (Format Transformation - Vendor Differences Only)
- **Protocol Translation**: Converts between different AI service protocols
- **Format Adaptation**: Transforms request/response formats between providers
- **Tool Integration**: Handles tool calling format conversion and execution
- **Configuration-Driven**: Uses JSON configuration for transformation rules
- **Scope Rule**: Compatibility only handles vendor-specific differences. All ‚Äústandard OpenAI‚Äù normalization lives in the LLM Switch.
- **Simple String Format**: Supports easy compatibility specification (e.g., `"compatibility": "passthrough"`)

## üîß Compatibility Field Configuration

The compatibility field supports both simple string format and complex object format, providing flexible configuration options for different use cases.

### Simple String Format

For basic usage, you can specify compatibility as a simple string:

```json
{
  "compatibility": "passthrough"
}
```

#### Supported String Values:
- `"passthrough"` - Direct pass-through without transformation (default)
- `"lmstudio"` - LM Studio compatibility mode
- `"qwen"` - Qwen compatibility mode
- `"iflow"` - iFlow compatibility mode
- Multiple providers: `"lmstudio/qwen"` - Will use first available provider

### Complex Object Format

For advanced configuration, use the complex object format:

```json
{
  "compatibility": {
    "type": "lmstudio-compatibility",
    "config": {
      "toolsEnabled": true,
      "customRules": [
        {
          "id": "ensure-standard-tools-format",
          "transform": "mapping",
          "sourcePath": "tools",
          "targetPath": "tools",
          "mapping": {
            "type": "type",
            "function": "function"
          }
        }
      ]
    }
  }
}
```

### Priority Hierarchy

The system follows this priority order when determining compatibility:

1. **User Config Compatibility** (highest priority)
2. **Model-Level Compatibility**
3. **Provider-Level Compatibility**
4. **Auto-Inference** (fallback based on provider type)

> Note on inference: To keep ‚Äúconfiguration as the single source of truth‚Äù, prefer setting `compatibility` explicitly (default is `passthrough`). Inference may be removed in future revisions.

### Configuration Examples

#### Basic Passthrough Configuration
```json
{
  "server": {
    "port": 5506
  },
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234",
      "apiKey": "your-api-key"
    }
  },
  "compatibility": "passthrough"
}
```

#### Multi-Provider Compatibility
```json
{
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234"
    },
    "qwen": {
      "type": "qwen",
      "baseUrl": "https://chat.qwen.ai"
    }
  },
  "compatibility": "lmstudio/qwen"
}
```

#### Advanced Configuration with Custom Rules
```json
{
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234"
    }
  },
  "compatibility": {
    "type": "lmstudio-compatibility",
    "config": {
      "toolsEnabled": true,
      "customRules": [
        {
          "id": "tool-format-conversion",
          "transform": "mapping",
          "sourcePath": "tools",
          "targetPath": "tools",
          "mapping": {
            "type": "type",
            "function": "function"
          }
        }
      ]
    }
  }
}
```

### Layer 4: Provider (Standard HTTP Server)
- **HTTP Communication**: Manages all HTTP communications with AI services
- **Authentication**: Handles provider authentication and authorization
- **Error Handling**: Manages network errors and provider-specific issues

## ‚ú® Key Features

### üîß Tool Calling Support
- Full OpenAI-compatible tool calling implementation
- Supports multiple AI providers (LM Studio, OpenAI, Qwen, iFlow, Anthropic)
- Automatic tool format conversion and execution
- Real-time tool response processing

### üîê OAuth 2.0 Authentication
- Secure OAuth 2.0 Device Flow implementation
- Automatic token management and refresh
- PKCE (Proof Key for Code Exchange) security
- Persistent token storage and recovery

## iFlow OAuth (CLI‚ÄëAligned)

This project‚Äôs iFlow provider implements the same end‚Äëto‚Äëend authorization behavior as the official iFlow CLI:

- Flow priority
  - Authorization Code + local callback (opens your default browser)
  - Fallback: Device Code (prints `verification_uri` and `user_code`; also opens browser when possible)

- Endpoints (aligned with iFlow CLI)
  - Authorization: `https://iflow.cn/oauth`
  - Token exchange: `https://iflow.cn/oauth/token`
  - Device code: `https://iflow.cn/oauth/device/code` (auto‚Äëfallback with `/device_code` variant)
  - LLM API (OpenAI‚Äëcompatible): `https://apis.iflow.cn/v1`

- Token storage (JSON)
  - File: `~/.iflow/oauth_creds.json`
  - Fields: `access_token`, `refresh_token`, `expires_at`, `token_type`, `scope`, `apiKey`
  - Note: `apiKey` is fetched after OAuth (`/api/oauth/getUserInfo?accessToken=...`) and is used for LLM API calls as the primary credential

- Auth header selection for LLM API
  - Prefer: `Authorization: Bearer <apiKey>`
  - Fallback: `Authorization: Bearer <access_token>`

- Client credentials (optional, CLI‚Äëstyle)
  - If you have official client credentials, set:
    - `IFLOW_CLIENT_ID`
    - `IFLOW_CLIENT_SECRET`
  - When `client_secret` is present, token exchange uses `Authorization: Basic base64(client_id:client_secret)` (same as CLI)
  - If not provided, PKCE is used where supported; otherwise device flow is used

### Quick Start: iFlow Auth

- One‚Äëshot auth helper (opens browser; writes `~/.iflow/oauth_creds.json`):

```bash
node --import tsx scripts/iflow-auth.ts
```

- Verify token file (should include `apiKey`):

```bash
sed -n '1,200p' ~/.iflow/oauth_creds.json
```

- Direct API sanity check (replace `<MODEL_ID>` with an available model for your account):

```bash
APIKEY=$(node -e 'const fs=require("fs");const p=process.env.HOME+"/.iflow/oauth_creds.json";if(fs.existsSync(p)){const j=JSON.parse(fs.readFileSync(p,"utf8"));if(j.apiKey)process.stdout.write(j.apiKey)}')
curl -sS \
  -H "Authorization: Bearer $APIKEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"<MODEL_ID>","messages":[{"role":"user","content":"Áî®‰∏ÄÂè•‰∏≠ÊñáËá™Êàë‰ªãÁªç„ÄÇ"}]}' \
  https://apis.iflow.cn/v1/chat/completions
```

### Server Config Examples

- Minimal provider (OAuth stored in default path):

```json
{
  "virtualrouter": {
    "providers": {
      "iflow": {
        "type": "iflow",
        "baseURL": "https://api.iflow.cn/v1",
        "oauth": {
          "deviceCodeUrl": "https://iflow.cn/oauth/device/code",
          "tokenUrl": "https://iflow.cn/oauth/token",
          "tokenFile": "~/.iflow/oauth_creds.json"
        },
        "compatibility": { "type": "iflow-compatibility" },
        "models": { "qwen3-coder": { "maxContext": 128000, "maxTokens": 8192 } }
      }
    }
  }
}
```

- With client credentials (CLI‚Äëstyle Basic token exchange):

```json
{
  "virtualrouter": {
    "providers": {
      "iflow": {
        "type": "iflow",
        "baseURL": "https://api.iflow.cn/v1",
        "oauth": {
          "deviceCodeUrl": "https://iflow.cn/oauth/device/code",
          "tokenUrl": "https://iflow.cn/oauth/token",
          "tokenFile": "~/.iflow/oauth_creds.json",
          "clientId": "${IFLOW_CLIENT_ID}",
          "clientSecret": "${IFLOW_CLIENT_SECRET}"
        },
        "compatibility": { "type": "iflow-compatibility" }
      }
    }
  }
}
```

### Troubleshooting

- 404 on `iflow.cn/v1/...` or `api.iflow.cn/v1/...`: use `apis.iflow.cn/v1` for LLM API
- 40308 (‚ÄúAPIKEYÈúÄË¶ÅÂçáÁ∫ß‚Ä¶‚Äù): regenerate your apiKey in iFlow user settings and retry
- 400 ‚ÄúNo provider for model: ‚Ä¶‚Äù: the model is not enabled for your apiKey/account; choose a model available to your plan
- Device code HTML/portal pages: the provider auto‚Äëretries with alternate path and may switch host to `api.iflow.cn` when necessary


### üåä Streaming Support
- Native streaming request/response handling
- Automatic streaming/non-streaming conversion
- Chunked response processing
- Backward compatibility with non-streaming clients

### üîÑ Multi-Protocol Support
- OpenAI-compatible API endpoints
- Native provider protocol support
- Automatic protocol translation
- Configuration-driven transformations

### üéØ Dynamic Routing
- Intelligent request routing based on content analysis
- 7 specialized routing categories
- Performance-optimized pipeline selection
- Custom routing rules support

### üß™ Advanced Dry-Run System
- **Comprehensive Pipeline Debugging**: Node-level dry-run execution with "pipeline break" debugging
- **Input Simulation**: Intelligent mock data generation for all-nodes dry-run scenarios
- **Bidirectional Pipeline**: Request and response pipeline dry-run with real server response integration
- **Memory Management**: Intelligent resource cleanup and memory leak prevention
- **Error Boundaries**: Multi-level error handling with automatic recovery mechanisms

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/routecodex.git
cd routecodex

# Install dependencies
npm install

# Build the project
npm run build
```

### Basic Usage

#### LM Studio Provider (API Key)

```javascript
import { OpenAINormalizerLLMSwitch } from './dist/modules/pipeline/modules/llmswitch/openai-normalizer.js';
import { LMStudioCompatibility } from './dist/modules/pipeline/modules/compatibility/lmstudio-compatibility.js';
import { LMStudioProviderSimple } from './dist/modules/pipeline/modules/provider/lmstudio-provider-simple.js';
import { StreamingControlWorkflow } from './dist/modules/pipeline/modules/workflow/streaming-control.js';
import { DebugCenter } from 'rcc-debugcenter';
import { ErrorHandlingCenter } from 'rcc-errorhandling';

// Initialize components
const errorHandlingCenter = new ErrorHandlingCenter();
const debugCenter = new DebugCenter();
await errorHandlingCenter.initialize();

// Create 4-layer pipeline
const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: {
    protocol: 'openai',
    targetFormat: 'lmstudio'
  }
}, { errorHandlingCenter, debugCenter, logger });

const workflow = new StreamingControlWorkflow({
  type: 'streaming-control',
  config: { enableStreaming: true }
}, { errorHandlingCenter, debugCenter, logger });

const compatibility = new LMStudioCompatibility({
  type: 'lmstudio-compatibility',
  config: { toolsEnabled: true }
}, { errorHandlingCenter, debugCenter, logger });

const provider = new LMStudioProviderSimple({
  type: 'lmstudio-http',
  config: {
    baseUrl: 'http://localhost:1234',
    auth: { type: 'apikey', apiKey: 'your-api-key' }
  }
}, { errorHandlingCenter, debugCenter, logger });

// Initialize all modules
await llmSwitch.initialize();
await workflow.initialize();
await compatibility.initialize();
await provider.initialize();

// Process request through 4-layer pipeline
const request = {
  model: 'gpt-oss-20b-mlx',
  messages: [{ role: 'user', content: 'Hello!' }],
  tools: [...] // Optional tools
};

const result = await provider.processIncoming(
  await compatibility.processIncoming(
    await workflow.processIncoming(
      await llmSwitch.processIncoming(request)
    )
  )
);
```

#### Qwen OAuth Provider

```javascript
import { OpenAINormalizerLLMSwitch } from './dist/modules/pipeline/modules/llmswitch/openai-normalizer.js';
import { QwenCompatibility } from './dist/modules/pipeline/modules/compatibility/qwen-compatibility.js';
import { QwenProvider } from './dist/modules/pipeline/modules/provider/qwen-provider.js';
import { StreamingControlWorkflow } from './dist/modules/pipeline/modules/workflow/streaming-control.js';
import { DebugCenter } from 'rcc-debugcenter';
import { ErrorHandlingCenter } from 'rcc-errorhandling';

// Initialize components
const errorHandlingCenter = new ErrorHandlingCenter();
const debugCenter = new DebugCenter();
await errorHandlingCenter.initialize();

// Create 4-layer pipeline with OAuth
const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: { protocol: 'openai', targetFormat: 'qwen' }
}, { errorHandlingCenter, debugCenter, logger });

const workflow = new StreamingControlWorkflow({
  type: 'streaming-control',
  config: { enableStreaming: true }
}, { errorHandlingCenter, debugCenter, logger });

const compatibility = new QwenCompatibility({
  type: 'qwen-compatibility',
  config: { toolsEnabled: true }
}, { errorHandlingCenter, debugCenter, logger });

const provider = new QwenProvider({
  type: 'qwen-provider',
  config: {
    type: 'qwen',
    baseUrl: 'https://chat.qwen.ai',
    oauth: {
      clientId: 'f0304373b74a44d2b584a3fb70ca9e56',
      deviceCodeUrl: 'https://chat.qwen.ai/api/v1/oauth2/device/code',
      tokenUrl: 'https://chat.qwen.ai/api/v1/oauth2/token',
      scopes: ['openid', 'profile', 'email', 'model.completion'],
      tokenFile: './qwen-token.json'
    }
  }
}, { errorHandlingCenter, debugCenter, logger });

// Initialize all modules
await llmSwitch.initialize();
await workflow.initialize();
await compatibility.initialize();
await provider.initialize(); // This will trigger OAuth flow if not authenticated

// Process request - OAuth is handled automatically
const request = {
  model: 'qwen-turbo',
  messages: [{ role: 'user', content: 'Hello! How can you help me?' }]
};

const result = await provider.processIncoming(
  await compatibility.processIncoming(
    await workflow.processIncoming(
      await llmSwitch.processIncoming(request)
    )
  )
);
```

### Configuration

- Set config path via env var:
  - `ROUTECODEX_CONFIG_PATH=~/.routecodex/config/modelscope.json npm start`
- If not set, default path is `~/.routecodex/config.json`
- Single Source Of Truth (SSOT):
  - HTTP port must come from user config only
  - Compatibility must be set in user config (default `passthrough` is recommended first)
  - LLM Switch can be set to `openai-normalizer` to enforce strict OpenAI request shape
  - In `~/.routecodex/config.json`, set top-level `port` and optional host:
    ```json
    {
      "port": 5506,
      "server": { "host": "localhost" }
    }
    ```
  - If no port is provided, the server will fail to start with an explicit error.
- Merged config output: `~/routecodex/config/merged-config.json`

### Helper Scripts

- `test-config.sh`
  - Starts server with `ROUTECODEX_CONFIG_PATH=~/.routecodex/config/modelscope.json`
  - Captures output to `server-output.log` and checks pipeline logs
- `graceful-port-handler.sh <port> [timeout]`
  - Frees port gracefully, then launches server with `ROUTECODEX_CONFIG_PATH` set to the config file path
  - Example: `./graceful-port-handler.sh 5506 10`
  - Note: The HTTP port is determined by your user config. The script argument is only for freeing the port.

### üîß Simplified Logging System

RouteCodex now includes a simplified logging system that provides easy-to-use logging configuration without the complexity of the full debug system.

#### Quick Start
```bash
# Enable simplified logging
routecodex simple-log on --level debug --output console

# Check current status
routecodex simple-log status

# Change log level
routecodex simple-log level error

# Set output to file
routecodex simple-log output file

# Disable simplified logging
routecodex simple-log off
```

#### Features
- **One-click enable/disable**: Simple commands to control logging
- **Multiple log levels**: debug, info, warn, error
- **Flexible output**: console, file, or both
- **Persistent configuration**: Settings survive restarts
- **Automatic application**: Config automatically applied when server starts

#### Configuration File
Settings are stored in `~/.routecodex/simple-log-config.json` and automatically loaded on server startup. Default log output directory is `~/routecodex/debug-logs` when file output is enabled.

#### Benefits
- **Reduced complexity**: 788-line complex indexer ‚Üí 150-line simplified version
- **No memory overhead**: Removed history management and compression
- **Always-on capability**: Once enabled, stays active until explicitly disabled
- **Zero configuration**: Works out of the box with sensible defaults

### Pipeline Scope

- Pipeline uses LLM Switch + Compatibility + Provider.
- LLM Switch `openai-normalizer` performs standard OpenAI normalization.
- Compatibility handles vendor-specific differences; `passthrough` does no changes.

### Tool Calling Example

```javascript
const requestWithTools = {
  model: 'gpt-oss-20b-mlx',
  messages: [
    {
      role: 'system',
      content: 'You are a helpful assistant with access to tools.'
    },
    {
      role: 'user',
      content: 'What is 15 * 25?'
    }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'calculate',
        description: 'Perform mathematical calculations',
        parameters: {
          type: 'object',
          properties: {
            expression: {
              type: 'string',
              description: 'Mathematical expression to evaluate'
            }
          },
          required: ['expression']
        }
      }
    }
  ]
};

// Process through pipeline - tool calls will be automatically handled
const response = await processRequest(requestWithTools);

// Tool calls will be in response.choices[0].message.tool_calls
console.log('Tool calls:', response.choices[0].message.tool_calls);
```

## üìñ Documentation

- [Architecture Documentation](./ARCHITECTURE_DOCUMENTATION.md) - Detailed 4-layer architecture explanation
- [Configuration Guide](./docs/CONFIG_ARCHITECTURE.md) - Configuration options and examples
- [Pipeline Architecture](./docs/pipeline/ARCHITECTURE.md) - Pipeline system details
- [LM Studio Integration](./docs/lmstudio-tool-calling.md) - LM Studio specific setup
- [Dry-Run System](./docs/dry-run/README.md) - Comprehensive dry-run debugging framework
- [Bidirectional Pipeline](./docs/dry-run/bidirectional-pipeline.md) - Advanced bidirectional pipeline features

## üîß Configuration

### Basic Configuration

```json
{
  "server": {
    "port": 5506,
    "host": "localhost"
  },
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234",
      "apiKey": "your-api-key"
    }
  },
  "routing": {
    "default": "lmstudio"
  },
  "compatibility": "passthrough",
  "features": {
    "tools": {
      "enabled": true,
      "maxTools": 10
    },
    "streaming": {
      "enabled": true,
      "chunkSize": 1024
    }
  }
}
```

### Advanced Pipeline Configuration

For detailed pipeline configuration, you can specify individual modules:

```json
{
  "pipeline": {
    "llmSwitch": {
      "type": "llmswitch-openai-openai",
      "config": {
        "protocol": "openai",
        "targetFormat": "lmstudio"
      }
    },
    "workflow": {
      "type": "streaming-control",
      "config": {
        "enableStreaming": true,
        "bufferSize": 1024,
        "timeout": 30000
      }
    },
    "compatibility": {
      "type": "lmstudio-compatibility",
      "config": {
        "toolsEnabled": true,
        "customRules": [
          {
            "id": "ensure-standard-tools-format",
            "transform": "mapping",
            "sourcePath": "tools",
            "targetPath": "tools",
            "mapping": {
              "type": "type",
              "function": "function"
            }
          }
        ]
      }
    },
    "provider": {
      "type": "lmstudio-http",
      "config": {
        "type": "lmstudio",
        "baseUrl": "http://localhost:1234",
        "auth": {
          "type": "apikey",
          "apiKey": "your-api-key"
        },
        "timeout": 60000
      }
    }
  }
}
```

## üß™ Testing

```bash
# Run all tests
npm test

# Run specific test
npm test -- --testNamePattern="tool calling"

# Run integration test with LM Studio
node test-llmswitch-workflow-integration.mjs

# Run dry-run system tests
node test-all-nodes-dry-run.mjs

# Run bidirectional pipeline tests
node test-bidirectional-pipeline-dry-run.mjs

# Run build
npm run build
```

## üéØ Supported Providers

### ‚úÖ Currently Supported (Fully Operational)
- **LM Studio**: Local AI model hosting with full tool support and comprehensive dry-run analysis
- **Qwen**: Alibaba's language models with OAuth 2.0 authentication and API key fallback
- **iFlow**: AI service provider with OAuth 2.0 + PKCE authentication, Kimi model support
- **OpenAI**: GPT models with function calling
- **Anthropic**: Claude model family

### üîß Latest Updates (v0.2.7)
This release enables full operational capability for three major AI providers:

- **LM Studio**: Complete tool calling implementation with dry-run testing framework
- **Qwen**: Working OAuth authentication with automatic token refresh and caching
- **iFlow**: Fixed authentication endpoints and API configuration, Kimi model integration
- **Unified Auth Framework**: EnhancedAuthResolver supporting multiple authentication types
- **Build System**: Resolved TypeScript compilation issues
- **üîß Offline Logging CLI**: New comprehensive CLI for offline log capture and analysis
  - Module-level and pipeline-level offline logging configuration
  - Automatic log file management with rotation and compression
  - Built-in log analysis with HTML visualization reports
  - Time series analysis for performance monitoring
  - Zero-dependency offline operation

### üîê OAuth Authentication Details

#### Qwen OAuth Provider
- **Authentication**: OAuth 2.0 Device Flow
- **Token Management**: Automatic refresh with persistent storage
- **Security**: Standard OAuth 2.0 security model
- **Models**: qwen-turbo, qwen-max, qwen-turbo-latest

#### iFlow OAuth Provider
- **Authentication**: OAuth 2.0 Device Flow with PKCE
- **Token Management**: Automatic refresh with persistent storage
- **Security**: Enhanced PKCE (Proof Key for Code Exchange) protection
- **Models**: iflow-turbo, iflow-pro, iflow-turbo-latest

### üîÑ Planned Support
- Google Gemini
- Cohere
- Custom provider framework

## üìä Performance

- **Request Processing**: < 50ms overhead per layer
- **Tool Calling**: Native provider optimization
- **Streaming**: Real-time chunk processing
- **Memory Usage**: Efficient module loading and caching

## üß™ Ê†áÂáÜOpenAI APIÊµãËØïÊñπÊ≥ï

RouteCodexÊèê‰æõÂÆåÊï¥ÁöÑOpenAIÂÖºÂÆπAPIÊµãËØïÔºåÁ°Æ‰øù‰∏éÂêÑÁßçAIÂÆ¢Êà∑Á´ØÁöÑÂÆåÁæéÈõÜÊàê„ÄÇ

### Á´ØÂà∞Á´ØÊµãËØïÊµÅÁ®ã

#### 1. ÂêØÂä®ÊúçÂä°Âô®
```bash
# ‰ΩøÁî®ModelScopeÈÖçÁΩÆÂêØÂä®
ROUTECODEX_CONFIG_PATH=~/.routecodex/config/modelscope.json npm start

# ÊúçÂä°Âô®Â∞ÜÂú®ÈÖçÁΩÆÁöÑÁ´ØÂè£‰∏äÂêØÂä® (ÈªòËÆ§5506)
# ÂÅ•Â∫∑Ê£ÄÊü•: curl http://localhost:5506/health
```

#### 2. Ê†áÂáÜÊµãËØïÂëΩ‰ª§

**Âü∫Á°ÄÂØπËØùÊµãËØï**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "Hello, please respond briefly"
      }
    ]
  }'
```

**Â∑•ÂÖ∑Ë∞ÉÁî®ÊµãËØï**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "What is 15 * 25? Use the calculator tool."
      }
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate",
          "description": "Perform mathematical calculations",
          "parameters": {
            "type": "object",
            "properties": {
              "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
              }
            },
            "required": ["expression"]
          }
        }
      }
    ]
  }'
```

**ÊµÅÂºèÂìçÂ∫îÊµãËØï**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "Count from 1 to 5 slowly"
      }
    ],
    "stream": true
  }'
```

#### 3. È™åËØÅÁ´ØÁÇπ

**ÊúçÂä°Âô®Áä∂ÊÄÅ**:
```bash
# Ê†πÁ´ØÁÇπ - ÊòæÁ§∫Á≥ªÁªü‰ø°ÊÅØ
curl http://localhost:5506/

# ÂÅ•Â∫∑Ê£ÄÊü•
curl http://localhost:5506/health

# ÈÖçÁΩÆ‰ø°ÊÅØ
curl http://localhost:5506/config
```

#### 4. ÂÆåÊï¥ÊµãËØïËÑöÊú¨

ÂàõÂª∫ÊµãËØïËÑöÊú¨ `test-modelscope.sh`:
```bash
#!/bin/bash

# ModelScope APIÊµãËØïËÑöÊú¨
set -e

BASE_URL="http://localhost:5506"
API_KEY="${MS_API_KEY:-YOUR_MODELSCOPE_API_KEY}"
MODEL="Qwen/Qwen3-Coder-480B-A35B-Instruct"

echo "üß™ RouteCodex ModelScope APIÊµãËØïÂºÄÂßã..."
echo "üì° ÊµãËØïÂú∞ÂùÄ: $BASE_URL"
echo "üîë ‰ΩøÁî®Ê®°Âûã: $MODEL"
echo ""

# 1. ÂÅ•Â∫∑Ê£ÄÊü•
echo "1Ô∏è‚É£ ÂÅ•Â∫∑Ê£ÄÊü•..."
curl -s "$BASE_URL/health" | jq .

# 2. Âü∫Á°ÄÂØπËØùÊµãËØï
echo ""
echo "2Ô∏è‚É£ Âü∫Á°ÄÂØπËØùÊµãËØï..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"Hello! Please respond briefly.\"
      }
    ]
  }" | jq .

# 3. ‰ª£Á†ÅÁîüÊàêÊµãËØï
echo ""
echo "3Ô∏è‚É£ ‰ª£Á†ÅÁîüÊàêÊµãËØï..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"Write a simple Python function to calculate factorial.\"
      }
    ]
  }" | jq .

# 4. Â∑•ÂÖ∑Ë∞ÉÁî®ÊµãËØï
echo ""
echo "4Ô∏è‚É£ Â∑•ÂÖ∑Ë∞ÉÁî®ÊµãËØï..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"What is the square root of 144?\"
      }
    ],
    \"tools\": [
      {
        \"type\": \"function\",
        \"function\": {
          \"name\": \"calculate\",
          \"description\": \"Perform mathematical calculations\",
          \"parameters\": {
            \"type\": \"object\",
            \"properties\": {
              \"expression\": {
                \"type\": \"string\",
                \"description\": \"Mathematical expression to evaluate\"
              }
            },
            \"required\": [\"expression\"]
          }
        }
      }
    ]
  }" | jq .

echo ""
echo "‚úÖ ÊâÄÊúâÊµãËØïÂÆåÊàê!"
```

#### 5. ÊµãËØïÁªìÊûúÈ™åËØÅ

ÊàêÂäüÁöÑÊµãËØïÂìçÂ∫îÂ∫îËØ•ÂåÖÂê´:
- **Áä∂ÊÄÅÁ†Å**: 200 OK
- **ÂìçÂ∫îÁªìÊûÑ**: Ê†áÂáÜOpenAIÊ†ºÂºè
- **Â§ÑÁêÜÊó∂Èó¥**: ÈÄöÂ∏∏ < 2Áßí
- **Token‰ΩøÁî®**: ÂêàÁêÜÁöÑtokenËÆ°Êï∞
- **Ë∑ØÁî±‰ø°ÊÅØ**: ÂåÖÂê´ `_metadata` Â≠óÊÆµ

#### 6. ÈÖçÁΩÆÊñá‰ª∂Á§∫‰æã

`~/.routecodex/config/modelscope.json`:
```json
{
  "port": 5506,
  "virtualrouter": {
    "inputProtocol": "openai",
    "outputProtocol": "openai",
    "providers": {
      "modelscope": {
        "type": "openai",
        "baseURL": "https://api-inference.modelscope.cn/v1",
        "apiKey": [
          "YOUR_MODELSCOPE_KEY1",
          "YOUR_MODELSCOPE_KEY2",
          "YOUR_MODELSCOPE_KEY3",
          "YOUR_MODELSCOPE_KEY4"
        ],
        "models": {
          "Qwen/Qwen3-Coder-480B-A35B-Instruct": {
            "maxContext": 262144,
            "maxTokens": 262144
          }
        }
      }
    },
    "routing": {
      "default": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ],
      "coding": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ],
      "longcontext": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ]
    }
  }
}
```

#### 7. Â∏∏ËßÅÈóÆÈ¢òÊéíÊü•

**ÈóÆÈ¢ò1: 404 Not Found**
```bash
# Ê£ÄÊü•Á´ØÁÇπÊòØÂê¶Ê≠£Á°Æ
curl http://localhost:5506/
# Â∫îËØ•ÁúãÂà∞ÂèØÁî®Á´ØÁÇπÂàóË°®ÔºåOpenAIÁ´ØÁÇπÊòØ /v1/*
```

**ÈóÆÈ¢ò2: ËÆ§ËØÅÂ§±Ë¥•**
```bash
# Ê£ÄÊü•APIÂØÜÈí•Ê†ºÂºè
# ÂØÜÈí•Â∫îËØ•ÊòØ: ms-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

**ÈóÆÈ¢ò3: Ê®°ÂûãÈîôËØØ**
```bash
# Ê£ÄÊü•Ê®°ÂûãÂêçÁß∞ÊòØÂê¶Âú®ÈÖçÁΩÆ‰∏≠Â≠òÂú®
# ‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÂÆö‰πâÁöÑÁ°ÆÂàáÊ®°ÂûãÂêçÁß∞
```

#### 8. ÊÄßËÉΩÂü∫ÂáÜ

- **ÂìçÂ∫îÊó∂Èó¥**: 800-1500ms (ÂèñÂÜ≥‰∫éÁΩëÁªúÂíåÊ®°ÂûãÂ§çÊùÇÂ∫¶)
- **TokenÂ§ÑÁêÜ**: ÊîØÊåÅ262K‰∏ä‰∏ãÊñáÈïøÂ∫¶
- **Âπ∂ÂèëËÉΩÂäõ**: ÊîØÊåÅÂ§öËØ∑Ê±ÇÂπ∂ÂèëÂ§ÑÁêÜ
- **ÈîôËØØÂ§ÑÁêÜ**: ÂÆåÊï¥ÁöÑÈîôËØØÊÅ¢Â§çÊú∫Âà∂

ËøôÂ•óÊ†áÂáÜÊµãËØïÊñπÊ≥ïÁ°Æ‰øùRouteCodex‰∏é‰ªª‰ΩïOpenAIÂÖºÂÆπÂÆ¢Êà∑Á´ØÁöÑÂÆåÁæéÈõÜÊàê„ÄÇ

## ‚úÖ ModelScope Áã¨Á´ãÈ™åËØÅÈÄöËøá

- Êó•Êúü: 2025-09-26
- ÈÖçÁΩÆ: `~/.routecodex/config/modelscope.json`
- Á´ØÂè£: 5507ÔºàÁ§∫‰æãÁéØÂ¢ÉÔºâ
- È™åËØÅÈ°π:
  - ÈùûÊµÅÂºèËøîÂõû OpenAI Ê†áÂáÜÁªìÊûÑÔºàid/object/created/model/choices/usageÔºâ
  - ÊµÅÂºèÔºàSSEÔºâÂàÜÁâáËßÑËåÉÔºàobject: chat.completion.chunkÔºådelta.contentÔºåÊúâ [DONE]Ôºâ
  - ‰∏•Ê†º JSON Ê®°ÂºèÔºö`response_format: { "type": "json_object" }` ÂÜÖÂÆπÊ∏ÖÊ¥ó‰∏∫Á∫Ø JSON Â≠óÁ¨¶‰∏≤
  - 429 Ë∞ÉÂ∫¶ÔºöÁî± PipelineManager Áªü‰∏ÄÂ§ÑÁêÜÔºåÂ§ö Key/Pipeline ËΩÆËØ¢ÈáçËØïÔºåÂÖ®ÈÉ®ÊûØÁ´≠ÊâçËøîÂõû 429
  - Áªü‰∏ÄÂìçÂ∫îÂ§¥Ôºö`x-request-id`„ÄÅ`Cache-Control: no-store`„ÄÅ`Content-Type: application/json; charset=utf-8`

ËØ¶ÊÉÖËßÅ docs/verification/modelscope-verify.md„ÄÇ

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **LM Studio**: For providing excellent local AI model hosting
- **OpenAI**: For the standard API specification
- **TypeScript**: For enabling type-safe development
- **Node.js**: For the robust runtime environment

## üìû Support

For support, please:
1. Check the [documentation](./docs/)
2. Search existing [issues](https://github.com/your-username/routecodex/issues)
3. Create a new issue with detailed information

---

**Built with ‚ù§Ô∏è using the 4-layer architecture pattern**
