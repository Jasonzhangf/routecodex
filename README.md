# RouteCodex - RouteCodex Claude Code Router

[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)
[![Architecture](https://img.shields.io/badge/Architecture-4--layer-purple.svg)](./ARCHITECTURE_DOCUMENTATION.md)

A sophisticated AI service routing and transformation system that provides seamless integration with multiple AI providers through a clean, modular architecture.

Note: Node.js engines updated ‚Äî requires Node >=20 and <26. Global installs on Node 24 are supported.

Release 0.41.1
- Fix: Anthropic streaming compliance for tool_use. RouteCodex now emits standard SSE events for tool inputs so Claude Code correctly accumulates parameters during streaming tool calls.

Important: Anthropic tool-calls (since 0.4.0)
- By default we trust tool schemas (trustSchema=true). When converting OpenAI `tool_calls` to Anthropic `tool_use`, we preserve `function.name` and `arguments` as-is without renaming tool names or remapping argument fields. This aligns with Claude Code Router and guarantees correct tool flow (emitting `stop_reason=tool_use` on `message_delta`).

## ‚úÖ GLM‚Äë4.6 Works With Codex CLI

RouteCodex now supports running GLM‚Äë4.6 with Codex CLI through the OpenAI‚Äëcompatible endpoint. Highlights:

- Wire API: `chat` (Codex requires chat protocol)
- Tool calls: reconstructed in Compatibility (no tool text leakage)
- Thinking: enabled for GLM‚Äë4.6; private <think>‚Ä¶</think> is stripped from output while preserving model reasoning behavior
- SSE: router emits `delta.tool_calls` when needed; content is suppressed to keep tool calls at the end

Quick setup

1) Start RouteCodex (default port 5520)

```bash
npm run dev
# OpenAI endpoint: http://localhost:5520/v1/openai
```

Singleton server (avoid port conflicts)

- Background (recommended): `npm run start:bg` uses `scripts/run-bg.sh`, which now checks the target port (default 5520 or `~/.routecodex/config.json:port`).
  - If another instance is listening, the command exits with code 9 and a hint to use `--replace`.
  - To force replace: `bash scripts/run-bg.sh --replace -- 'node dist/index.js'` (or set `ROUTECODEX_PORT`/`--port <n>` when not using default).

- Foreground (timed): `npm run start:fg` uses `scripts/run-fg-gtimeout.sh <timeout> -- ...`, which also supports `--replace` and `--port`.
  - Example: `bash scripts/run-fg-gtimeout.sh 12 --replace -- 'node dist/index.js'`.

This prevents accidental coexistence of a dev worktree server and a globally installed server on the same port.

2) Ensure user config includes GLM provider (auto‚Äëgenerated by default at `~/.routecodex/config.json`):

```json
{
  "virtualrouter": {
    "providers": {
      "glm": {
        "type": "glm",
        "baseURL": "https://open.bigmodel.cn/api/coding/paas/v4",
        "apiKey": ["<YOUR_GLM_KEY>"],
        "models": {
          "glm-4.6": {
            "compatibility": { "type": "glm-compatibility", "config": { "thinking": { "enabled": true, "payload": { "type": "enabled" } } } }
          }
        }
      }
    },
    "routing": { "default": ["glm.glm-4.6"] }
  },
  "port": 5520
}
```

3) Configure Codex profile (`~/.codex/config.toml`):

```toml
[model_providers.rc]
name = "glm"
base_url = "http://localhost:5520/v1/openai"
wire_api = "chat"
api_key = "<YOUR_GLM_KEY>"

[profiles.rc]
model_provider = "rc"
model = "glm-4.6"
```

4) Test with Codex

```bash
codex -p rc "‰Ω†Â•ΩÔºåÂàóÂá∫ÂΩìÂâçÁõÆÂΩï (Ëß¶ÂèëÂ∑•ÂÖ∑Ë∞ÉÁî®)"
```

Notes
- Compatibility converts `<invoke ‚Ä¶>`, `<function ‚Ä¶>`, `<function_call>{‚Ä¶}</function_call>` and `[tool_call:name]{‚Ä¶}` into OpenAI `tool_calls`.
- Private reasoning blocks `<think>‚Ä¶</think>` are removed from the visible content to keep OpenAI/Codex semantics intact.

## üß≠ Responses Module (Design Overview)

We decouple ‚Äúconversion‚Äù from ‚Äústreaming‚Äù and drive both via JSON configuration.

- Conversion (Requests/Responses)
  - Requests: Convert OpenAI Responses input (nested `input[]` with `message` + `content[{type,text}]`) into Chat messages. Expansion is recursive and controlled by JSON mapping (no hard-coding).
  - Responses: Convert provider Chat payloads back to OpenAI Responses JSON (text, tools, usage). llmswitch preferred; fallback mapper based on JSON.

- Streaming (SSE)
  - SSE only reads normalized Responses objects and emits `response.*` events. Options for message lifecycle, required_action, and heartbeat are configurable.

- Where to configure
  - Module switches: `config/modules.json` ‚Üí `responses` section (useLlmswitch, fallback, provider non-stream, SSE lifecycle/required_action/heartbeat)
  - Field mappings: `config/responses-conversion.json` (wrapper/type/text/blocks keys; allowed content types; text extraction paths; tool args paths)
  - Env overrides: `ROUTECODEX_RESP_*` variables

- Files
  - `src/server/config/responses-config.ts` ‚Äî load module config + mappings
  - `src/server/conversion/responses-converter.ts` ‚Äî conversions (Responses‚ÜîChat)
  - `src/server/handlers/responses.ts` ‚Äî handler: calls converter, then streams SSE based on config

- Debug artifacts
  - `~/.routecodex/codex-samples/anth-replay/raw-request_req_<RID>.json`
  - `~/.routecodex/codex-samples/anth-replay/pre-pipeline_req_<RID>.json`
  - `~/.routecodex/codex-samples/anth-replay/sse-events-req_<RID>.log`

### üî¨ Responses SSE Implementation Details

This router aligns its SSE output with the OpenAI Responses API event model. Key details:

- Event order (tool‚Äëfirst typical path)
  1) `response.created` ‚Üí 2) `response.in_progress`
  3) `response.output_item.added`(reasoning, `output_index=0`)
  4) `response.reasoning_summary_part.added`(summary_index=0)
  5) multiple `response.reasoning_summary_text.delta` (with `obfuscation` placeholder)
  6) `response.reasoning_summary_text.done` ‚Üí `response.reasoning_summary_part.done`
  7) repeat 4‚Äì6 (summary_index=1)
  8) `response.output_item.added`(function_call, `output_index=2`)
  9) `response.content_part.added`(input_json)
  10) multiple `response.function_call_arguments.delta`
  11) `response.function_call_arguments.done`
  12) `response.output_item.done`(function_call)
  13) `response.completed`

- Event order (text‚Äëfirst): insert message lifecycle between reasoning and function_call
  - `response.output_item.added`(message, `output_index=1`) ‚Üí `response.content_part.added` ‚Üí multiple `response.output_text.delta` ‚Üí `response.output_text.done` ‚Üí `response.content_part.done` ‚Üí `response.output_item.done`(message)

- Required fields and indices
  - All events carry `sequence_number`, starting at 0 and incrementing by 1 per event.
  - `created_at` (seconds) is used for `response.{created|in_progress|completed}.response.created_at`.
  - `output_index` mapping is fixed: `0=reasoning`, `1=message`, `2=function_call`; `content_index` is 0.
  - Reasoning:
    - `response.output_item.added`(reasoning) ‚Üí `item` includes `{ id, type: "reasoning", encrypted_content, summary: [] }`.
    - `response.reasoning_summary_part.added/done` and `response.reasoning_summary_text.delta/done` include `item_id/output_index/summary_index`; deltas include `obfuscation` placeholder.
  - Message:
    - `response.output_item.added`(message) ‚Üí `item` includes `{ id, type: "message", role: "assistant", status: "in_progress", content: [] }`.
    - `response.content_part.added`(message) ‚Üí `part: { type: "output_text", annotations: [], logprobs: [], text: "" }`.
    - `response.output_text.delta/done` include `item_id/output_index/content_index/logprobs` (empty arrays are valid).
  - Function call:
    - `response.output_item.added`(function_call) ‚Üí `item` includes `{ id, type: "function_call", call_id, name, status: "in_progress", arguments: "" }`.
    - `response.content_part.added`(function_call) ‚Üí `part: { type: "input_json", partial_json: "" }`.
    - `response.function_call_arguments.delta` ‚Üí `item_id/output_index/delta`; `done` ‚Üí `item_id/output_index/arguments/name`.
    - `response.output_item.done`(function_call) ‚Üí `{ status: "completed", arguments, call_id, name }`.
  - `response.completed` ‚Üí `response` contains `output` array (reasoning/message/function_call in order) and `usage.input_tokens|output_tokens|total_tokens`. No top‚Äëlevel `usage` or `required_action` in events.

- Message lifecycle emission policy
  - Message lifecycle is emitted only when there is actual text delta; in tool‚Äëfirst turns, no empty message skeleton is sent. Tool‚Äëfirst detection recognizes `function_call/tool_call/tool_use`.

- Not emitted
  - `response.required_action` is not emitted in Responses SSE.

- Tool execution
  - The server never executes tools; the client executes tools and decides follow‚Äëup turns.

- Heartbeat
  - Controlled by `responses.sse.heartbeatMs` (or env `ROUTECODEX_RESPONSES_HEARTBEAT_MS`). When >0, the router sends SSE heartbeats to keep connections alive.

- Mapping and configuration
  - Request mapping (Responses‚ÜíChat) is driven by `llmswitch-response-chat` and `config/responses-conversion.json`.
  - Response mapping (Chat‚ÜíResponses) emits `created_at` and `function_call`; the SSE layer replays events as specified above.
  - Implementation references:
    - Event replay: `src/server/handlers/responses.ts`
    - Mapping: `src/modules/pipeline/modules/llmswitch/llmswitch-response-chat.ts`, `src/server/conversion/responses-mapper.ts`
    - Config: `config/responses-conversion.json`, `src/server/config/responses-config.ts`

## üèóÔ∏è 4-Layer Architecture

RouteCodex implements a sophisticated 4-layer pipeline architecture that provides clean separation of concerns and flexible protocol handling:

```
HTTP Request ‚Üí LLM Switch ‚Üí Workflow ‚Üí Compatibility ‚Üí Provider ‚Üí AI Service
     ‚Üì             ‚Üì          ‚Üì            ‚Üì           ‚Üì
  Request      Protocol   Flow       Format     Standard     Response
  Analysis     Routing    Control    Conversion   HTTP Server  Processing
```

### Layer 1: LLM Switch (Dynamic Routing + Standardization)
- **Request Analysis**: Analyzes incoming requests to determine optimal routing
- **Protocol Routing**: Routes requests to appropriate processing pipelines
- **OpenAI Standardization**: Enforces strict OpenAI Chat Completions shape before downstream modules
  - `assistant.tool_calls[].function.arguments` must be a JSON string (objects are stringified)
  - `assistant.content` must be a string (default "")
  - `tool.content` must be a string
- **Dynamic Classification**: Supports 7 routing categories (default, longcontext, thinking, background, websearch, vision, coding)

### Layer 2: Workflow (Flow Control)
- **Streaming Control**: Handles streaming/non-streaming request conversion
- **Request Processing**: Manages request flow and processing state
- **Response Handling**: Processes and transforms responses as needed

### Layer 3: Compatibility (Format Transformation - Vendor Differences Only)
- **Protocol Translation**: Converts between different AI service protocols
- **Format Adaptation**: Transforms request/response formats between providers
- **Tool Integration**: Handles tool calling format conversion and execution
- **Configuration-Driven**: Uses JSON configuration for transformation rules
- **Scope Rule**: Compatibility only handles vendor-specific differences. All ‚Äústandard OpenAI‚Äù normalization lives in the LLM Switch.
- **Simple String Format**: Supports easy compatibility specification (e.g., `"compatibility": "passthrough"`)

### Passive Monitoring (Design‚ÄëReady, not enabled by default)
- A passive Monitoring skeleton is included to capture request/response artifacts for OpenAI and Anthropic without changing behavior.
- Records (when enabled in future) are stored under `~/.routecodex/monitor/sessions/<YYYYMMDD>/<protocol>/<reqId>/` and can be used for:
  - Offline routing dry‚Äërun (simulate routing decisions from recorded inputs)
  - Response replay (non‚Äëstream/stream events)
- See `docs/monitoring/Design.md` for details.

### Virtual Router Dry‚ÄëRun (Matrix)
- Run a local matrix dry‚Äërun against your config (no provider calls):

```bash
npm run build
node scripts/virtualrouter-dry-run-matrix.mjs --config ~/.routecodex/config/verified_0.46.32/multi-provider.json
```

## üîß Compatibility Field Configuration

The compatibility field supports both simple string format and complex object format, providing flexible configuration options for different use cases.

### Simple String Format

For basic usage, you can specify compatibility as a simple string:

```json
{
  "compatibility": "passthrough"
}
```

#### Supported String Values:
- `"passthrough"` - Direct pass-through without transformation (default)
- `"lmstudio"` - LM Studio compatibility mode
- `"qwen"` - Qwen compatibility mode
- `"iflow"` - iFlow compatibility mode
- Multiple providers: `"lmstudio/qwen"` - Will use first available provider

### Complex Object Format

For advanced configuration, use the complex object format:

```json
{
  "compatibility": {
    "type": "lmstudio-compatibility",
    "config": {
      "toolsEnabled": true,
      "customRules": [
        {
          "id": "ensure-standard-tools-format",
          "transform": "mapping",
          "sourcePath": "tools",
          "targetPath": "tools",
          "mapping": {
            "type": "type",
            "function": "function"
          }
        }
      ]
    }
  }
}
```

### Priority Hierarchy

The system follows this priority order when determining compatibility:

1. **User Config Compatibility** (highest priority)
2. **Model-Level Compatibility**
3. **Provider-Level Compatibility**
4. **Auto-Inference** (fallback based on provider type)

> Note on inference: To keep ‚Äúconfiguration as the single source of truth‚Äù, prefer setting `compatibility` explicitly (default is `passthrough`). Inference may be removed in future revisions.

### Configuration Examples

#### Basic Passthrough Configuration
```json
{
  "server": {
    "port": 5506
  },
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234",
      "apiKey": "your-api-key"
    }
  },
  "compatibility": "passthrough"
}
```

#### Multi-Provider Compatibility
```json
{
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234"
    },
    "qwen": {
      "type": "qwen",
      "baseUrl": "https://chat.qwen.ai"
    }
  },
  "compatibility": "lmstudio/qwen"
}
```

#### Advanced Configuration with Custom Rules
```json
{
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234"
    }
  },
  "compatibility": {
    "type": "lmstudio-compatibility",
    "config": {
      "toolsEnabled": true,
      "customRules": [
        {
          "id": "tool-format-conversion",
          "transform": "mapping",
          "sourcePath": "tools",
          "targetPath": "tools",
          "mapping": {
            "type": "type",
            "function": "function"
          }
        }
      ]
    }
  }
}
```

### Layer 4: Provider (Standard HTTP Server)
- **HTTP Communication**: Manages all HTTP communications with AI services
- **Authentication**: Handles provider authentication and authorization
- **Error Handling**: Manages network errors and provider-specific issues

## ‚ú® Key Features

### üîß Tool Calling Support
- Full OpenAI-compatible tool calling implementation
- Supports multiple AI providers (LM Studio, OpenAI, Qwen, iFlow, Anthropic)
- Automatic tool format conversion and execution
- Real-time tool response processing

### üîê OAuth 2.0 Authentication
- Secure OAuth 2.0 Device Flow implementation
- Automatic token management and refresh
- PKCE (Proof Key for Code Exchange) security
- Persistent token storage and recovery

## iFlow OAuth (CLI‚ÄëAligned)

This project‚Äôs iFlow provider implements the same end‚Äëto‚Äëend authorization behavior as the official iFlow CLI:

- Flow priority
  - Authorization Code + local callback (opens your default browser)
  - Fallback: Device Code (prints `verification_uri` and `user_code`; also opens browser when possible)

- Endpoints (aligned with iFlow CLI)
  - Authorization: `https://iflow.cn/oauth`
  - Token exchange: `https://iflow.cn/oauth/token`
  - Device code: `https://iflow.cn/oauth/device/code` (auto‚Äëfallback with `/device_code` variant)
  - LLM API (OpenAI‚Äëcompatible): `https://apis.iflow.cn/v1`

- Token storage (JSON)
  - File: `~/.iflow/oauth_creds.json`
  - Fields: `access_token`, `refresh_token`, `expires_at`, `token_type`, `scope`, `apiKey`
  - Note: `apiKey` is fetched after OAuth (`/api/oauth/getUserInfo?accessToken=...`) and is used for LLM API calls as the primary credential

- Auth header selection for LLM API
  - Prefer: `Authorization: Bearer <apiKey>`
  - Fallback: `Authorization: Bearer <access_token>`

- Client credentials (optional, CLI‚Äëstyle)
  - If you have official client credentials, set:
    - `IFLOW_CLIENT_ID`
    - `IFLOW_CLIENT_SECRET`
  - When `client_secret` is present, token exchange uses `Authorization: Basic base64(client_id:client_secret)` (same as CLI)
  - If not provided, PKCE is used where supported; otherwise device flow is used

### Quick Start: iFlow Auth

- One‚Äëshot auth helper (opens browser; writes `~/.iflow/oauth_creds.json`):

```bash
node --import tsx scripts/iflow-auth.ts
```

- Verify token file (should include `apiKey`):

```bash
sed -n '1,200p' ~/.iflow/oauth_creds.json
```

- Direct API sanity check (replace `<MODEL_ID>` with an available model for your account):

```bash
APIKEY=$(node -e 'const fs=require("fs");const p=process.env.HOME+"/.iflow/oauth_creds.json";if(fs.existsSync(p)){const j=JSON.parse(fs.readFileSync(p,"utf8"));if(j.apiKey)process.stdout.write(j.apiKey)}')
curl -sS \
  -H "Authorization: Bearer $APIKEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"<MODEL_ID>","messages":[{"role":"user","content":"Áî®‰∏ÄÂè•‰∏≠ÊñáËá™Êàë‰ªãÁªç„ÄÇ"}]}' \
  https://apis.iflow.cn/v1/chat/completions
```

### Server Config Examples

- Minimal provider (OAuth stored in default path):

```json
{
  "virtualrouter": {
    "providers": {
      "iflow": {
        "type": "iflow",
        "baseURL": "https://api.iflow.cn/v1",
        "oauth": {
          "deviceCodeUrl": "https://iflow.cn/oauth/device/code",
          "tokenUrl": "https://iflow.cn/oauth/token",
          "tokenFile": "~/.iflow/oauth_creds.json"
        },
        "compatibility": { "type": "iflow-compatibility" },
        "models": { "qwen3-coder": { "maxContext": 128000, "maxTokens": 8192 } }
      }
    }
  }
}
```

- With client credentials (CLI‚Äëstyle Basic token exchange):

```json
{
  "virtualrouter": {
    "providers": {
      "iflow": {
        "type": "iflow",
        "baseURL": "https://api.iflow.cn/v1",
        "oauth": {
          "deviceCodeUrl": "https://iflow.cn/oauth/device/code",
          "tokenUrl": "https://iflow.cn/oauth/token",
          "tokenFile": "~/.iflow/oauth_creds.json",
          "clientId": "${IFLOW_CLIENT_ID}",
          "clientSecret": "${IFLOW_CLIENT_SECRET}"
        },
        "compatibility": { "type": "iflow-compatibility" }
      }
    }
  }
}
```

### Troubleshooting

- 404 on `iflow.cn/v1/...` or `api.iflow.cn/v1/...`: use `apis.iflow.cn/v1` for LLM API
- 40308 (‚ÄúAPIKEYÈúÄË¶ÅÂçáÁ∫ß‚Ä¶‚Äù): regenerate your apiKey in iFlow user settings and retry
- 400 ‚ÄúNo provider for model: ‚Ä¶‚Äù: the model is not enabled for your apiKey/account; choose a model available to your plan
- Device code HTML/portal pages: the provider auto‚Äëretries with alternate path and may switch host to `api.iflow.cn` when necessary


### üåä Streaming Support
- Native streaming request/response handling
- Automatic streaming/non-streaming conversion
- Chunked response processing
- Backward compatibility with non-streaming clients

### üîÑ Multi-Protocol Support
- OpenAI-compatible API endpoints (`/v1/chat/completions`)
- OpenAI Responses API support (`/v1/responses`)
- Native provider protocol support
- Automatic protocol translation
- Configuration-driven transformations

### üéØ Dynamic Routing
- Intelligent request routing based on content analysis
- 7 specialized routing categories
- Performance-optimized pipeline selection
- Custom routing rules support

### üß™ Advanced Dry-Run System
- **Comprehensive Pipeline Debugging**: Node-level dry-run execution with "pipeline break" debugging
- **Input Simulation**: Intelligent mock data generation for all-nodes dry-run scenarios
- **Bidirectional Pipeline**: Request and response pipeline dry-run with real server response integration
- **Memory Management**: Intelligent resource cleanup and memory leak prevention
- **Error Boundaries**: Multi-level error handling with automatic recovery mechanisms

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/routecodex.git
cd routecodex

# Install dependencies
npm install

# Build the project
npm run build
```

### Basic Usage

#### LM Studio Provider (API Key)

```javascript
import { OpenAINormalizerLLMSwitch } from './dist/modules/pipeline/modules/llmswitch/openai-normalizer.js';
import { LMStudioCompatibility } from './dist/modules/pipeline/modules/compatibility/lmstudio-compatibility.js';
import { LMStudioProviderSimple } from './dist/modules/pipeline/modules/provider/lmstudio-provider-simple.js';
import { StreamingControlWorkflow } from './dist/modules/pipeline/modules/workflow/streaming-control.js';
import { DebugCenter } from 'rcc-debugcenter';
import { ErrorHandlingCenter } from 'rcc-errorhandling';

// Initialize components
const errorHandlingCenter = new ErrorHandlingCenter();
const debugCenter = new DebugCenter();
await errorHandlingCenter.initialize();

// Create 4-layer pipeline
const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: {
    protocol: 'openai',
    targetFormat: 'lmstudio'
  }
}, { errorHandlingCenter, debugCenter, logger });

const workflow = new StreamingControlWorkflow({
  type: 'streaming-control',
  config: { enableStreaming: true }
}, { errorHandlingCenter, debugCenter, logger });

const compatibility = new LMStudioCompatibility({
  type: 'lmstudio-compatibility',
  config: { toolsEnabled: true }
}, { errorHandlingCenter, debugCenter, logger });

const provider = new LMStudioProviderSimple({
  type: 'lmstudio-http',
  config: {
    baseUrl: 'http://localhost:1234',
    auth: { type: 'apikey', apiKey: 'your-api-key' }
  }
}, { errorHandlingCenter, debugCenter, logger });

// Initialize all modules
await llmSwitch.initialize();
await workflow.initialize();
await compatibility.initialize();
await provider.initialize();

// Process request through 4-layer pipeline
const request = {
  model: 'gpt-oss-20b-mlx',
  messages: [{ role: 'user', content: 'Hello!' }],
  tools: [...] // Optional tools
};

const result = await provider.processIncoming(
  await compatibility.processIncoming(
    await workflow.processIncoming(
      await llmSwitch.processIncoming(request)
    )
  )
);
```

#### Qwen OAuth Provider

```javascript
import { OpenAINormalizerLLMSwitch } from './dist/modules/pipeline/modules/llmswitch/openai-normalizer.js';
import { QwenCompatibility } from './dist/modules/pipeline/modules/compatibility/qwen-compatibility.js';
import { QwenProvider } from './dist/modules/pipeline/modules/provider/qwen-provider.js';
import { StreamingControlWorkflow } from './dist/modules/pipeline/modules/workflow/streaming-control.js';
import { DebugCenter } from 'rcc-debugcenter';
import { ErrorHandlingCenter } from 'rcc-errorhandling';

// Initialize components
const errorHandlingCenter = new ErrorHandlingCenter();
const debugCenter = new DebugCenter();
await errorHandlingCenter.initialize();

// Create 4-layer pipeline with OAuth
const llmSwitch = new OpenAINormalizerLLMSwitch({
  type: 'llmswitch-openai-openai',
  config: { protocol: 'openai', targetFormat: 'qwen' }
}, { errorHandlingCenter, debugCenter, logger });

const workflow = new StreamingControlWorkflow({
  type: 'streaming-control',
  config: { enableStreaming: true }
}, { errorHandlingCenter, debugCenter, logger });

const compatibility = new QwenCompatibility({
  type: 'qwen-compatibility',
  config: { toolsEnabled: true }
}, { errorHandlingCenter, debugCenter, logger });

const provider = new QwenProvider({
  type: 'qwen-provider',
  config: {
    type: 'qwen',
    baseUrl: 'https://chat.qwen.ai',
    oauth: {
      clientId: 'f0304373b74a44d2b584a3fb70ca9e56',
      deviceCodeUrl: 'https://chat.qwen.ai/api/v1/oauth2/device/code',
      tokenUrl: 'https://chat.qwen.ai/api/v1/oauth2/token',
      scopes: ['openid', 'profile', 'email', 'model.completion'],
      tokenFile: './qwen-token.json'
    }
  }
}, { errorHandlingCenter, debugCenter, logger });

// Initialize all modules
await llmSwitch.initialize();
await workflow.initialize();
await compatibility.initialize();
await provider.initialize(); // This will trigger OAuth flow if not authenticated

// Process request - OAuth is handled automatically
const request = {
  model: 'qwen-turbo',
  messages: [{ role: 'user', content: 'Hello! How can you help me?' }]
};

const result = await provider.processIncoming(
  await compatibility.processIncoming(
    await workflow.processIncoming(
      await llmSwitch.processIncoming(request)
    )
  )
);
```

### Configuration

- Set config path via env var:
  - `ROUTECODEX_CONFIG_PATH=~/.routecodex/config/modelscope.json npm start`
- If not set, default path is `~/.routecodex/config.json`
- Single Source Of Truth (SSOT):
  - HTTP port must come from user config only
  - Compatibility must be set in user config (default `passthrough` is recommended first)
  - LLM Switch can be set to `openai-normalizer` to enforce strict OpenAI request shape
  - In `~/.routecodex/config.json`, set top-level `port` and optional host:
    ```json
    {
      "port": 5506,
      "server": { "host": "localhost" }
    }
    ```
  - If no port is provided, the server will fail to start with an explicit error.
- Merged config output: `~/routecodex/config/merged-config.json`

### Helper Notes

Port cleanup is now handled by the built‚Äëin CLI (routecodex start --restart) and server shutdown endpoint. No external helper scripts are required.

### üîß Simplified Logging System

RouteCodex now includes a simplified logging system that provides easy-to-use logging configuration without the complexity of the full debug system.

#### Quick Start
```bash
# Enable simplified logging
routecodex simple-log on --level debug --output console

# Check current status
routecodex simple-log status

# Change log level
routecodex simple-log level error

# Set output to file
routecodex simple-log output file

# Disable simplified logging
routecodex simple-log off
```

#### Features
- **One-click enable/disable**: Simple commands to control logging
- **Multiple log levels**: debug, info, warn, error
- **Flexible output**: console, file, or both
- **Persistent configuration**: Settings survive restarts
- **Automatic application**: Config automatically applied when server starts

#### Configuration File
Settings are stored in `~/.routecodex/simple-log-config.json` and automatically loaded on server startup. Default log output directory is `~/routecodex/debug-logs` when file output is enabled.

#### Benefits
- **Reduced complexity**: 788-line complex indexer ‚Üí 150-line simplified version
- **No memory overhead**: Removed history management and compression
- **Always-on capability**: Once enabled, stays active until explicitly disabled
- **Zero configuration**: Works out of the box with sensible defaults

### Pipeline Scope

- Pipeline uses LLM Switch + Compatibility + Provider.
- LLM Switch `openai-normalizer` performs standard OpenAI normalization.
- Compatibility handles vendor-specific differences; `passthrough` does no changes.

### Tool Calling Example

```javascript
const requestWithTools = {
  model: 'gpt-oss-20b-mlx',
  messages: [
    {
      role: 'system',
      content: 'You are a helpful assistant with access to tools.'
    },
    {
      role: 'user',
      content: 'What is 15 * 25?'
    }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'calculate',
        description: 'Perform mathematical calculations',
        parameters: {
          type: 'object',
          properties: {
            expression: {
              type: 'string',
              description: 'Mathematical expression to evaluate'
            }
          },
          required: ['expression']
        }
      }
    }
  ]
};

// Process through pipeline - tool calls will be automatically handled
const response = await processRequest(requestWithTools);

// Tool calls will be in response.choices[0].message.tool_calls
console.log('Tool calls:', response.choices[0].message.tool_calls);
```

## üìñ Documentation

- [Architecture Documentation](./ARCHITECTURE_DOCUMENTATION.md) - Detailed 4-layer architecture explanation
- [Configuration Guide](./docs/CONFIG_ARCHITECTURE.md) - Configuration options and examples
- [Pipeline Architecture](./docs/pipeline/ARCHITECTURE.md) - Pipeline system details
- [LM Studio Integration](./docs/lmstudio-tool-calling.md) - LM Studio specific setup
- [Dry-Run System](./docs/dry-run/README.md) - Comprehensive dry-run debugging framework
- [Bidirectional Pipeline](./docs/dry-run/bidirectional-pipeline.md) - Advanced bidirectional pipeline features

## üîß Configuration

### Basic Configuration

```json
{
  "server": {
    "port": 5506,
    "host": "localhost"
  },
  "providers": {
    "lmstudio": {
      "type": "lmstudio",
      "baseUrl": "http://localhost:1234",
      "apiKey": "your-api-key"
    }
  },
  "routing": {
    "default": "lmstudio"
  },
  "compatibility": "passthrough",
  "features": {
    "tools": {
      "enabled": true,
      "maxTools": 10
    },
    "streaming": {
      "enabled": true,
      "chunkSize": 1024
    }
  }
}
```

### Advanced Pipeline Configuration

For detailed pipeline configuration, you can specify individual modules:

```json
{
  "pipeline": {
    "llmSwitch": {
      "type": "llmswitch-openai-openai",
      "config": {
        "protocol": "openai",
        "targetFormat": "lmstudio"
      }
    },
    "workflow": {
      "type": "streaming-control",
      "config": {
        "enableStreaming": true,
        "bufferSize": 1024,
        "timeout": 30000
      }
    },
    "compatibility": {
      "type": "lmstudio-compatibility",
      "config": {
        "toolsEnabled": true,
        "customRules": [
          {
            "id": "ensure-standard-tools-format",
            "transform": "mapping",
            "sourcePath": "tools",
            "targetPath": "tools",
            "mapping": {
              "type": "type",
              "function": "function"
            }
          }
        ]
      }
    },
    "provider": {
      "type": "lmstudio-http",
      "config": {
        "type": "lmstudio",
        "baseUrl": "http://localhost:1234",
        "auth": {
          "type": "apikey",
          "apiKey": "your-api-key"
        },
        "timeout": 60000
      }
    }
  }
}
```

## üß™ Testing

```bash
# Run all tests
npm test

# Run specific test
npm test -- --testNamePattern="tool calling"

# Run integration test with LM Studio
node test-llmswitch-workflow-integration.mjs

# Run dry-run system tests
node test-all-nodes-dry-run.mjs

# Run bidirectional pipeline tests
node test-bidirectional-pipeline-dry-run.mjs

# Run build
npm run build
```

## üéØ Supported Providers

### ‚úÖ Currently Supported (Fully Operational)
- **LM Studio**: Local AI model hosting with full tool support and comprehensive dry-run analysis
- **Qwen**: Alibaba's language models with OAuth 2.0 authentication and API key fallback
- **iFlow**: AI service provider with OAuth 2.0 + PKCE authentication, Kimi model support
- **OpenAI**: GPT models with function calling
- **Anthropic**: Claude model family

### üîß Latest Updates (v0.2.7)
This release enables full operational capability for three major AI providers:

- **LM Studio**: Complete tool calling implementation with dry-run testing framework
- **Qwen**: Working OAuth authentication with automatic token refresh and caching
- **iFlow**: Fixed authentication endpoints and API configuration, Kimi model integration
- **Unified Auth Framework**: EnhancedAuthResolver supporting multiple authentication types
- **Build System**: Resolved TypeScript compilation issues
- **üîß Offline Logging CLI**: New comprehensive CLI for offline log capture and analysis
  - Module-level and pipeline-level offline logging configuration
  - Automatic log file management with rotation and compression
  - Built-in log analysis with HTML visualization reports
  - Time series analysis for performance monitoring
  - Zero-dependency offline operation

### üîê OAuth Authentication Details

#### Qwen OAuth Provider
- **Authentication**: OAuth 2.0 Device Flow
- **Token Management**: Automatic refresh with persistent storage
- **Security**: Standard OAuth 2.0 security model
- **Models**: qwen-turbo, qwen-max, qwen-turbo-latest

#### iFlow OAuth Provider
- **Authentication**: OAuth 2.0 Device Flow with PKCE
- **Token Management**: Automatic refresh with persistent storage
- **Security**: Enhanced PKCE (Proof Key for Code Exchange) protection
- **Models**: iflow-turbo, iflow-pro, iflow-turbo-latest

### üîÑ Planned Support
- Google Gemini
- Cohere
- Custom provider framework

## üìä Performance

- **Request Processing**: < 50ms overhead per layer
- **Tool Calling**: Native provider optimization
- **Streaming**: Real-time chunk processing
- **Memory Usage**: Efficient module loading and caching

## üß™ Ê†áÂáÜOpenAI APIÊµãËØïÊñπÊ≥ï

RouteCodexÊèê‰æõÂÆåÊï¥ÁöÑOpenAIÂÖºÂÆπAPIÊµãËØïÔºåÁ°Æ‰øù‰∏éÂêÑÁßçAIÂÆ¢Êà∑Á´ØÁöÑÂÆåÁæéÈõÜÊàê„ÄÇ

### Á´ØÂà∞Á´ØÊµãËØïÊµÅÁ®ã

#### 1. ÂêØÂä®ÊúçÂä°Âô®
```bash
# ‰ΩøÁî®ModelScopeÈÖçÁΩÆÂêØÂä®
ROUTECODEX_CONFIG_PATH=~/.routecodex/config/modelscope.json npm start

# ÊúçÂä°Âô®Â∞ÜÂú®ÈÖçÁΩÆÁöÑÁ´ØÂè£‰∏äÂêØÂä® (ÈªòËÆ§5506)
# ÂÅ•Â∫∑Ê£ÄÊü•: curl http://localhost:5506/health
```

#### 2. Ê†áÂáÜÊµãËØïÂëΩ‰ª§

**Âü∫Á°ÄÂØπËØùÊµãËØï**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "Hello, please respond briefly"
      }
    ]
  }'
```

**Â∑•ÂÖ∑Ë∞ÉÁî®ÊµãËØï**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "What is 15 * 25? Use the calculator tool."
      }
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate",
          "description": "Perform mathematical calculations",
          "parameters": {
            "type": "object",
            "properties": {
              "expression": {
                "type": "string",
                "description": "Mathematical expression to evaluate"
              }
            },
            "required": ["expression"]
          }
        }
      }
    ]
  }'
```

**ÊµÅÂºèÂìçÂ∫îÊµãËØï**:
```bash
curl -X POST http://localhost:5506/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $MS_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "messages": [
      {
        "role": "user",
        "content": "Count from 1 to 5 slowly"
      }
    ],
    "stream": true
  }'
```

#### 3. È™åËØÅÁ´ØÁÇπ

**ÊúçÂä°Âô®Áä∂ÊÄÅ**:
```bash
# Ê†πÁ´ØÁÇπ - ÊòæÁ§∫Á≥ªÁªü‰ø°ÊÅØ
curl http://localhost:5506/

# ÂÅ•Â∫∑Ê£ÄÊü•
curl http://localhost:5506/health

# ÈÖçÁΩÆ‰ø°ÊÅØ
curl http://localhost:5506/config
```

#### 4. ÂÆåÊï¥ÊµãËØïËÑöÊú¨

ÂàõÂª∫ÊµãËØïËÑöÊú¨ `test-modelscope.sh`:
```bash
#!/bin/bash

# ModelScope APIÊµãËØïËÑöÊú¨
set -e

BASE_URL="http://localhost:5506"
API_KEY="${MS_API_KEY:-YOUR_MODELSCOPE_API_KEY}"
MODEL="Qwen/Qwen3-Coder-480B-A35B-Instruct"

echo "üß™ RouteCodex ModelScope APIÊµãËØïÂºÄÂßã..."
echo "üì° ÊµãËØïÂú∞ÂùÄ: $BASE_URL"
echo "üîë ‰ΩøÁî®Ê®°Âûã: $MODEL"
echo ""

# 1. ÂÅ•Â∫∑Ê£ÄÊü•
echo "1Ô∏è‚É£ ÂÅ•Â∫∑Ê£ÄÊü•..."
curl -s "$BASE_URL/health" | jq .

# 2. Âü∫Á°ÄÂØπËØùÊµãËØï
echo ""
echo "2Ô∏è‚É£ Âü∫Á°ÄÂØπËØùÊµãËØï..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"Hello! Please respond briefly.\"
      }
    ]
  }" | jq .

# 3. ‰ª£Á†ÅÁîüÊàêÊµãËØï
echo ""
echo "3Ô∏è‚É£ ‰ª£Á†ÅÁîüÊàêÊµãËØï..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"Write a simple Python function to calculate factorial.\"
      }
    ]
  }" | jq .

# 4. Â∑•ÂÖ∑Ë∞ÉÁî®ÊµãËØï
echo ""
echo "4Ô∏è‚É£ Â∑•ÂÖ∑Ë∞ÉÁî®ÊµãËØï..."
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d "{
    \"model\": \"$MODEL\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"What is the square root of 144?\"
      }
    ],
    \"tools\": [
      {
        \"type\": \"function\",
        \"function\": {
          \"name\": \"calculate\",
          \"description\": \"Perform mathematical calculations\",
          \"parameters\": {
            \"type\": \"object\",
            \"properties\": {
              \"expression\": {
                \"type\": \"string\",
                \"description\": \"Mathematical expression to evaluate\"
              }
            },
            \"required\": [\"expression\"]
          }
        }
      }
    ]
  }" | jq .

echo ""
echo "‚úÖ ÊâÄÊúâÊµãËØïÂÆåÊàê!"
```

#### 5. ÊµãËØïÁªìÊûúÈ™åËØÅ

ÊàêÂäüÁöÑÊµãËØïÂìçÂ∫îÂ∫îËØ•ÂåÖÂê´:
- **Áä∂ÊÄÅÁ†Å**: 200 OK
- **ÂìçÂ∫îÁªìÊûÑ**: Ê†áÂáÜOpenAIÊ†ºÂºè
- **Â§ÑÁêÜÊó∂Èó¥**: ÈÄöÂ∏∏ < 2Áßí
- **Token‰ΩøÁî®**: ÂêàÁêÜÁöÑtokenËÆ°Êï∞
- **Ë∑ØÁî±‰ø°ÊÅØ**: ÂåÖÂê´ `_metadata` Â≠óÊÆµ

#### 6. ÈÖçÁΩÆÊñá‰ª∂Á§∫‰æã

`~/.routecodex/config/modelscope.json`:
```json
{
  "port": 5506,
  "virtualrouter": {
    "inputProtocol": "openai",
    "outputProtocol": "openai",
    "providers": {
      "modelscope": {
        "type": "openai",
        "baseURL": "https://api-inference.modelscope.cn/v1",
        "apiKey": [
          "YOUR_MODELSCOPE_KEY1",
          "YOUR_MODELSCOPE_KEY2",
          "YOUR_MODELSCOPE_KEY3",
          "YOUR_MODELSCOPE_KEY4"
        ],
        "models": {
          "Qwen/Qwen3-Coder-480B-A35B-Instruct": {
            "maxContext": 262144,
            "maxTokens": 262144
          }
        }
      }
    },
    "routing": {
      "default": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ],
      "coding": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ],
      "longcontext": [
        "modelscope.Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ]
    }
  }
}
```

#### 7. Â∏∏ËßÅÈóÆÈ¢òÊéíÊü•

**ÈóÆÈ¢ò1: 404 Not Found**
```bash
# Ê£ÄÊü•Á´ØÁÇπÊòØÂê¶Ê≠£Á°Æ
curl http://localhost:5506/
# Â∫îËØ•ÁúãÂà∞ÂèØÁî®Á´ØÁÇπÂàóË°®ÔºåOpenAIÁ´ØÁÇπÊòØ /v1/*
```

**ÈóÆÈ¢ò2: ËÆ§ËØÅÂ§±Ë¥•**
```bash
# Ê£ÄÊü•APIÂØÜÈí•Ê†ºÂºè
# ÂØÜÈí•Â∫îËØ•ÊòØ: ms-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

**ÈóÆÈ¢ò3: Ê®°ÂûãÈîôËØØ**
```bash
# Ê£ÄÊü•Ê®°ÂûãÂêçÁß∞ÊòØÂê¶Âú®ÈÖçÁΩÆ‰∏≠Â≠òÂú®
# ‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÂÆö‰πâÁöÑÁ°ÆÂàáÊ®°ÂûãÂêçÁß∞
```

#### 8. ÊÄßËÉΩÂü∫ÂáÜ

- **ÂìçÂ∫îÊó∂Èó¥**: 800-1500ms (ÂèñÂÜ≥‰∫éÁΩëÁªúÂíåÊ®°ÂûãÂ§çÊùÇÂ∫¶)
- **TokenÂ§ÑÁêÜ**: ÊîØÊåÅ262K‰∏ä‰∏ãÊñáÈïøÂ∫¶
- **Âπ∂ÂèëËÉΩÂäõ**: ÊîØÊåÅÂ§öËØ∑Ê±ÇÂπ∂ÂèëÂ§ÑÁêÜ
- **ÈîôËØØÂ§ÑÁêÜ**: ÂÆåÊï¥ÁöÑÈîôËØØÊÅ¢Â§çÊú∫Âà∂

ËøôÂ•óÊ†áÂáÜÊµãËØïÊñπÊ≥ïÁ°Æ‰øùRouteCodex‰∏é‰ªª‰ΩïOpenAIÂÖºÂÆπÂÆ¢Êà∑Á´ØÁöÑÂÆåÁæéÈõÜÊàê„ÄÇ

## üÜï Responses API Support ‚≠ê

RouteCodex now provides comprehensive support for OpenAI's **Responses API** through its 4-layer pipeline architecture, enabling seamless integration with the next-generation AI interface.

### üéØ Key Features
- **üîÑ Full Protocol Conversion**: Automatic conversion between Responses and Chat formats
- **üîß Complete Tool Support**: Full tool calling support in Responses API format
- **üì° Advanced Streaming**: Complete streaming event support with SSE processing
- **üèóÔ∏è Pipeline Integration**: Seamless integration with existing 4-layer architecture
- **üß™ Comprehensive Testing**: Extensive test fixtures and regression coverage

### üì° API Endpoints

**Responses API**: `http://localhost:5506/v1/responses` ‚≠ê
**Chat Completions**: `http://localhost:5506/v1/chat/completions`

### üèóÔ∏è Architecture Integration

The Responses API integrates seamlessly into RouteCodex's 4-layer pipeline:

```
Responses Request ‚Üí LLM Switch ‚Üí Workflow ‚Üí Compatibility ‚Üí Provider ‚Üí AI Service
       ‚Üì               ‚Üì          ‚Üì            ‚Üì           ‚Üì
   Protocol       Format     Flow       Format     Standard
   Detection      Conversion  Control    Conversion  HTTP Server
   (responses)   (‚Üí Chat)   (Events)   (‚Üí Chat)   (Chat Format)
```

**Key Components**:
- **LLM Switch**: `llmswitch-response-chat` module handles protocol conversion
- **Workflow**: `responses-streaming-workflow` manages streaming events
- **Compatibility**: Format adaptation for different providers
- **Provider**: Standard HTTP communication with AI services

### ‚öôÔ∏è Configuration

#### Basic Responses API Configuration

```json
{
  "virtualrouter": {
    "inputProtocol": "responses",
    "outputProtocol": "openai",
    "providers": {
      "lmstudio": {
        "type": "lmstudio",
        "baseURL": "http://localhost:1234",
        "apiKey": "your-api-key",
        "models": {
          "gpt-4": {
            "compatibility": { "type": "responses-chat-switch" }
          }
        }
      }
    },
    "routing": {
      "default": ["lmstudio.gpt-4"]
    }
  }
}
```

#### Advanced Pipeline Configuration

```json
{
  "pipeline": {
    "llmSwitch": {
      "type": "llmswitch-response-chat",
      "config": {
        "enableValidation": true,
        "enableMetadata": true,
        "preserveReasoning": true
      }
    },
    "workflow": {
      "type": "responses-streaming-workflow",
      "config": {
        "enableEventProcessing": true,
        "rebuildCompleteResponse": true
      }
    },
    "compatibility": {
      "type": "passthrough-compatibility",
      "config": {}
    },
    "provider": {
      "type": "lmstudio-http",
      "config": {
        "baseUrl": "http://localhost:1234",
        "auth": { "type": "apikey", "apiKey": "your-api-key" }
      }
    }
  }
}
```

### üöÄ Usage Examples

#### Basic Responses API Request

```bash
curl -X POST http://localhost:5506/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "gpt-4",
    "instructions": "You are a helpful assistant.",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "Hello! How can you help me today?"
          }
        ]
      }
    ],
    "stream": true
  }'
```

#### Responses API with Tool Calling

```bash
curl -X POST http://localhost:5506/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "gpt-4",
    "instructions": "You are a helpful assistant with access to tools.",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "What is 15 * 25? Use the calculator tool."
          }
        ]
      }
    ],
    "tools": [
      {
        "type": "function",
        "name": "calculate",
        "description": "Perform mathematical calculations",
        "parameters": {
          "type": "object",
          "properties": {
            "expression": {
              "type": "string",
              "description": "Mathematical expression to evaluate"
            }
          },
          "required": ["expression"]
        }
      }
    ],
    "stream": true
  }'
```

### üì° Streaming Events

The Responses API supports all standard streaming events with proper SSE formatting:

| Event Type | Description | Example |
|------------|-------------|---------|
| `response.created` | Response initialization | `{"type": "response.created", "response": {...}}` |
| `response.output_item.added` | New output item | `{"type": "response.output_item.added", "item": {...}}` |
| `response.content_part.added` | Content part added | `{"type": "response.content_part.added", "part": {...}}` |
| `response.output_text.delta` | Text delta | `{"type": "response.output_text.delta", "delta": "Hello"} ‚≠ê` |
| `response.tool_call.delta` | Tool call delta | `{"type": "response.tool_call.delta", "delta": {...}}` |
| `response.output_item.done` | Item completed | `{"type": "response.output_item.done", "item": {...}}` |
| `response.done` | Response complete | `{"type": "response.done", "response": {...}}` |

### üîÑ Protocol Conversion Details

#### Request Flow: Responses ‚Üí Chat

1. **Input Processing**: Convert `input[]` array to `messages[]` format
2. **Instructions Handling**: Convert `instructions` to system message
3. **Tool Format**: Convert Responses tool format to OpenAI format
4. **Metadata**: Preserve original protocol information

```typescript
// Responses Format Input
{
  "model": "gpt-4",
  "instructions": "You are helpful",
  "input": [
    {
      "type": "message",
      "role": "user",
      "content": [{"type": "input_text", "text": "Hello"}]
    }
  ]
}

// Converted to Chat Format
{
  "model": "gpt-4",
  "messages": [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Hello"}
  ]
}
```

#### Response Flow: Chat ‚Üí Responses

1. **Message Reconstruction**: Convert Chat messages back to Responses output format
2. **Tool Call Conversion**: Convert OpenAI tool_calls to Responses tool format
3. **Streaming Events**: Generate appropriate SSE events for streaming responses
4. **Metadata Handling**: Preserve conversion context and protocol information

### üß™ Testing and Validation

#### Test Fixtures

Comprehensive test fixtures are available in `tests/fixtures/`:

- `responses_1760615123370_request.json` - Complete sample request format
- `responses_output_text_events.json` - Streaming event samples
- `responses_tool_call_events.json` - Tool calling event samples

#### Integration Testing

```bash
# Test Responses API with streaming
curl -X POST http://localhost:5506/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer test-key" \
  -d @tests/fixtures/responses_1760615123370_request.json

# Verify streaming events
# Should see proper SSE formatting with event types and data
```

### üîß Client Configuration

#### Codex CLI Integration

```toml
[model_providers.rc]
name = "routecodex"
base_url = "http://localhost:5506/v1"
wire_api = "responses"
api_key = "your-api-key"
```

#### OpenAI SDK Integration

```typescript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'http://localhost:5506/v1',
  apiKey: 'your-api-key'
});

// Use Responses API
const response = await client.responses.create({
  model: 'gpt-4',
  instructions: 'You are a helpful assistant.',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Hello!' }]
    }
  ]
});
```

### üõ°Ô∏è Error Handling

The Responses API includes comprehensive error handling:

- **Format Validation**: Validate Requests API request format
- **Protocol Errors**: Handle conversion errors gracefully
- **Streaming Errors**: Proper error event streaming
- **Tool Call Errors**: Detailed tool calling error information

### üìà Performance Features

- **Event Batching**: Efficient processing of streaming events
- **Memory Management**: Intelligent buffering for large responses
- **Async Processing**: Non-blocking event handling
- **Error Recovery**: Automatic retry and fallback mechanisms

### üö® Limitations and Considerations

**Current Limitations**:
- Some advanced Responses features may still be in development
- Certain edge cases in tool calling format conversion
- Complex multimodal content handling

**Best Practices**:
- Use standard Responses API format for best compatibility
- Test with provided fixtures before production deployment
- Monitor streaming event processing for performance optimization

### üîÑ Migration from Chat Completions

#### Easy Migration Path

Existing Chat Completions users can easily migrate:

1. **Update Endpoint**: Change from `/v1/chat/completions` to `/v1/responses`
2. **Format Conversion**: Convert `messages[]` to `input[]` array format
3. **Instructions**: Move system messages to `instructions` field
4. **Tools**: Update tool format to Responses specification

#### Migration Example

```typescript
// Before (Chat Completions)
const chatRequest = {
  model: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are helpful' },
    { role: 'user', content: 'Hello!' }
  ],
  tools: [...] // OpenAI tool format
};

// After (Responses API)
const responsesRequest = {
  model: 'gpt-4',
  instructions: 'You are helpful',
  input: [
    {
      type: 'message',
      role: 'user',
      content: [{ type: 'input_text', text: 'Hello!' }]
    }
  ],
  tools: [...] // Responses tool format
};
```

### üìö Documentation and Resources

- **Module Documentation**: See `src/modules/pipeline/modules/llmswitch/README.md`
- **Architecture Details**: See `docs/llmswitch-responses-plan.md`
- **Test Examples**: See `tests/fixtures/` directory
- **Configuration Guide**: See individual module READMEs

---

**Responses API support represents a major enhancement to RouteCodex**, providing next-generation AI interface capabilities while maintaining full backward compatibility with existing Chat Completions workflows.

## ‚úÖ ModelScope Áã¨Á´ãÈ™åËØÅÈÄöËøá

- Êó•Êúü: 2025-09-26
- ÈÖçÁΩÆ: `~/.routecodex/config/modelscope.json`
- Á´ØÂè£: 5507ÔºàÁ§∫‰æãÁéØÂ¢ÉÔºâ
- È™åËØÅÈ°π:
  - ÈùûÊµÅÂºèËøîÂõû OpenAI Ê†áÂáÜÁªìÊûÑÔºàid/object/created/model/choices/usageÔºâ
  - ÊµÅÂºèÔºàSSEÔºâÂàÜÁâáËßÑËåÉÔºàobject: chat.completion.chunkÔºådelta.contentÔºåÊúâ [DONE]Ôºâ
  - ‰∏•Ê†º JSON Ê®°ÂºèÔºö`response_format: { "type": "json_object" }` ÂÜÖÂÆπÊ∏ÖÊ¥ó‰∏∫Á∫Ø JSON Â≠óÁ¨¶‰∏≤
  - 429 Ë∞ÉÂ∫¶ÔºöÁî± PipelineManager Áªü‰∏ÄÂ§ÑÁêÜÔºåÂ§ö Key/Pipeline ËΩÆËØ¢ÈáçËØïÔºåÂÖ®ÈÉ®ÊûØÁ´≠ÊâçËøîÂõû 429
  - Áªü‰∏ÄÂìçÂ∫îÂ§¥Ôºö`x-request-id`„ÄÅ`Cache-Control: no-store`„ÄÅ`Content-Type: application/json; charset=utf-8`

ËØ¶ÊÉÖËßÅ docs/verification/modelscope-verify.md„ÄÇ

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **LM Studio**: For providing excellent local AI model hosting
- **OpenAI**: For the standard API specification
- **TypeScript**: For enabling type-safe development
- **Node.js**: For the robust runtime environment

## üìû Support

For support, please:
1. Check the [documentation](./docs/)
2. Search existing [issues](https://github.com/your-username/routecodex/issues)
3. Create a new issue with detailed information

---

**Built with ‚ù§Ô∏è using the 4-layer architecture pattern**
