# RouteCodex 4-Layer Pipeline Architecture Documentation

## Overview

The RouteCodex system implements a sophisticated 4-layer pipeline architecture that provides clean separation of concerns, modular design, and flexible protocol handling. This architecture enables seamless integration with multiple AI providers while maintaining consistent interfaces and proper workflow management.

## Architecture Diagram

```
HTTP Request â†’ LLM Switch â†’ Compatibility â†’ Provider â†’ AI Service
     â†“             â†“             â†“            â†“           â†“
  Request      Protocol      Format       Standard     Response
  Analysis     Routing     Conversion     HTTP Server  Processing
```

## Layer 1: LLM Switch (Dynamic Routing Classification)

### Core Functionality
- **Request Analysis**: Analyzes incoming requests to determine optimal routing
- **Protocol Routing**: Routes requests to appropriate processing pipelines
- **Dynamic Classification**: Supports 7 routing categories:
  - `default`: Standard request routing
  - `longcontext`: Long text processing requests
  - `thinking`: Complex reasoning requests
  - `background`: Background processing requests
  - `websearch`: Web search requests
  - `vision`: Image processing requests
  - `coding`: Code generation requests

### Key Responsibilities
1. **Request Validation**: Validates incoming request format and parameters
2. **Protocol Detection**: Determines source and target protocols
3. **Route Selection**: Selects appropriate processing pipeline based on request characteristics
4. **Metadata Enrichment**: Adds routing and processing metadata

### Implementation Example
```typescript
export class OpenAIPassthroughLLMSwitch implements LLM SwitchModule {
  async processIncoming(request: any): Promise<any> {
    // Analyze request and determine routing
    const routing = this.analyzeRequest(request);

    // Add routing metadata
    return {
      ...request,
      _metadata: {
        switchType: 'openai-passthrough',
        timestamp: Date.now(),
        originalProtocol: 'openai',
        targetProtocol: 'openai',
        routingCategory: routing.category
      }
    };
  }
}
```

## Layer 2: Compatibility (Format Transformation)

### Core Functionality
- **Protocol Translation**: Converts between different AI service protocols
- **Format Adaptation**: Transforms request/response formats between providers
- **Tool Integration**: Handles tool calling format conversion and execution
- **Configuration-Driven**: Uses JSON configuration for transformation rules

### Key Responsibilities
1. **Request Transformation**: Converts requests to target provider format
2. **Response Processing**: Transforms provider responses back to expected format
3. **Tool Format Conversion**: Handles tool calling format differences
4. **Error Handling**: Manages transformation errors and fallbacks

### Transformation Engine
```typescript
// Example transformation rules
const transformationRules = [
  {
    id: 'openai-to-lmstudio-tools',
    transform: 'mapping',
    sourcePath: 'tools',
    targetPath: 'tools',
    mapping: {
      'type': 'type',
      'function': 'function'
    }
  }
];
```

### Implementation Example
```typescript
export class LMStudioCompatibility implements CompatibilityModule {
  async processIncoming(request: any): Promise<any> {
    // Apply transformation rules
    const transformed = await this.transformationEngine.transform(
      request,
      this.config.transformationRules
    );

    return transformed.data || transformed;
  }
}
```

## Layer 3: Provider (Standard HTTP Server)

### Core Functionality
- **HTTP Communication**: Manages all HTTP communications with AI services
- **Authentication**: Handles provider authentication and authorization
- **Error Handling**: Manages network errors and provider-specific issues
- **Health Monitoring**: Monitors provider health and connectivity

### Key Responsibilities
1. **Request Execution**: Sends HTTP requests to AI providers
2. **Response Handling**: Processes HTTP responses from providers
3. **Authentication Management**: Handles API keys, tokens, and auth contexts
4. **Connection Management**: Manages HTTP connections and timeouts

### Architecture Principle
**CRITICAL**: Provider modules do NOT perform any format transformations. They are standard HTTP servers that only send and receive raw HTTP requests/responses. All transformations are handled by the Compatibility layer.

### Implementation Example
```typescript
export class LMStudioProviderSimple implements ProviderModule {
  async processIncoming(request: any): Promise<any> {
    // Compatibilityæ¨¡å—å·²ç»å¤„ç†äº†æ‰€æœ‰è½¬æ¢ï¼Œç›´æ¥å‘é€è¯·æ±‚
    const response = await this.sendChatRequest(request);
    return response;
  }

  private async sendChatRequest(request: any): Promise<ProviderResponse> {
    // Standard HTTP request to AI provider
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: this.headers,
      body: JSON.stringify(request)
    });

    return {
      data: await response.json(),
      status: response.status,
      headers: Object.fromEntries(response.headers.entries()),
      metadata: { /* processing metadata */ }
    };
  }
}
```

## Layer 4: AI Service (External Provider)

### Core Functionality
- **Model Processing**: Executes AI models and generates responses
- **Tool Execution**: Handles tool calling and function execution
- **Response Generation**: Produces AI-generated content and tool calls

### Supported Providers
- **LM Studio**: Local AI model hosting with tool support
- **OpenAI**: GPT models with function calling
- **Qwen**: Alibaba's language models
- **Anthropic**: Claude model family
- **Custom Providers**: Extensible architecture for additional providers

## Data Flow Example

### Request Flow
```
1. User Request: {
  "model": "qwen3-4b-thinking-2507-mlx",
  "messages": [...],
  "tools": [...]
}

2. LLM Switch Output: {
  "model": "qwen3-4b-thinking-2507-mlx",
  "messages": [...],
  "tools": [...],
  "_metadata": {
    "switchType": "openai-passthrough",
    "timestamp": 1758554010322,
    "originalProtocol": "openai",
    "targetProtocol": "openai"
  }
}

3. Compatibility Output: {
  "model": "qwen3-4b-thinking-2507-mlx",
  "messages": [...],
  "tools": [...],
  "_metadata": { ... }
}

4. Provider HTTP Request: {
  "model": "qwen3-4b-thinking-2507-mlx",
  "messages": [...],
  "tools": [...]
}
```

### Response Flow
```
1. AI Service Response: {
  "id": "chat-xxx",
  "object": "chat.completion",
  "choices": [{
    "finish_reason": "tool_calls",
    "message": {
      "content": "\n\n",
      "tool_calls": [...]
    }
  }]
}

2. Provider Response: {
  "data": { /* AI service response */ },
  "status": 200,
  "headers": { ... },
  "metadata": { ... }
}

3. Compatibility Processing: {
  "id": "chat-xxx",
  "object": "chat.completion",
  "choices": [...],
  "_transformed": true
}

4. Final User Response: {
  "id": "chat-xxx",
  "object": "chat.completion",
  "choices": [...],
  "usage": { ... }
}
```

## Configuration Structure

### ğŸ”§ **é‡è¦ï¼šç”¨æˆ·é…ç½® vs ç³»ç»Ÿé…ç½®åŒºåˆ†**

RouteCodexç³»ç»Ÿä¸¥æ ¼åŒºåˆ†**ç”¨æˆ·åŸºç¡€é…ç½®**å’Œ**ç³»ç»Ÿæ‰©å±•é…ç½®**ï¼Œç¡®ä¿ä¸¤è€…ä¸é‡åˆï¼Œé¿å…é…ç½®å†²çªï¼š

#### **ç”¨æˆ·åŸºç¡€é…ç½®** (User Basic Configuration)
- **ä½œç”¨åŸŸ**: ç”¨æˆ·ä¸ªäººè®¾ç½®ï¼Œä»…å½±å“æ—¥å¿—è¡Œä¸º
- **æ–‡ä»¶ä½ç½®**: `~/.routecodex/simple-log-config.json`
- **é…ç½®å†…å®¹**: ä»…åŒ…å«ç®€åŒ–æ—¥å¿—ç›¸å…³è®¾ç½®
- **ä¼˜å…ˆçº§**: åŸºç¡€çº§åˆ«ï¼Œä¸ä¸å…¶ä»–ç³»ç»Ÿé…ç½®é‡å 

#### **ç³»ç»Ÿæ‰©å±•é…ç½®** (System Extended Configuration) 
- **ä½œç”¨åŸŸ**: ç³»ç»Ÿçº§åŠŸèƒ½ï¼Œå½±å“æ•´ä½“æ¶æ„è¡Œä¸º
- **æ–‡ä»¶ä½ç½®**: é¡¹ç›®ç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶
- **é…ç½®å†…å®¹**: ç®¡é“ã€æ¨¡å—ã€éƒ¨ç½²ç­‰ç³»ç»Ÿçº§è®¾ç½®
- **ä¼˜å…ˆçº§**: é«˜çº§åˆ«ï¼Œæ‰©å±•ç”¨æˆ·åŸºç¡€åŠŸèƒ½

### **é…ç½®ä¸é‡åˆåŸåˆ™**
```
ç”¨æˆ·åŸºç¡€é…ç½® â† ç‹¬ç«‹è¿è¡Œ â†’ ç³»ç»Ÿæ‰©å±•é…ç½®
     â†“                        â†“
ç®€åŒ–æ—¥å¿—ç³»ç»Ÿ              4å±‚ç®¡é“æ¶æ„
(ä¸ªäººè®¾ç½®)                (ç³»ç»Ÿæ¶æ„)
```

---

### ç”¨æˆ·åŸºç¡€é…ç½® (ç®€åŒ–æ—¥å¿—ç³»ç»Ÿ)
```json
{
  "enabled": true,
  "logLevel": "debug",
  "output": "console",
  "logDirectory": "/Users/fanzhang/.routecodex/logs",
  "autoStart": true
}
```
**æ³¨æ„**: æ­¤é…ç½®**å®Œå…¨ç‹¬ç«‹**äºä¸‹é¢çš„ç³»ç»Ÿæ¶æ„é…ç½®ï¼Œä»…æ§åˆ¶ç®€åŒ–æ—¥å¿—åŠŸèƒ½ã€‚

---

### ç³»ç»Ÿæ‰©å±•é…ç½® (4å±‚ç®¡é“æ¶æ„)
```json
{
  "pipeline": {
    "llmSwitch": {
      "type": "openai-passthrough",
      "config": {
        "protocol": "openai",
        "targetFormat": "lmstudio"
      }
    },
    "compatibility": {
      "type": "lmstudio-compatibility",
      "config": {
        "toolsEnabled": true,
        "customRules": [...]
      }
    },
    "provider": {
      "type": "lmstudio-http",
      "config": {
        "type": "lmstudio",
        "baseUrl": "http://localhost:1234",
        "auth": {
          "type": "apikey",
          "apiKey": "your-api-key"
        }
      }
    }
  }
}
```
**æ³¨æ„**: æ­¤é…ç½®**å®Œå…¨ä¸æ¶‰åŠ**ç®€åŒ–æ—¥å¿—è®¾ç½®ï¼Œä»…æ§åˆ¶ç³»ç»Ÿæ¶æ„åŠŸèƒ½ã€‚

### **é…ç½®äº¤äº’è§„åˆ™**
1. **ç‹¬ç«‹æ€§**: ç”¨æˆ·é…ç½®ä¿®æ”¹ä¸å½±å“ç³»ç»Ÿé…ç½®
2. **æ— é‡å **: ä¸¤å¥—é…ç½®æ§åˆ¶å®Œå…¨ä¸åŒçš„åŠŸèƒ½åŸŸ
3. **äº’è¡¥æ€§**: ç®€åŒ–æ—¥å¿— + 4å±‚ç®¡é“ = å®Œæ•´åŠŸèƒ½
4. **ä¼˜å…ˆçº§**: ç³»ç»Ÿé…ç½®è¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹ç”¨æˆ·é…ç½®çŠ¶æ€

## Key Design Principles

### 1. Separation of Concerns
- **LLM Switch**: Routing and classification
- **Compatibility**: Format transformation
- **Provider**: HTTP communication
- **AI Service**: Model processing

### 2. Configuration-Driven
- JSON configuration for all transformations
- Dynamic rule application
- Hot reload capabilities

### 3. Modular Design
- Each layer can be independently replaced
- Plugin architecture for extensibility
- Interface-based contracts

### 4. Error Handling
- Comprehensive error handling at each layer
- Graceful degradation
- Detailed error reporting

### 5. Performance Optimization
- Minimal overhead between layers
- Efficient transformation algorithms
- Connection pooling and caching

## Benefits

1. **Flexibility**: Easy to add new providers and protocols
2. **Maintainability**: Clear separation of responsibilities
3. **Testability**: Each layer can be tested independently
4. **Extensibility**: Plugin architecture for custom functionality
5. **Performance**: Optimized for high-throughput scenarios
6. **Reliability**: Robust error handling and recovery

## Best Practices

1. **Always use Compatibility layer** for transformations
2. **Keep Provider layer simple** - HTTP communication only
3. **Configure proper routing** in LLM Switch for optimal performance
4. **Implement comprehensive logging** for debugging
5. **Use appropriate timeouts** and retry mechanisms
6. **Validate all configurations** before deployment
7. **Monitor system health** and performance metrics

## Testing Strategy

### Unit Tests
- Test each layer independently
- Mock external dependencies
- Verify transformation rules
- Validate error handling

### Integration Tests
- Test complete request/response flow
- Verify provider integration
- Test tool calling functionality
- Performance benchmarking

### End-to-End Tests
- Real AI model testing
- Tool execution validation
- Error scenario testing
- Load testing

## ğŸ”§ Simplified Logging System

RouteCodex includes a simplified logging system designed for users who need basic logging functionality without the complexity of the full debug system.

### Architecture Overview

The simplified logging system reduces complexity from 788 lines to 150 lines while maintaining essential functionality:

```
Original System (788 lines) â†’ Simplified System (150 lines)
â”œâ”€â”€ Time Series Indexing        â†’ Basic log storage
â”œâ”€â”€ Real-time Compression       â†’ Removed
â”œâ”€â”€ Complex Query Engine        â†’ Removed  
â”œâ”€â”€ Memory History Management   â†’ Removed
â””â”€â”€ Advanced Analytics          â†’ Basic filtering
```

### Key Components

#### 1. SimpleLogConfigManager
- **Location**: `src/logging/simple-log-integration.ts`
- **Purpose**: Manages configuration loading and monitoring
- **Features**: 
  - File-based configuration storage
  - Automatic configuration reloading
  - Environment variable integration

#### 2. SimpleTimeSeriesIndexer
- **Location**: `src/logging/indexer/SimpleTimeSeriesIndexer.ts`
- **Purpose**: Basic log storage without complex indexing
- **Features**:
  - Simple file-based storage
  - No compression or sharding
  - Basic time-based organization

#### 3. Simple Log CLI
- **Location**: `src/commands/simple-log.ts`
- **Purpose**: User-friendly CLI for log configuration
- **Commands**:
  ```bash
  routecodex simple-log on [--level debug] [--output console]
  routecodex simple-log off
  routecodex simple-log status
  routecodex simple-log level <level>
  routecodex simple-log output <output>
  ```

### Configuration Integration

The simplified logging system integrates seamlessly with the existing RouteCodex architecture:

1. **CLI Detection**: `src/cli.ts` detects simple log configuration
2. **Server Integration**: `src/server/http-server.ts` applies configuration during startup
3. **Environment Variables**: Configuration applied via `SIMPLE_LOG_*` environment variables
4. **Persistent Storage**: Settings stored in `~/.routecodex/simple-log-config.json`

### Usage Flow

```bash
# User enables simplified logging
routecodex simple-log on --level debug --output console

# Configuration saved to ~/.routecodex/simple-log-config.json
{
  "enabled": true,
  "logLevel": "debug",
  "output": "console",
  "autoStart": true
}

# Server startup detects and applies configuration
routecodex start
# Output: "æ£€æµ‹åˆ°ç®€å•æ—¥å¿—é…ç½®ï¼Œæ­£åœ¨åº”ç”¨..."
# Output: "âœ¨ ç®€å•æ—¥å¿—é…ç½®å·²åº”ç”¨åˆ°ç³»ç»Ÿï¼"
```

### Benefits

1. **Simplicity**: One-click configuration with sensible defaults
2. **Persistence**: Configuration survives system restarts
3. **Flexibility**: Support for multiple log levels and output modes
4. **Performance**: Reduced memory footprint and faster startup
5. **Compatibility**: Works alongside existing debug systems

### Implementation Details

#### Configuration Schema
```typescript
interface SimpleLogConfig {
  enabled: boolean;
  logLevel: 'debug' | 'info' | 'warn' | 'error';
  output: 'console' | 'file' | 'both';
  logDirectory?: string;
  autoStart: boolean;
}
```

#### Integration Points
- **Startup**: Configuration loaded in `src/index.ts`
- **Module Loading**: Applied during module initialization
- **Runtime**: Configuration changes monitored and applied dynamically

#### Log Level Filtering
```typescript
// Simplified logger respects log level settings
const logger = createLoggerWithSimpleConfig(moduleId, moduleType);

// Only logs at or above configured level are output
if (levelPriority[level] >= levelPriority[config.logLevel]) {
  console.log(`[${level}] [${moduleId}] ${message}`);
}
```

This architecture provides a solid foundation for building scalable, maintainable AI service integrations with proper separation of concerns and flexible configuration options.