{
  "metadata": {
    "generatedAt": "2025-11-02T04:35:35.818Z",
    "analysisType": "Static Code Analysis",
    "projectPath": "/Users/fanzhang/Documents/github/routecodex-worktree/dev"
  },
  "summary": {
    "v1": {
      "totalFiles": 37,
      "totalLines": 11679,
      "estimatedSizeKB": 406
    },
    "v2": {
      "totalFiles": 8,
      "totalLines": 2309,
      "estimatedSizeKB": 63
    },
    "improvements": {
      "fileChange": -29,
      "lineChange": -9370,
      "sizeChange": -350956
    }
  },
  "detailedAnalysis": {
    "v1": {
      "path": "./src/server",
      "totalFiles": 37,
      "totalLines": 11679,
      "totalSize": 415486,
      "modules": {
        "src/server/RouteCodexServer.ts": {
          "path": "src/server/RouteCodexServer.ts",
          "size": 21616,
          "lines": 769,
          "imports": [
            "express",
            "cors",
            "helmet",
            "rcc-basemodule",
            "rcc-errorhandling",
            "rcc-debugcenter",
            "path",
            "fs",
            "os",
            "../types/common-types.js"
          ],
          "exports": [
            "ServerConfig",
            "RouteCodexServer"
          ],
          "classes": [
            "RouteCodexServer"
          ],
          "functions": [],
          "content": "/**\n * RouteCodex Server - Main server implementation\n * Multi-provider OpenAI proxy server\n */\n\nimport express, { type Application, type Request, type Response, type NextFunction } from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { BaseModule, type ModuleInfo } from 'rcc-basemodule';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport path from 'path';\nimport fs from 'fs';\nimport { homedir } from 'os';\nimport type { UnknownObject } from '../types/common-types.js';\n\ninterface ErrorContext {\n  error: Error | string;\n  source: string;\n  severity: 'low' | 'medium' | 'high' | 'critical';\n  timestamp: number;\n  moduleId?: string;\n  context?: UnknownObject;\n}\n\n/**\n * Server configuration interface\n */\nexport interface ServerConfig {\n  server: {\n    port: number;\n    host: string;\n    cors?: {\n      origin: string | string[];\n      credentials?: boolean;\n    };\n    timeout?: number;\n    bodyLimit?: string;\n  };\n  logging: {\n    level: 'debug' | 'info' | 'warn' | 'error';\n    enableConsole?: boolean;\n    enableFile?: boolean;\n    filePath?: string;\n    categories?: string[];\n    categoryPath?: string;\n  };\n  providers: Record<string, UnknownObject>;\n}\n\n/**\n * RouteCodex Server class\n */\nexport class RouteCodexServer extends BaseModule {\n  private app: Application;\n  private server?: unknown;\n  private config: ServerConfig;\n  private errorHandling: ErrorHandlingCenter;\n  private debugEventBus: DebugEventBus;\n  private logFileStream?: fs.WriteStream;\n  private categoryLogStreams: Map<string, fs.WriteStream> = new Map();\n  private _isInitialized: boolean = false;\n  private _isRunning: boolean = false;\n\n  // Debug enhancement properties\n  private serverMetrics: Map<string, UnknownObject> = new Map();\n  private requestHistory: UnknownObject[] = [];\n  private errorHistory: UnknownObject[] = [];\n  private maxHistorySize = 50;\n\n  constructor(config: ServerConfig) {\n    const moduleInfo: ModuleInfo = {\n      id: 'routecodex-server',\n      name: 'RouteCodexServer',\n      version: '0.50.1',\n      description: 'Multi-provider OpenAI proxy server',\n      type: 'server',\n    };\n\n    super(moduleInfo);\n\n    this.config = config;\n    this.app = express();\n    this.errorHandling = new ErrorHandlingCenter();\n    this.debugEventBus = DebugEventBus.getInstance();\n\n    // Initialize debug enhancements\n    this.initializeDebugEnhancements();\n  }\n\n  /**\n   * Initialize the server\n   */\n  public async initialize(): Promise<void> {\n    try {\n      // Setup logging directory\n      this.setupLoggingDirectory();\n\n      // Load module configurations\n      this.loadModuleConfigurations();\n\n      // Initialize error handling\n      await this.errorHandling.initialize();\n\n      // Setup debug event listener for console logging\n      this.setupDebugLogging();\n\n      // Setup Express middleware\n      this.setupMiddleware();\n\n      // Setup routes\n      this.setupRoutes();\n\n      // Setup error handling\n      this.setupErrorHandling();\n\n      this._isInitialized = true;\n\n      // Log initialization\n      this.logEvent('server', 'initialized', {\n        port: this.config.server.port,\n        host: this.config.server.host,\n      });\n    } catch (error) {\n      await this.handleError(error as Error, 'initialization');\n      throw error;\n    }\n  }\n\n  /**\n   * Start the server\n   */\n  public async start(): Promise<void> {\n    if (!this._isInitialized) {\n      await this.initialize();\n    }\n\n    return new Promise((resolve, reject) => {\n      this.server = this.app.listen(this.config.server.port, this.config.server.host, () => {\n        this._isRunning = true;\n        this.logEvent('server', 'started', {\n          port: this.config.server.port,\n          host: this.config.server.host,\n        });\n        resolve();\n      });\n\n      (this.server as any).on('error', async (error: Error) => {\n        await this.handleError(error, 'server_start');\n        reject(error);\n      });\n    });\n  }\n\n  /**\n   * Stop the server\n   */\n  public async stop(): Promise<void> {\n    if (this.server) {\n      return new Promise(resolve => {\n        (this.server as any)?.close(async () => {\n          this._isRunning = false;\n          this.logEvent('server', 'stopped', {});\n\n          // Cleanup modules\n          await this.errorHandling.destroy();\n\n          // Close log file stream\n          if (this.logFileStream) {\n            this.logFileStream.end();\n            console.log('Log file stream closed');\n          }\n\n          // Close category log file streams\n          this.categoryLogStreams.forEach((stream, category) => {\n            stream.end();\n            console.log(`Category log file stream closed for: ${category}`);\n          });\n          this.categoryLogStreams.clear();\n\n          resolve();\n        });\n      });\n    }\n  }\n\n  /**\n   * Get server status\n   */\n  public getStatus() {\n    return {\n      initialized: this._isInitialized,\n      running: this._isRunning,\n      port: this.config.server.port,\n      host: this.config.server.host,\n      uptime: process.uptime(),\n      memory: process.memoryUsage(),\n    };\n  }\n\n  /**\n   * Override BaseModule methods\n   */\n  public isInitialized(): boolean {\n    return this._isInitialized;\n  }\n\n  public isRunning(): boolean {\n    return this._isRunning;\n  }\n\n  /**\n   * Setup logging directory and file stream\n   */\n  private setupLoggingDirectory(): void {\n    if (this.config.logging.enableFile && this.config.logging.filePath) {\n      // Resolve ~ to home directory\n      let logPath = this.config.logging.filePath;\n      if (logPath.startsWith('~')) {\n        logPath = path.join(homedir(), logPath.substring(1));\n      }\n\n      // Create main log directory if it doesn't exist\n      const logDir = path.dirname(logPath);\n      if (!fs.existsSync(logDir)) {\n        fs.mkdirSync(logDir, { recursive: true });\n        console.log(`Created logging directory: ${logDir}`);\n      }\n\n      // Create category subdirectories\n      const categories = this.config.logging.categories || [\n        'server',\n        'api',\n        'request',\n        'config',\n        'error',\n        'message',\n      ];\n      const categoryPath = this.config.logging.categoryPath || logDir;\n\n      categories.forEach(category => {\n        const categoryDir = path.join(categoryPath, category);\n        if (!fs.existsSync(categoryDir)) {\n          fs.mkdirSync(categoryDir, { recursive: true });\n          console.log(`Created category logging directory: ${categoryDir}`);\n        }\n\n        // Create category-specific log file stream\n        const categoryLogPath = path.join(categoryDir, `${category}.log`);\n        const categoryStream = fs.createWriteStream(categoryLogPath, { flags: 'a' });\n        categoryStream.on('error', error => {\n          console.error(`Category log file stream error for ${category}:`, error);\n        });\n        this.categoryLogStreams.set(category, categoryStream);\n      });\n\n      // Create main log file write stream\n      this.logFileStream = fs.createWriteStream(logPath, { flags: 'a' });\n      this.logFileStream.on('error', error => {\n        console.error('Main log file stream error:', error);\n      });\n\n      console.log(`Logging configured for file: ${logPath}`);\n      console.log(`Category logging enabled for: ${categories.join(', ')}`);\n      console.log(`Category log directory: ${categoryPath}`);\n    }\n  }\n\n  /**\n   * Load module configurations\n   */\n  private loadModuleConfigurations(): void {\n    // Use import.meta.url to get the current file's directory\n    const currentDir = path.dirname(new URL(import.meta.url).pathname);\n    const moduleConfigPath = path.join(currentDir, '..', '..', 'config', 'modules.json');\n\n    if (fs.existsSync(moduleConfigPath)) {\n      try {\n        const moduleConfig = JSON.parse(fs.readFileSync(moduleConfigPath, 'utf8'));\n        console.log('Module configurations loaded successfully');\n\n        // Log each module's configuration status\n        if (moduleConfig.modules) {\n          Object.entries(moduleConfig.modules).forEach(([moduleName, config]: [string, unknown]) => {\n            const configObj = config as UnknownObject;\n            console.log(`[CONFIG] ${moduleName}: ${configObj.enabled ? 'enabled' : 'disabled'}`);\n            if (configObj.enabled && configObj.config) {\n              console.log(`[CONFIG] ${moduleName} settings:`, Object.keys(configObj.config));\n            }\n          });\n        }\n\n        this.logEvent('config', 'modules_loaded', {\n          modules: Object.keys(moduleConfig.modules || {}),\n          configPath: moduleConfigPath,\n        });\n      } catch (error) {\n        console.error('Failed to load module configurations:', error);\n        this.logEvent('config', 'modules_load_failed', {\n          error: error instanceof Error ? error.message : String(error),\n          configPath: moduleConfigPath,\n        });\n      }\n    } else {\n      console.log('Module configuration file not found, using defaults');\n      this.logEvent('config', 'modules_not_found', {\n        configPath: moduleConfigPath,\n      });\n    }\n  }\n\n  /**\n   * Write to category-specific log file\n   */\n  private writeToCategoryLog(category: string, message: string): void {\n    const categoryStream = this.categoryLogStreams.get(category);\n    if (categoryStream) {\n      categoryStream.write(message);\n    }\n  }\n\n  /**\n   * Setup debug logging\n   */\n  private setupDebugLogging(): void {\n    // Subscribe to all debug events\n    this.debugEventBus.subscribe('*', (event: UnknownObject) => {\n      if (this.config.logging.level === 'debug') {\n        const timestamp = new Date(event.timestamp as any).toISOString();\n        const eventData = event.data as Record<string, unknown>;\n        const category = (eventData?.category as string) || 'general';\n        const action = (eventData?.action as string) || 'unknown';\n\n        const logMessage = `[${timestamp}] [DEBUG] ${event.operationId}: ${JSON.stringify(\n          {\n            category,\n            action,\n            moduleId: event.moduleId,\n            data: event.data,\n          },\n          null,\n          2\n        )}\\n`;\n\n        // Log to console\n        console.log(`[DEBUG] ${event.operationId}:`, {\n          category,\n          action,\n          timestamp,\n          moduleId: event.moduleId,\n          data: event.data,\n        });\n\n        // Log to main file\n        if (this.logFileStream) {\n          this.logFileStream.write(logMessage);\n        }\n\n        // Log to category-specific file\n        this.writeToCategoryLog(category, logMessage);\n      }\n    });\n\n    // Log debug center setup\n    console.log('Debug logging enabled for event:', '*');\n  }\n\n  /**\n   * Setup Express middleware\n   */\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet());\n\n    // CORS middleware\n    if (this.config.server.cors) {\n      this.app.use(cors(this.config.server.cors));\n    } else {\n      this.app.use(\n        cors({\n          origin: '*',\n          credentials: true,\n        })\n      );\n    }\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: this.config.server.bodyLimit || '10mb' }));\n    this.app.use(\n      express.urlencoded({ extended: true, limit: this.config.server.bodyLimit || '10mb' })\n    );\n\n    // Request logging middleware\n    this.app.use((req: Request, res: Response, next: NextFunction) => {\n      const start = Date.now();\n\n      res.on('finish', () => {\n        const duration = Date.now() - start;\n        this.logEvent('request', 'completed', {\n          method: req.method,\n          url: req.url,\n          status: res.statusCode,\n          duration: duration,\n          userAgent: req.get('user-agent'),\n          ip: req.ip,\n        });\n      });\n\n      next();\n    });\n  }\n\n  /**\n   * Setup routes\n   */\n  private setupRoutes(): void {\n    // Health check endpoint\n    this.app.get('/health', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        status: status.running ? 'healthy' : 'unhealthy',\n        timestamp: new Date().toISOString(),\n        uptime: status.uptime,\n        memory: status.memory,\n        version: this.getModuleInfo().version,\n      });\n    });\n\n    // OpenAI-compatible endpoints\n    this.app.post('/v1/chat/completions', this.handleChatCompletions.bind(this));\n    this.app.post('/v1/completions', this.handleCompletions.bind(this));\n    this.app.get('/v1/models', this.handleModels.bind(this));\n\n    // Status endpoint\n    this.app.get('/status', (req: Request, res: Response) => {\n      res.json(this.getStatus());\n    });\n\n    // Test error endpoint\n    this.app.get('/test-error', async (req: Request, res: Response) => {\n      try {\n        // Simulate an error that should be handled by error handling center\n        const testError = new Error('This is a test error for error handling validation');\n        await this.handleError(testError, 'test_error_endpoint');\n\n        res.json({\n          message: 'Test error processed successfully',\n          error: testError.message,\n        });\n      } catch (error) {\n        res.status(500).json({\n          error: {\n            message: 'Failed to process test error',\n            type: 'test_error_failed',\n          },\n        });\n      }\n    });\n\n    // 404 handler\n    this.app.use('*', (req: Request, res: Response) => {\n      res.status(404).json({\n        error: {\n          message: 'Not Found',\n          type: 'not_found_error',\n          code: 'not_found',\n        },\n      });\n    });\n  }\n\n  /**\n   * Setup error handling\n   */\n  private setupErrorHandling(): void {\n    this.app.use((error: UnknownObject, req: Request, res: Response, _next: NextFunction) => {\n      this.handleError(error as unknown as Error, 'request_handler').catch(() => {\n        // Ignore handler errors, just send response\n      });\n\n      const errorStatus = (error as any).status || 500;\n      res.status(errorStatus).json({\n        error: {\n          message: (error as any).message || 'Internal Server Error',\n          type: (error as any).type || 'internal_error',\n          code: error.code || 'internal_error',\n        },\n      });\n    });\n  }\n\n  /**\n   * Handle chat completions\n   */\n  private async handleChatCompletions(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api', 'chat_completions_request', {\n        model: req.body.model,\n        messages: req.body.messages?.length || 0,\n      });\n\n      // Provider routing not yet implemented - return proper error\n      res.status(501).json({\n        error: {\n          message: 'Provider routing not yet implemented',\n          type: 'not_implemented',\n          code: 'provider_routing_not_implemented',\n        },\n      });\n      return;\n    } catch (error) {\n      await this.handleError(error as Error, 'chat_completions_handler');\n      throw error;\n    }\n  }\n\n  /**\n   * Handle completions\n   */\n  private async handleCompletions(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api', 'completions_request', {\n        model: req.body.model,\n        prompt: req.body.prompt?.length || 0,\n      });\n\n      // Provider routing not yet implemented - return proper error\n      res.status(501).json({\n        error: {\n          message: 'Provider routing not yet implemented',\n          type: 'not_implemented',\n          code: 'provider_routing_not_implemented',\n        },\n      });\n      return;\n    } catch (error) {\n      await this.handleError(error as Error, 'completions_handler');\n      throw error;\n    }\n  }\n\n  /**\n   * Handle models list\n   */\n  private async handleModels(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api', 'models_request', {});\n\n      // Return available models from configured providers\n      const models = [];\n\n      for (const [providerId, providerConfig] of Object.entries(this.config.providers)) {\n        if (providerConfig.enabled && providerConfig.models) {\n          for (const [modelId, _modelConfig] of Object.entries(providerConfig.models)) { // eslint-disable-line @typescript-eslint/no-unused-vars\n            models.push({\n              id: modelId,\n              object: 'model',\n              created: Math.floor(Date.now() / 1000),\n              owned_by: providerId,\n            });\n          }\n        }\n      }\n\n      res.json({\n        object: 'list',\n        data: models,\n      });\n    } catch (error) {\n      await this.handleError(error as Error, 'models_handler');\n      throw error;\n    }\n  }\n\n  /**\n   * Log event to debug center\n   */\n  private logEvent(category: string, action: string, data: UnknownObject): void {\n    try {\n      // Also log to category-specific file directly\n      if (this.config.logging.enableFile) {\n        const timestamp = new Date().toISOString();\n        const logMessage = `[${timestamp}] [EVENT] ${category}_${action}: ${JSON.stringify(\n          {\n            category,\n            action,\n            ...data,\n          },\n          null,\n          2\n        )}\\n`;\n\n        this.writeToCategoryLog(category, logMessage);\n      }\n\n      this.debugEventBus.publish({\n        sessionId: `session_${Date.now()}`,\n        moduleId: this.getModuleInfo().id,\n        operationId: `${category}_${action}`,\n        timestamp: Date.now(),\n        type: 'start',\n        position: 'middle',\n        data: {\n          category,\n          action,\n          ...data,\n        },\n      });\n    } catch (error) {\n      // Don't let logging errors break the server\n      console.error('Failed to log event:', error);\n    }\n  }\n\n  /**\n   * Handle error with error handling center\n   */\n  private async handleError(error: Error, context: string): Promise<void> {\n    try {\n      const errorContext: ErrorContext = {\n        error: error.message,\n        source: `${this.getModuleInfo().id}.${context}`,\n        severity: 'medium',\n        timestamp: Date.now(),\n        moduleId: this.getModuleInfo().id,\n        context: {\n          stack: error.stack,\n          name: error.name,\n        },\n      };\n\n      await this.errorHandling.handleError(errorContext);\n    } catch (handlerError) {\n      console.error('Failed to handle error:', handlerError);\n      console.error('Original error:', error);\n    }\n  }\n\n  /**\n   * Get module info\n   */\n  public getModuleInfo(): ModuleInfo {\n    return {\n      id: 'routecodex-server',\n      name: 'RouteCodexServer',\n      version: '0.0.1',\n      description: 'Multi-provider OpenAI proxy server',\n      type: 'server',\n    };\n  }\n\n  /**\n   * Initialize debug enhancements\n   */\n  private initializeDebugEnhancements(): void {\n    try {\n      console.log('RouteCodexServer debug enhancements initialized');\n    } catch (error) {\n      console.warn('Failed to initialize RouteCodexServer debug enhancements:', error);\n    }\n  }\n\n  /**\n   * Record server metric\n   */\n  private recordServerMetric(operation: string, data: UnknownObject): void {\n    if (!this.serverMetrics.has(operation)) {\n      this.serverMetrics.set(operation, {\n        values: [],\n        lastUpdated: Date.now(),\n      });\n    }\n\n    const metric = this.serverMetrics.get(operation)! as any;\n    metric.values.push(data);\n    metric.lastUpdated = Date.now();\n\n    // Keep only last 50 measurements\n    if (metric.values.length > 50) {\n      metric.values.shift();\n    }\n  }\n\n  /**\n   * Add to request history\n   */\n  private addToRequestHistory(operation: UnknownObject): void {\n    this.requestHistory.push(operation);\n\n    // Keep only recent history\n    if (this.requestHistory.length > this.maxHistorySize) {\n      this.requestHistory.shift();\n    }\n  }\n\n  /**\n   * Add to error history\n   */\n  private addToErrorHistory(operation: UnknownObject): void {\n    this.errorHistory.push(operation);\n\n    // Keep only recent history\n    if (this.errorHistory.length > this.maxHistorySize) {\n      this.errorHistory.shift();\n    }\n  }\n\n  /**\n   * Get debug status with enhanced information\n   */\n  getDebugStatus(): UnknownObject {\n    const baseStatus = {\n      serverId: this.getModuleInfo().id,\n      name: this.getModuleInfo().name,\n      version: this.getModuleInfo().version,\n      isInitialized: this._isInitialized,\n      isRunning: this._isRunning,\n      config: this.config.server,\n      isEnhanced: true,\n    };\n\n    return {\n      ...baseStatus,\n      debugInfo: this.getDebugInfo(),\n      serverMetrics: this.getServerMetrics(),\n      requestHistory: [...this.requestHistory.slice(-10)],\n      errorHistory: [...this.errorHistory.slice(-10)],\n    };\n  }\n\n  /**\n   * Get detailed debug information\n   */\n  private getDebugInfo(): UnknownObject {\n    return {\n      serverId: this.getModuleInfo().id,\n      name: this.getModuleInfo().name,\n      version: this.getModuleInfo().version,\n      enhanced: true,\n      uptime: this._isRunning ? process.uptime() : 0,\n      memory: process.memoryUsage(),\n      requestHistorySize: this.requestHistory.length,\n      errorHistorySize: this.errorHistory.length,\n      serverMetricsSize: this.serverMetrics.size,\n      maxHistorySize: this.maxHistorySize,\n      categoryLogStreams: this.categoryLogStreams.size,\n      hasLogFile: !!this.logFileStream,\n    };\n  }\n\n  /**\n   * Get server metrics\n   */\n  private getServerMetrics(): UnknownObject {\n    const metrics: UnknownObject = {};\n\n    for (const [operation, metric] of this.serverMetrics.entries()) {\n      const metricObj = metric as any;\n      metrics[operation] = {\n        count: metricObj.values.length,\n        lastUpdated: metricObj.lastUpdated,\n        recentValues: metricObj.values.slice(-5),\n      };\n    }\n\n    return metrics;\n  }\n}\n"
        },
        "src/server/anthropic-sse-simulator.ts": {
          "path": "src/server/anthropic-sse-simulator.ts",
          "size": 4131,
          "lines": 76,
          "imports": [],
          "exports": [
            "AnthropicSSESimulator"
          ],
          "classes": [
            "AnthropicSSESimulator"
          ],
          "functions": [],
          "content": "/**\n * Anthropic SSE Simulator (non-stream → SSE)\n *\n * Emits Anthropic-compatible SSE events from a complete Anthropic message object\n * by simulating incremental delivery (input_json_delta for tool_use, text_delta for text).\n * This module is stable and independently maintained.\n */\n\nexport type AnthropicEvent = { event: string; data: Record<string, unknown> };\n\nexport class AnthropicSSESimulator {\n  constructor(private jsonChunkBytes: number = Math.max(128, Math.min(4096, Number(process.env.ROUTECODEX_SSE_JSON_CHUNK || 1024)))) {}\n\n  /**\n   * Build SSE events sequence from a complete Anthropic message object.\n   * Expected shape: { type:'message', role:'assistant', model, content:[ {type:'text'| 'tool_use', ...} ], stop_reason?, usage? }\n   */\n  public buildEvents(message: any): AnthropicEvent[] {\n    const events: AnthropicEvent[] = [];\n    const id = (message && message.id) || `msg_${Date.now()}`;\n    const model = (message && typeof message.model === 'string' ? message.model : 'unknown');\n    const blocks: any[] = Array.isArray(message?.content) ? message.content : [];\n    const stopReason: string | null = (message?.stop_reason ?? null);\n    const usage: any = message?.usage ?? null;\n\n    // Start message\n    events.push({ event: 'message_start', data: { type: 'message_start', message: { id, type: 'message', role: 'assistant', model, content: [], stop_reason: null, stop_sequence: null } } });\n\n    let index = 0;\n    for (const block of blocks) {\n      const bType = block?.type;\n      if (bType === 'tool_use') {\n        const toolId = block?.id || `call_${Math.random().toString(36).slice(2, 10)}`;\n        const name = block?.name || 'tool';\n        const input = (block?.input && typeof block.input === 'object') ? block.input : {};\n        // Start tool block with input:{} as per Anthropic protocol\n        events.push({ event: 'content_block_start', data: { type: 'content_block_start', index, content_block: { type: 'tool_use', id: toolId, name, input: {} } } });\n        try {\n          const json = JSON.stringify(input);\n          for (let offset = 0; offset < json.length; offset += this.jsonChunkBytes) {\n            const partial = json.slice(offset, Math.min(json.length, offset + this.jsonChunkBytes));\n            events.push({ event: 'content_block_delta', data: { type: 'content_block_delta', index, delta: { type: 'input_json_delta', partial_json: partial } } });\n          }\n        } catch {\n          events.push({ event: 'content_block_delta', data: { type: 'content_block_delta', index, delta: { type: 'input_json_delta', partial_json: '{}' } } });\n        }\n        events.push({ event: 'content_block_stop', data: { type: 'content_block_stop', index } });\n        index++;\n        continue;\n      }\n      if (bType === 'text') {\n        const text = typeof block?.text === 'string' ? block.text : '';\n        events.push({ event: 'content_block_start', data: { type: 'content_block_start', index, content_block: { type: 'text', text: '' } } });\n        if (text) {\n          events.push({ event: 'content_block_delta', data: { type: 'content_block_delta', index, delta: { type: 'text_delta', text } } });\n        }\n        events.push({ event: 'content_block_stop', data: { type: 'content_block_stop', index } });\n        index++;\n        continue;\n      }\n      // Unknown block types -> stringify as text\n      events.push({ event: 'content_block_start', data: { type: 'content_block_start', index, content_block: { type: 'text', text: '' } } });\n      try { events.push({ event: 'content_block_delta', data: { type: 'content_block_delta', index, delta: { type: 'text_delta', text: JSON.stringify(block) } } }); } catch { /* ignore */ }\n      events.push({ event: 'content_block_stop', data: { type: 'content_block_stop', index } });\n      index++;\n    }\n\n    // message_delta\n    events.push({ event: 'message_delta', data: { type: 'message_delta', delta: { stop_reason: stopReason ?? null, stop_sequence: null }, ...(usage ? { usage } : {}) } });\n    // message_stop\n    events.push({ event: 'message_stop', data: { type: 'message_stop' } });\n    return events;\n  }\n}\n\n"
        },
        "src/server/anthropic-sse-transformer.ts": {
          "path": "src/server/anthropic-sse-transformer.ts",
          "size": 7846,
          "lines": 225,
          "imports": [],
          "exports": [
            "AnthropicSSETransformer"
          ],
          "classes": [
            "AnthropicSSETransformer"
          ],
          "functions": [],
          "content": "/**\n * Anthropic SSE Transformer (encapsulated, stable)\n *\n * Converts OpenAI ChatCompletion streaming chunks (JSON objects from `data: {...}`)\n * into Anthropic-compatible SSE events, emitting incremental input_json_delta for\n * tool_use blocks and text_delta for text. It maintains minimal per-stream state\n * to ensure correct content_block_start/stop and message_stop sequencing.\n *\n * This module is intended to be stable and independently maintained.\n */\n\nexport type AnthropicEvent = { event: string; data: Record<string, unknown> };\n\ntype ToolCallAccum = {\n  id: string;\n  name: string;\n  buffer: string;\n  started: boolean;\n  stopped: boolean;\n};\n\nexport class AnthropicSSETransformer {\n  private messageId = '';\n  private model = '';\n  private createdAt = 0;\n  private started = false;\n  private textStarted = false;\n  private textStopped = false;\n  private toolCalls: Map<number, ToolCallAccum> = new Map();\n  private finishReason: string | null = null;\n  private usage: { input_tokens?: number; output_tokens?: number } | null = null;\n  private messageDeltaSent = false;\n\n  constructor() {}\n\n  /**\n   * Map OpenAI finish_reason to Anthropic stop_reason\n   */\n  private mapFinish(reason: string | null | undefined): string | null {\n    if (!reason) {return null;}\n    switch (reason) {\n      case 'stop':\n        return 'end_turn';\n      case 'length':\n        return 'max_tokens';\n      case 'tool_calls':\n        return 'tool_use';\n      default:\n        return reason;\n    }\n  }\n\n  /**\n   * Process a single OpenAI streaming chunk (already parsed JSON object)\n   * and return Anthropic SSE events to emit for this chunk.\n   */\n  public processOpenAIChunk(chunk: any): AnthropicEvent[] {\n    const events: AnthropicEvent[] = [];\n\n    // Initialize IDs/model/created\n    try {\n      if (!this.messageId) {this.messageId = String(chunk.id || `chatcmpl_${Date.now()}`);}\n      if (!this.model) {this.model = String(chunk.model || 'unknown');}\n      if (!this.createdAt) {this.createdAt = Number(chunk.created || Math.floor(Date.now() / 1000));}\n    } catch { /* Empty catch block */ }\n\n    const choice = Array.isArray(chunk?.choices) ? chunk.choices[0] : undefined;\n    const delta = choice?.delta || {};\n\n    // message_start on first assistant role\n    if (!this.started && delta?.role === 'assistant') {\n      events.push({\n        event: 'message_start',\n        data: {\n          type: 'message_start',\n          message: {\n            id: this.messageId,\n            type: 'message',\n            role: 'assistant',\n            model: this.model,\n            content: [],\n            stop_reason: null,\n            stop_sequence: null,\n            usage: { input_tokens: 0, output_tokens: 0 }\n          }\n        }\n      });\n      this.started = true;\n    }\n\n    // Text delta\n    const contentText: string | undefined = typeof delta?.content === 'string' ? delta.content : undefined;\n    if (typeof contentText === 'string' && contentText.length > 0) {\n      if (!this.textStarted) {\n        events.push({\n          event: 'content_block_start',\n          data: { type: 'content_block_start', index: 0, content_block: { type: 'text', text: '' } }\n        });\n        this.textStarted = true;\n      }\n      events.push({\n        event: 'content_block_delta',\n        data: { type: 'content_block_delta', index: 0, delta: { type: 'text_delta', text: contentText } }\n      });\n    }\n\n    // Tool calls delta (array of deltas)\n    const toolCalls = Array.isArray(delta?.tool_calls) ? delta.tool_calls : [];\n    for (const tc of toolCalls) {\n      const index = Number(tc?.index ?? 0) + 1; // text is at index 0, tools start from 1\n      let acc = this.toolCalls.get(index);\n      if (!acc) {\n        acc = {\n          id: String(tc?.id || `call_${Math.random().toString(36).slice(2, 10)}`),\n          name: String(tc?.function?.name || 'tool'),\n          buffer: '',\n          started: false,\n          stopped: false\n        };\n        this.toolCalls.set(index, acc);\n      }\n\n      // Start tool_use block if not started\n      if (!acc.started) {\n        events.push({\n          event: 'content_block_start',\n          data: { type: 'content_block_start', index, content_block: { type: 'tool_use', id: acc.id, name: acc.name, input: {} } }\n        });\n        acc.started = true;\n      }\n\n      // Accumulate arguments string and emit input_json_delta\n      const argsPartRaw = tc?.function?.arguments;\n      if (typeof argsPartRaw === 'string' && argsPartRaw.length > 0) {\n        acc.buffer += argsPartRaw;\n        events.push({\n          event: 'content_block_delta',\n          data: { type: 'content_block_delta', index, delta: { type: 'input_json_delta', partial_json: argsPartRaw } }\n        });\n      }\n    }\n\n    // Finish reason on this chunk\n    const fr = choice?.finish_reason;\n    if (typeof fr === 'string' && fr.length > 0) {\n      this.finishReason = fr;\n      // Stop text block\n      if (this.textStarted && !this.textStopped) {\n        events.push({ event: 'content_block_stop', data: { type: 'content_block_stop', index: 0 } });\n        this.textStopped = true;\n      }\n      // Stop all tool blocks\n      for (const [index, acc] of this.toolCalls.entries()) {\n        if (acc.started && !acc.stopped) {\n          events.push({ event: 'content_block_stop', data: { type: 'content_block_stop', index } });\n          acc.stopped = true;\n        }\n      }\n      // Do not send message_delta yet; wait for usage if it will arrive in a later chunk\n    }\n\n    // Usage may arrive on separate chunk\n    const usage = chunk?.usage;\n    if (usage && typeof usage === 'object') {\n      const inTok = Number(usage?.prompt_tokens || 0);\n      const outTok = Number(usage?.completion_tokens || 0);\n      this.usage = { input_tokens: inTok, output_tokens: outTok };\n      // Send message_delta with final stop_reason and usage\n      const stop = this.mapFinish(this.finishReason);\n      events.push({\n        event: 'message_delta',\n        data: { type: 'message_delta', delta: { stop_reason: stop ?? null, stop_sequence: null }, usage: { input_tokens: inTok, output_tokens: outTok } }\n      });\n      this.messageDeltaSent = true;\n    }\n\n    return events;\n  }\n\n  /**\n   * Finalize stream (called when [DONE] is received) and return any trailing events\n   */\n  public finalize(): AnthropicEvent[] {\n    const events: AnthropicEvent[] = [];\n\n    // If we never sent message_start (no chunks), still emit minimal message\n    if (!this.started) {\n      events.push({\n        event: 'message_start',\n        data: {\n          type: 'message_start',\n          message: { id: this.messageId || `msg_${Date.now()}`, type: 'message', role: 'assistant', model: this.model || 'unknown', content: [], stop_reason: null, stop_sequence: null, usage: { input_tokens: 0, output_tokens: 0 } }\n        }\n      });\n      this.started = true;\n    }\n\n    // Ensure blocks are stopped if finish_reason was set but stops not sent\n    if (this.finishReason) {\n      if (this.textStarted && !this.textStopped) {\n        events.push({ event: 'content_block_stop', data: { type: 'content_block_stop', index: 0 } });\n        this.textStopped = true;\n      }\n      for (const [index, acc] of this.toolCalls.entries()) {\n        if (acc.started && !acc.stopped) {\n          events.push({ event: 'content_block_stop', data: { type: 'content_block_stop', index } });\n          acc.stopped = true;\n        }\n      }\n    }\n\n    // Send message_delta if not sent yet\n    if (!this.messageDeltaSent) {\n      const stop = this.mapFinish(this.finishReason);\n      events.push({ event: 'message_delta', data: { type: 'message_delta', delta: { stop_reason: stop ?? null, stop_sequence: null } } });\n      this.messageDeltaSent = true;\n    }\n\n    // Final stop\n    events.push({ event: 'message_stop', data: { type: 'message_stop' } });\n    return events;\n  }\n}\n\n"
        },
        "src/server/config/responses-config.ts": {
          "path": "src/server/config/responses-config.ts",
          "size": 9615,
          "lines": 262,
          "imports": [
            "../../utils/module-config-reader.js"
          ],
          "exports": [
            "ResponsesConversionConfig",
            "ResponsesSSEConfig",
            "ResponsesConversionMapping",
            "ResponsesModuleConfig",
            "ResponsesConfigUtil"
          ],
          "classes": [
            "ResponsesConfigUtil"
          ],
          "functions": [],
          "content": "import { ModuleConfigReader } from '../../utils/module-config-reader.js';\n\nexport interface ResponsesConversionConfig {\n  useLlmswitch: boolean;\n  fallbackEnabled: boolean;\n  forceProviderStream: boolean; // provider side stream=false when true\n}\n\nexport interface ResponsesSSEConfig {\n  heartbeatMs: number; // 0 to disable\n  emitTextItemLifecycle: boolean;\n  emitRequiredAction: boolean;\n}\n\nexport interface ResponsesConversionMapping {\n  request: {\n    instructionsPaths: string[];\n    inputBlocks: {\n      wrapperType: string; // e.g., 'message'\n      typeKey: string;     // e.g., 'type'\n      roleKey: string;     // e.g., 'role'\n      blocksKey: string;   // e.g., 'content'\n      textKey: string;     // e.g., 'text'\n      allowedContentTypes: string[]; // e.g., ['input_text','text']\n      ignoreRoles?: string[];\n      dedupe?: boolean;\n      dedupeDelimiter?: string;\n    };\n    fallback: {\n      useRawMessages: boolean;\n      rawMessagesPath: string; // e.g., 'messages'\n      pickLastUser: boolean;\n      dedupe?: boolean;\n      dedupeDelimiter?: string;\n    };\n    systemContentPaths?: string[];\n    systemJoiner?: string;\n  };\n  response: {\n    textPaths: string[];         // preferred text fields, with simple path syntax\n    textArrayTextKey: string;    // e.g., 'text'\n    contentBlocksKey: string;    // e.g., 'content'\n    messageWrapperType: string;  // e.g., 'message'\n    passthroughFields?: string[]; // additional fields to copy from provider to response\n    defaultValues?: Record<string, unknown>;\n  };\n  tools: {\n    toolCallTypes: string[];     // e.g., ['tool_call','function_call']\n    functionArgsPaths: string[]; // e.g., ['arguments','tool_call.function.arguments']\n    emitRequiredAction: boolean;\n  };\n}\n\nexport interface ResponsesModuleConfig {\n  conversion: ResponsesConversionConfig;\n  sse: ResponsesSSEConfig;\n  mappings: ResponsesConversionMapping;\n}\n\nconst DEFAULT_MAPPING: ResponsesConversionMapping = {\n  request: {\n    instructionsPaths: ['instructions'],\n    inputBlocks: {\n      wrapperType: 'message',\n      typeKey: 'type',\n      roleKey: 'role',\n      blocksKey: 'content',\n      textKey: 'text',\n      allowedContentTypes: ['input_text', 'text', 'output_text'],\n      ignoreRoles: ['system'],\n      dedupe: true,\n      dedupeDelimiter: '\\n\\n'\n    },\n    fallback: { useRawMessages: true, rawMessagesPath: 'messages', pickLastUser: true, dedupe: true, dedupeDelimiter: '\\n\\n' },\n    systemContentPaths: ['instructions'],\n    systemJoiner: '\\n\\n'\n  },\n  response: {\n    textPaths: ['output_text', 'choices[0].message.content'],\n    textArrayTextKey: 'text',\n    contentBlocksKey: 'content',\n    messageWrapperType: 'message',\n    passthroughFields: [\n      'background',\n      'error',\n      'incomplete_details',\n      'instructions',\n      'reasoning',\n      'tool_choice',\n      'tools',\n      'parallel_tool_calls',\n      'max_output_tokens',\n      'max_tool_calls',\n      'previous_response_id',\n      'prompt_cache_key',\n      'safety_identifier',\n      'service_tier',\n      'store',\n      'temperature',\n      'top_p',\n      'top_logprobs',\n      'truncation',\n      'metadata',\n      'user',\n      'text',\n      'usage'\n    ],\n    defaultValues: {\n      background: false,\n      error: null,\n      incomplete_details: null,\n      service_tier: 'default',\n      store: false,\n      temperature: 1,\n      top_p: 1,\n      top_logprobs: 0,\n      truncation: 'disabled',\n      metadata: {},\n      user: null,\n      text: { format: { type: 'text' }, verbosity: 'medium' },\n      max_output_tokens: null,\n      max_tool_calls: null,\n      previous_response_id: null,\n      prompt_cache_key: null,\n      reasoning: null,\n      safety_identifier: null\n    }\n  },\n  tools: {\n    toolCallTypes: ['tool_call', 'function_call'],\n    functionArgsPaths: ['arguments', 'tool_call.function.arguments'],\n    emitRequiredAction: true\n  }\n};\n\nconst DEFAULTS: ResponsesModuleConfig = {\n  conversion: {\n    useLlmswitch: true,\n    fallbackEnabled: false,\n    forceProviderStream: true // default: provider non-stream, server re-stream\n  },\n  sse: {\n    heartbeatMs: Number(process.env.ROUTECODEX_RESPONSES_HEARTBEAT_MS || 5000),\n    emitTextItemLifecycle: true,\n    emitRequiredAction: true\n  },\n  mappings: DEFAULT_MAPPING\n};\n\nexport class ResponsesConfigUtil {\n  static async load(): Promise<ResponsesModuleConfig> {\n    const fs = await import('fs/promises');\n    const path = await import('path');\n    const { fileURLToPath } = await import('url');\n    const fallbackDisabled = String(process.env.ROUTECODEX_DISABLE_CONFIG_FALLBACK || process.env.RCC_DISABLE_CONFIG_FALLBACK || '0') === '1';\n\n    // Helper: ascend from a starting dir to locate the package root (directory containing package.json)\n    const findPackageRoot = async (startDir: string): Promise<string> => {\n      let dir = startDir;\n      for (let i = 0; i < 6; i++) {\n        try {\n          const cand = path.join(dir, 'package.json');\n          const stat = await fs.stat(cand).catch(() => null as any);\n          if (stat && stat.isFile()) {return dir;}\n        } catch { /* continue */ }\n        const parent = path.dirname(dir);\n        if (parent === dir) {break;}\n        dir = parent;\n      }\n      return startDir; // best effort\n    };\n\n    // Resolve default mapping/schema paths relative to package root\n    const moduleDir = path.dirname(fileURLToPath(import.meta.url));\n    const pkgRoot = await findPackageRoot(moduleDir);\n    const defaultMappingPath = path.join(pkgRoot, 'config', 'responses-conversion.json');\n    const defaultSchemaPath = path.join(pkgRoot, 'config', 'schemas', 'responses-conversion.schema.json');\n\n    // First: try to load modules.json (optional). If missing and fallback allowed, continue with defaults.\n    let withEnv: ResponsesModuleConfig = DEFAULTS;\n    let mappingsPathFromConfig: string | null = null;\n    try {\n      const reader = new ModuleConfigReader('./config/modules.json');\n      const cfg = await reader.load();\n      const mod = reader.getModuleConfigValue<ResponsesModuleConfig>('responses', DEFAULTS) || DEFAULTS;\n      withEnv = this.mergeWithEnv(mod);\n      mappingsPathFromConfig = (cfg as any)?.modules?.responses?.config?.conversion?.mappingsPath || null;\n    } catch (e) {\n      if (fallbackDisabled) {\n        throw new Error('Failed to load responses module or mapping configuration.');\n      }\n      // Use DEFAULTS with env overrides\n      withEnv = this.mergeWithEnv(DEFAULTS);\n    }\n\n    // Env override for mappings path\n    const envMappingPath = (process.env.ROUTECODEX_RESP_MAPPINGS_PATH || process.env.RCC_RESP_MAPPINGS_PATH || '').trim();\n    const preferPath = envMappingPath || (mappingsPathFromConfig || '').trim();\n\n    // Resolve final mapping file path\n    const resolveMappingPath = (p: string | null): string => {\n      const raw = (p || '').trim();\n      if (!raw) {return defaultMappingPath;}\n      if (raw.startsWith('~')) {\n        const home = process.env.HOME || process.env.USERPROFILE || '';\n        return path.join(home, raw.slice(1));\n      }\n      if (path.isAbsolute(raw)) {return raw;}\n      // Treat as package-root relative or process CWD relative depending on prefix\n      if (raw.startsWith('./') || raw.startsWith('config/')) {return path.join(pkgRoot, raw.replace(/^\\.\\//, ''));}\n      return path.join(pkgRoot, raw);\n    };\n\n    // Load and validate mapping; if fails, fallback to DEFAULT_MAPPING when allowed\n    try {\n      const mappingFile = resolveMappingPath(preferPath || null);\n      const content = await fs.readFile(mappingFile, 'utf-8');\n      const parsed = JSON.parse(content);\n      const { SchemaValidator } = await import('./schema-validator.js');\n      await SchemaValidator.validateMapping(parsed, defaultSchemaPath);\n      withEnv.mappings = { ...(DEFAULT_MAPPING as any), ...(parsed as any) } as ResponsesConversionMapping;\n      return withEnv;\n    } catch (e) {\n      if (fallbackDisabled) {\n        throw new Error('Failed to load responses module or mapping configuration.');\n      }\n      // Fallback to DEFAULT_MAPPING embedded\n      withEnv.mappings = DEFAULT_MAPPING;\n      return withEnv;\n    }\n  }\n\n  private static mergeWithEnv(base: ResponsesModuleConfig): ResponsesModuleConfig {\n    const envBool = (v: string | undefined): boolean | undefined => {\n      if (v == null) {return undefined;}\n      const s = String(v).trim().toLowerCase();\n      if (['1','true','yes','on'].includes(s)) {return true;}\n      if (['0','false','no','off'].includes(s)) {return false;}\n      return undefined;\n    };\n\n    const c = { ...base } as ResponsesModuleConfig;\n    const useLlmswitch = envBool(process.env.ROUTECODEX_RESP_CONVERT_LLMSWITCH);\n    const fallback = envBool(process.env.ROUTECODEX_RESP_CONVERT_FALLBACK);\n    const forceNonStream = envBool(process.env.ROUTECODEX_RESP_PROVIDER_NONSTREAM);\n    const lifecycle = envBool(process.env.ROUTECODEX_RESP_SSE_LIFECYCLE);\n    const requiredAction = envBool(process.env.ROUTECODEX_RESP_SSE_REQUIRED_ACTION);\n    const hb = process.env.ROUTECODEX_RESPONSES_HEARTBEAT_MS;\n\n    if (useLlmswitch !== undefined) {c.conversion.useLlmswitch = useLlmswitch;}\n    if (fallback !== undefined) {c.conversion.fallbackEnabled = fallback;}\n    if (forceNonStream !== undefined) {c.conversion.forceProviderStream = forceNonStream;}\n    if (lifecycle !== undefined) {c.sse.emitTextItemLifecycle = lifecycle;}\n    if (requiredAction !== undefined) {c.sse.emitRequiredAction = requiredAction;}\n    if (hb !== undefined && hb !== '') {\n      const n = Number(hb);\n      if (Number.isFinite(n)) {c.sse.heartbeatMs = n;}\n    }\n    return c;\n  }\n}\n"
        },
        "src/server/config/schema-validator.ts": {
          "path": "src/server/config/schema-validator.ts",
          "size": 972,
          "lines": 24,
          "imports": [],
          "exports": [
            "SchemaValidator"
          ],
          "classes": [
            "SchemaValidator"
          ],
          "functions": [],
          "content": "export class SchemaValidator {\n  static async validateMapping(mapping: unknown, schemaPath: string): Promise<void> {\n    try {\n      const fs = await import('fs/promises');\n      const AjvMod: any = await import('ajv');\n      const schemaText = await fs.readFile(schemaPath, 'utf-8');\n      const schema = JSON.parse(schemaText);\n      const ajv = new AjvMod.default({ allErrors: true, strict: false });\n      const validate = ajv.compile(schema);\n      const ok = validate(mapping);\n      if (!ok) {\n        const errs = (validate.errors || []).map((e: any) => `${e.instancePath || ''} ${e.message || ''}`.trim()).join('; ');\n        throw new Error(`responses-conversion.json validation failed: ${errs || 'invalid mapping'}`);\n      }\n    } catch (e: any) {\n      // Fail hard per product requirement (no fallback)\n      const err = new Error(`Mapping schema validation error: ${e?.message || String(e)}`);\n      (err as any).status = 500;\n      throw err;\n    }\n  }\n}\n\n"
        },
        "src/server/conversion/responses-converter.ts": {
          "path": "src/server/conversion/responses-converter.ts",
          "size": 831,
          "lines": 26,
          "imports": [
            "express"
          ],
          "exports": [
            "ResponsesConverter"
          ],
          "classes": [
            "ResponsesConverter"
          ],
          "functions": [],
          "content": "import type { Request } from 'express';\n\n/**\n * Minimal Responses converter helpers used by ResponsesHandler.\n * We keep logic intentionally small to avoid over-validation.\n */\nexport class ResponsesConverter {\n  /**\n   * Infer streaming flag only when not explicitly provided.\n   * - If Accept header contains text/event-stream → true\n   * - Else undefined (do not override existing values)\n   */\n  static inferStreamingFlag(body: unknown, req: Request): boolean | undefined {\n    try {\n      const hasStream = (body as any)?.stream;\n      if (typeof hasStream === 'boolean') {return hasStream;}\n    } catch { /* ignore */ }\n    try {\n      const accept = String(req.headers['accept'] || '').toLowerCase();\n      if (accept.includes('text/event-stream')) {return true;}\n    } catch { /* ignore */ }\n    return undefined;\n  }\n}\n\n"
        },
        "src/server/conversion/responses-mapper.ts": {
          "path": "src/server/conversion/responses-mapper.ts",
          "size": 1690,
          "lines": 39,
          "imports": [],
          "exports": [
            "ResponsesMapper"
          ],
          "classes": [
            "ResponsesMapper"
          ],
          "functions": [],
          "content": "/**\n * Minimal Responses payload enrichment.\n * The goal is to preserve payloads and attach lightweight metadata when provided.\n */\nexport class ResponsesMapper {\n  static async enrichResponsePayload(\n    base: Record<string, unknown>,\n    _source?: Record<string, unknown>,\n    reqMeta?: Record<string, unknown>\n  ): Promise<Record<string, unknown>> {\n    const out: Record<string, unknown> = { ...(base || {}) };\n    try {\n      // Attach request tools metadata if present (do not mutate shapes)\n      if (reqMeta && typeof reqMeta === 'object') {\n        const metaIn = (out as any).metadata && typeof (out as any).metadata === 'object' ? { ...(out as any).metadata } : {};\n        if ((reqMeta as any).tools !== undefined && metaIn.tools === undefined) {\n          metaIn.tools = (reqMeta as any).tools;\n          try {\n            const crypto = await import('crypto');\n            const str = JSON.stringify(metaIn.tools);\n            const hash = crypto.createHash('sha256').update(str).digest('hex');\n            (metaIn as any).tools_hash = hash;\n            if (Array.isArray(metaIn.tools)) {(metaIn as any).tools_count = (metaIn.tools as any[]).length;}\n          } catch { /* ignore */ }\n        }\n        if ((reqMeta as any).tool_choice !== undefined && metaIn.tool_choice === undefined) {\n          metaIn.tool_choice = (reqMeta as any).tool_choice;\n        }\n        if ((reqMeta as any).parallel_tool_calls !== undefined && metaIn.parallel_tool_calls === undefined) {\n          metaIn.parallel_tool_calls = (reqMeta as any).parallel_tool_calls;\n        }\n        (out as any).metadata = metaIn;\n      }\n    } catch { /* ignore enrichment errors */ }\n    return out;\n  }\n}\n\n"
        },
        "src/server/core/service-container.ts": {
          "path": "src/server/core/service-container.ts",
          "size": 7900,
          "lines": 328,
          "imports": [
            "rcc-errorhandling",
            "rcc-debugcenter",
            "../../modules/pipeline/utils/debug-logger.js"
          ],
          "exports": [
            "ServiceDescriptor",
            "ServiceContainerConfig",
            "ServiceContainer",
            "getGlobalContainer",
            "initializeDefaultServices",
            "ServiceTokens"
          ],
          "classes": [
            "ServiceContainer"
          ],
          "functions": [
            "getGlobalContainer",
            "initializeDefaultServices"
          ],
          "content": "/**\n * Service Container - IoC Container for Dependency Injection\n * Provides lightweight dependency injection for RouteCodex server components\n */\n\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\n\n/**\n * Service lifetime enum\n */\nexport enum ServiceLifetime {\n  Transient = 'transient',\n  Singleton = 'singleton',\n  Scoped = 'scoped'\n}\n\n/**\n * Service descriptor interface\n */\nexport interface ServiceDescriptor {\n  token: string;\n  factory: () => any;\n  lifetime: ServiceLifetime;\n  instance?: any;\n}\n\n/**\n * Service container configuration\n */\nexport interface ServiceContainerConfig {\n  enableAutoRegistration?: boolean;\n  enableCircularDependencyDetection?: boolean;\n  defaultLifetime?: ServiceLifetime;\n}\n\n/**\n * Lightweight IoC Container\n */\nexport class ServiceContainer {\n  private static instance: ServiceContainer | null = null;\n  private services: Map<string, ServiceDescriptor> = new Map();\n  private config: ServiceContainerConfig;\n  private isDisposed = false;\n\n  constructor(config: ServiceContainerConfig = {}) {\n    this.config = {\n      enableAutoRegistration: true,\n      enableCircularDependencyDetection: true,\n      defaultLifetime: ServiceLifetime.Singleton,\n      ...config\n    };\n  }\n\n  /**\n   * Get or create the global ServiceContainer instance\n   */\n  public static getInstance(config: ServiceContainerConfig = {}): ServiceContainer {\n    if (!ServiceContainer.instance || ServiceContainer.instance.isDisposed) {\n      ServiceContainer.instance = new ServiceContainer(config);\n    }\n    return ServiceContainer.instance;\n  }\n\n  /**\n   * Register a service\n   */\n  register<T>(\n    token: string,\n    factory: () => T,\n    lifetime: ServiceLifetime = this.config.defaultLifetime!\n  ): void {\n    if (this.isDisposed) {\n      throw new Error('Cannot register services on disposed container');\n    }\n\n    this.services.set(token, {\n      token,\n      factory,\n      lifetime,\n      instance: undefined\n    });\n  }\n\n  /**\n   * Register a singleton instance\n   */\n  registerInstance<T>(token: string, instance: T): void {\n    if (this.isDisposed) {\n      throw new Error('Cannot register services on disposed container');\n    }\n\n    this.services.set(token, {\n      token,\n      factory: () => instance,\n      lifetime: ServiceLifetime.Singleton,\n      instance\n    });\n  }\n\n  /**\n   * Resolve a service\n   */\n  resolve<T>(token: string): T {\n    if (this.isDisposed) {\n      throw new Error('Cannot resolve services from disposed container');\n    }\n\n    const descriptor = this.getDescriptor(token);\n\n    switch (descriptor.lifetime) {\n      case ServiceLifetime.Singleton:\n        if (!descriptor.instance) {\n          descriptor.instance = this.createInstance(descriptor);\n        }\n        return descriptor.instance;\n\n      case ServiceLifetime.Transient:\n        return this.createInstance(descriptor);\n\n      case ServiceLifetime.Scoped:\n        // For now, treat scoped as transient\n        return this.createInstance(descriptor);\n\n      default:\n        throw new Error(`Unsupported service lifetime: ${descriptor.lifetime}`);\n    }\n  }\n\n  /**\n   * Try resolve service without throwing\n   */\n  tryResolve<T>(token: string): T | undefined {\n    try {\n      return this.resolve<T>(token);\n    } catch {\n      return undefined;\n    }\n  }\n\n  /**\n   * Check if service is registered\n   */\n  isRegistered(token: string): boolean {\n    return this.services.has(token);\n  }\n\n  /**\n   * Backwards-compatible alias for ServiceRegistry\n   */\n  has(token: string): boolean {\n    return this.isRegistered(token);\n  }\n\n  /**\n   * Resolve service or undefined (compat API)\n   */\n  get<T>(token: string): T | undefined {\n    return this.tryResolve<T>(token);\n  }\n\n  /**\n   * Create a fresh instance regardless of lifetime (compat API)\n   */\n  create<T>(token: string): T {\n    if (this.isDisposed) {\n      throw new Error('Cannot create services from disposed container');\n    }\n\n    const descriptor = this.getDescriptor(token);\n    return this.createInstance<T>({ ...descriptor, instance: undefined });\n  }\n\n  /**\n   * Get all registered service tokens\n   */\n  getRegisteredTokens(): string[] {\n    return Array.from(this.services.keys());\n  }\n\n  /**\n   * Create service instance with dependency detection\n   */\n  private createInstance<T>(descriptor: ServiceDescriptor): T {\n    try {\n      if (this.config.enableCircularDependencyDetection) {\n        // Simple circular dependency detection could be added here\n        // For now, just log the creation\n      }\n\n      return descriptor.factory();\n    } catch (error) {\n      throw new Error(`Failed to create instance of service ${descriptor.token}: ${error}`);\n    }\n  }\n\n  /**\n   * Dispose container and cleanup\n   */\n  dispose(): void {\n    if (this.isDisposed) {\n      return;\n    }\n\n    this.clear();\n    this.isDisposed = true;\n  }\n\n  /**\n   * Clear registered services without disposing container\n   */\n  clear(): void {\n    // Cleanup singleton instances if they have dispose\n    for (const descriptor of this.services.values()) {\n      if (descriptor.instance && typeof descriptor.instance.dispose === 'function') {\n        try {\n          descriptor.instance.dispose();\n        } catch (error) {\n          console.error(`Error disposing service ${descriptor.token}:`, error);\n        }\n      }\n    }\n    this.services.clear();\n  }\n\n  /**\n   * Get container statistics\n   */\n  getStats(): {\n    totalServices: number;\n    singletons: number;\n    transients: number;\n    scoped: number;\n  } {\n    let singletons = 0;\n    let transients = 0;\n    let scoped = 0;\n\n    for (const descriptor of this.services.values()) {\n      switch (descriptor.lifetime) {\n        case ServiceLifetime.Singleton:\n          singletons++;\n          break;\n        case ServiceLifetime.Transient:\n          transients++;\n          break;\n        case ServiceLifetime.Scoped:\n          scoped++;\n          break;\n      }\n    }\n\n    return {\n      totalServices: this.services.size,\n      singletons,\n      transients,\n      scoped\n    };\n  }\n  private getDescriptor(token: string): ServiceDescriptor {\n    const descriptor = this.services.get(token);\n    if (!descriptor) {\n      throw new Error(`Service not registered: ${token}`);\n    }\n    return descriptor;\n  }\n}\n\n/**\n * Global service container instance\n */\nlet globalContainer: ServiceContainer | null = null;\n\nexport function getGlobalContainer(): ServiceContainer {\n  if (!globalContainer) {\n    globalContainer = ServiceContainer.getInstance();\n  }\n  return globalContainer;\n}\n\n/**\n * Initialize default services in container\n */\nexport function initializeDefaultServices(container: ServiceContainer): void {\n  // Register core services\n  container.register(\n    'ErrorHandlingCenter',\n    () => new ErrorHandlingCenter(),\n    ServiceLifetime.Singleton\n  );\n\n  container.register(\n    'DebugEventBus',\n    () => DebugEventBus.getInstance(),\n    ServiceLifetime.Singleton\n  );\n\n  container.register(\n    'PipelineDebugLogger',\n    () => new PipelineDebugLogger(null, {\n      enableConsoleLogging: true,\n      enableDebugCenter: true\n    }),\n    ServiceLifetime.Singleton\n  );\n}\n\n/**\n * Service tokens for type-safe dependency injection\n */\nexport const ServiceTokens = {\n  ERROR_HANDLING_CENTER: 'ErrorHandlingCenter',\n  DEBUG_EVENT_BUS: 'DebugEventBus',\n  PIPELINE_DEBUG_LOGGER: 'PipelineDebugLogger',\n  REQUEST_VALIDATOR: 'RequestValidator',\n  RESPONSE_NORMALIZER: 'ResponseNormalizer',\n  STREAMING_MANAGER: 'StreamingManager',\n  PROTOCOL_DETECTOR: 'ProtocolDetector',\n  CONVERSION_ENGINE: 'ConversionEngine',\n  PIPELINE_MANAGER: 'PipelineManager',\n  ROUTE_POOLS: 'RoutePools',\n  ROUTE_META: 'RouteMeta',\n  ROUTING_CLASSIFIER: 'RoutingClassifier'\n} as const;\n\nexport type ServiceTokenType = typeof ServiceTokens[keyof typeof ServiceTokens];\n"
        },
        "src/server/core/service-initializer.ts": {
          "path": "src/server/core/service-initializer.ts",
          "size": 5071,
          "lines": 198,
          "imports": [
            "./service-container.js",
            "./service-registry.js",
            "rcc-errorhandling",
            "rcc-debugcenter",
            "../../modules/pipeline/utils/debug-logger.js"
          ],
          "exports": [
            "ServiceConfig",
            "ServiceInitializer"
          ],
          "classes": [
            "ServiceInitializer"
          ],
          "functions": [
            "createServiceInitializer"
          ],
          "content": "/**\n * Service Initializer\n * Initializes and configures all core services for the RouteCodex system\n */\n\nimport { ServiceContainer, ServiceTokens } from './service-container.js';\nimport { ServiceRegistry } from './service-registry.js';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\n\n/**\n * Service configuration interface\n */\nexport interface ServiceConfig {\n  errorHandling?: {\n    enableMetrics?: boolean;\n    logLevel?: 'debug' | 'info' | 'warn' | 'error';\n  };\n  debugEventBus?: {\n    enableMetrics?: boolean;\n    bufferSize?: number;\n  };\n  pipelineLogger?: {\n    enableConsoleLogging?: boolean;\n    enableDebugCenter?: boolean;\n    logLevel?: 'none' | 'basic' | 'detailed' | 'verbose';\n  };\n}\n\n/**\n * Default service configuration\n */\nconst DEFAULT_CONFIG: ServiceConfig = {\n  errorHandling: {\n    enableMetrics: true,\n    logLevel: 'info'\n  },\n  debugEventBus: {\n    enableMetrics: false,\n    bufferSize: 1000\n  },\n  pipelineLogger: {\n    enableConsoleLogging: true,\n    enableDebugCenter: false,\n    logLevel: 'detailed'\n  }\n};\n\n/**\n * Service Initializer Class\n * Responsible for setting up and configuring all core services\n */\nexport class ServiceInitializer {\n  private container: ServiceContainer;\n  private registry: ServiceRegistry;\n  private config: ServiceConfig;\n\n  constructor(config: ServiceConfig = {}) {\n    this.config = { ...DEFAULT_CONFIG, ...config };\n    this.container = ServiceContainer.getInstance();\n    this.registry = ServiceRegistry.getInstance();\n  }\n\n  /**\n   * Initialize all core services\n   */\n  async initialize(): Promise<void> {\n    try {\n      // Register core services\n      this.registerErrorHandling();\n      this.registerDebugEventBus();\n      this.registerPipelineLogger();\n\n      // Initialize services\n      await this.initializeServices();\n\n      console.log('✅ 核心服务初始化完成');\n    } catch (error) {\n      console.error('❌ 核心服务初始化失败:', error);\n      throw error;\n    }\n  }\n\n  /**\n   * Register ErrorHandlingCenter service\n   */\n  private registerErrorHandling(): void {\n    this.registry.registerService(ServiceTokens.ERROR_HANDLING_CENTER, {\n      singleton: true,\n      factory: () => {\n        const errorHandling = new ErrorHandlingCenter();\n\n        if (this.config.errorHandling?.enableMetrics) {\n          console.log('📊 Error handling metrics enabled');\n        }\n\n        return errorHandling;\n      }\n    });\n  }\n\n  /**\n   * Register DebugEventBus service\n   */\n  private registerDebugEventBus(): void {\n    this.registry.registerService(ServiceTokens.DEBUG_EVENT_BUS, {\n      singleton: true,\n      factory: () => {\n        const eventBus = DebugEventBus.getInstance();\n\n        if (this.config.debugEventBus?.enableMetrics) {\n          console.log('📊 Debug event bus metrics enabled');\n        }\n\n        return eventBus;\n      }\n    });\n  }\n\n  /**\n   * Register PipelineDebugLogger service\n   */\n  private registerPipelineLogger(): void {\n    this.registry.registerService(ServiceTokens.PIPELINE_DEBUG_LOGGER, {\n      singleton: true,\n      factory: () => {\n        const loggerConfig = {\n          enableConsoleLogging: this.config.pipelineLogger?.enableConsoleLogging ?? true,\n          enableDebugCenter: this.config.pipelineLogger?.enableDebugCenter ?? true,\n          logLevel: this.config.pipelineLogger?.logLevel ?? 'detailed'\n        };\n\n        return new PipelineDebugLogger(null, loggerConfig);\n      }\n    });\n  }\n\n  /**\n   * Initialize all registered services\n   */\n  private async initializeServices(): Promise<void> {\n    const serviceNames = ['ErrorHandlingCenter', 'DebugEventBus', 'PipelineDebugLogger'];\n\n    for (const serviceName of serviceNames) {\n      try {\n        const service = this.container.resolve(serviceName);\n        if (service) {\n          console.log(`✅ ${serviceName} service initialized`);\n        } else {\n          console.warn(`⚠️  ${serviceName} service not found in container`);\n        }\n      } catch (error) {\n        console.error(`❌ Failed to initialize ${serviceName}:`, error);\n        throw error;\n      }\n    }\n  }\n\n  /**\n   * Get service container instance\n   */\n  getContainer(): ServiceContainer {\n    return this.container;\n  }\n\n  /**\n   * Get service registry instance\n   */\n  getRegistry(): ServiceRegistry {\n    return this.registry;\n  }\n\n  /**\n   * Shutdown all services\n   */\n  async shutdown(): Promise<void> {\n    try {\n      // Clean up service container\n      this.container.clear();\n\n      console.log('✅ 核心服务已关闭');\n    } catch (error) {\n      console.error('❌ 关闭核心服务时出错:', error);\n      throw error;\n    }\n  }\n}\n\n/**\n * Create and initialize service initializer with default configuration\n */\nexport async function createServiceInitializer(config?: ServiceConfig): Promise<ServiceInitializer> {\n  const initializer = new ServiceInitializer(config);\n  await initializer.initialize();\n  return initializer;\n}\n"
        },
        "src/server/core/service-registry.ts": {
          "path": "src/server/core/service-registry.ts",
          "size": 4533,
          "lines": 170,
          "imports": [
            "./service-container.js",
            "rcc-errorhandling",
            "rcc-debugcenter",
            "../../modules/pipeline/utils/debug-logger.js"
          ],
          "exports": [
            "ServiceConfig",
            "ServiceRegistry"
          ],
          "classes": [
            "ServiceRegistry"
          ],
          "functions": [],
          "content": "/**\n * Core Service Registry\n * Centralized service registration and management for RouteCodex server\n */\n\nimport { ServiceContainer, ServiceLifetime, ServiceTokens } from './service-container.js';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\n\n/**\n * Service configuration interface\n */\nexport interface ServiceConfig {\n  singleton?: boolean;\n  factory?: () => any;\n  dependencies?: string[];\n  lazy?: boolean;\n}\n\n/**\n * Service registry for managing all core services\n */\nexport class ServiceRegistry {\n  private static instance: ServiceRegistry;\n  private container: ServiceContainer;\n\n  private constructor() {\n    this.container = ServiceContainer.getInstance();\n  }\n\n  /**\n   * Get singleton instance\n   */\n  public static getInstance(): ServiceRegistry {\n    if (!ServiceRegistry.instance) {\n      ServiceRegistry.instance = new ServiceRegistry();\n    }\n    return ServiceRegistry.instance;\n  }\n\n  /**\n   * Initialize core services\n   */\n  public initializeCoreServices(): void {\n    this.container.register(\n      ServiceTokens.ERROR_HANDLING_CENTER,\n      () => new ErrorHandlingCenter(),\n      ServiceLifetime.Singleton\n    );\n\n    this.container.register(\n      ServiceTokens.DEBUG_EVENT_BUS,\n      () => DebugEventBus.getInstance(),\n      ServiceLifetime.Singleton\n    );\n\n    this.container.register(\n      ServiceTokens.PIPELINE_DEBUG_LOGGER,\n      () => new PipelineDebugLogger(null, {\n        enableConsoleLogging: true,\n        enableDebugCenter: true\n      }),\n      ServiceLifetime.Singleton\n    );\n\n    this.container.register(\n      'ServiceRegistry',\n      () => this,\n      ServiceLifetime.Singleton\n    );\n  }\n\n  /**\n   * Register handler-specific services\n   */\n  public registerHandlerServices(handlerName: string, config?: any): void {\n    const servicePrefix = handlerName.toLowerCase().replace('handler', '');\n\n    // Register request validator\n    this.container.register(\n      `${servicePrefix}RequestValidator`,\n      () => {\n        const { RequestValidator } = require('../utils/request-validator.js');\n        return new RequestValidator();\n      },\n      ServiceLifetime.Singleton\n    );\n\n    // Register response normalizer\n    this.container.register(\n      `${servicePrefix}ResponseNormalizer`,\n      () => {\n        const { ResponseNormalizer } = require('../utils/response-normalizer.js');\n        return new ResponseNormalizer();\n      },\n      ServiceLifetime.Singleton\n    );\n\n    // Register streaming manager if enabled\n    if (config?.enableStreaming) {\n      this.container.register(\n        `${servicePrefix}StreamingManager`,\n        () => {\n          const { StreamingManager } = require('../utils/streaming-manager.js');\n          return new StreamingManager(config);\n        },\n        ServiceLifetime.Singleton\n      );\n    }\n  }\n\n  /**\n   * Register converter-specific services\n   */\n  public registerConverterServices(converterType: string, config?: any): void {\n    const servicePrefix = converterType.toLowerCase().replace('converter', '');\n\n    // 统一改为使用 sharedmodule/llmswitch-core，移除本地可选 utils 注册以避免重复实现。\n    // 如需额外的工具注册与 schema 归一化，请在 rcc-llmswitch-core 内扩展并从此处通过 core 进行接入。\n  }\n\n  /**\n   * Get service from container\n   */\n  public getService<T = any>(name: string): T | undefined {\n    return this.container.get<T>(name);\n  }\n\n  /**\n   * Check if service exists\n   */\n  public hasService(name: string): boolean {\n    return this.container.has(name);\n  }\n\n  /**\n   * Create service with configuration\n   */\n  public createService<T = any>(name: string): T {\n    return this.container.create<T>(name);\n  }\n\n  /**\n   * Register a new service\n   */\n  public registerService(name: string, config: ServiceConfig): void {\n    if (typeof config.factory !== 'function') {\n      throw new Error(`Service factory must be provided when registering ${name}`);\n    }\n    const lifetime = config.singleton === false ? ServiceLifetime.Transient : ServiceLifetime.Singleton;\n    this.container.register(name, config.factory, lifetime);\n  }\n\n  /**\n   * Get service container for advanced operations\n   */\n  public getContainer(): ServiceContainer {\n    return this.container;\n  }\n\n  /**\n   * Clear all services (mainly for testing)\n   */\n  public clear(): void {\n    this.container.clear();\n    ServiceRegistry.instance = null!;\n  }\n}\n"
        },
        "src/server/handlers/base-handler.ts": {
          "path": "src/server/handlers/base-handler.ts",
          "size": 14099,
          "lines": 459,
          "imports": [
            "express",
            "rcc-errorhandling",
            "rcc-debugcenter",
            "../../modules/pipeline/utils/debug-logger.js",
            "../types.js"
          ],
          "exports": [
            "ValidationResult",
            "ProtocolHandlerConfig",
            "ErrorResponse"
          ],
          "classes": [
            "BaseHandler"
          ],
          "functions": [
            "fallback"
          ],
          "content": "/**\n * Base Handler Abstract Class\n * Provides common functionality for all protocol handlers\n */\n\nimport { type Request, type Response } from 'express';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\nimport { RouteCodexError } from '../types.js';\nimport {\n  ServiceContainer,\n  ServiceLifetime,\n  ServiceTokens,\n  initializeDefaultServices\n} from '../core/service-container.js';\n\n/**\n * Request validation result interface\n */\nexport interface ValidationResult {\n  isValid: boolean;\n  errors: string[];\n}\n\n/**\n * Protocol handler configuration interface\n */\nexport interface ProtocolHandlerConfig {\n  enableStreaming?: boolean;\n  enableMetrics?: boolean;\n  enableValidation?: boolean;\n  rateLimitEnabled?: boolean;\n  authEnabled?: boolean;\n  targetUrl?: string;\n  timeout?: number;\n  enablePipeline?: boolean;\n  pipelineProvider?: {\n    defaultProvider: string;\n    modelMapping: Record<string, string>;\n  };\n}\n\n/**\n * Error response interface\n */\nexport interface ErrorResponse {\n  status: number;\n  body: {\n    error: {\n      message: string;\n      type: string;\n      code: string;\n      param?: string | null;\n      details?: Record<string, unknown>;\n    };\n  };\n}\n\n/**\n * Base Handler Abstract Class\n * Provides common functionality for all endpoint handlers\n */\nexport abstract class BaseHandler {\n  protected config: ProtocolHandlerConfig;\n  protected serviceContainer: ServiceContainer;\n  protected errorHandling: ErrorHandlingCenter;\n  protected debugEventBus: DebugEventBus;\n  protected logger: PipelineDebugLogger;\n  private routePools: Record<string, string[]> | null = null;\n  private routeMeta: Record<string, { providerId: string; modelId: string; keyId?: string }> | null = null;\n  private pipelineManager: any | null = null;\n  private classifier: { classify: (payload: unknown) => Promise<unknown> } | null = null;\n  private classifierConfig: Record<string, unknown> | null = null;\n  private rrIndex: Map<string, number> = new Map();\n\n  constructor(config: ProtocolHandlerConfig, serviceContainer?: ServiceContainer) {\n    this.config = {\n      enableStreaming: true,\n      enableMetrics: true,\n      enableValidation: true,\n      rateLimitEnabled: false,\n      authEnabled: false,\n      timeout: 30000,\n      enablePipeline: false,\n      ...config,\n    };\n\n    // Use provided service container or create default\n    this.serviceContainer = serviceContainer || ServiceContainer.getInstance();\n\n    this.ensureCoreServices();\n\n    this.errorHandling = this.resolveOrRegister(\n      ServiceTokens.ERROR_HANDLING_CENTER,\n      () => new ErrorHandlingCenter(),\n      ServiceLifetime.Singleton\n    );\n\n    this.debugEventBus = this.resolveOrRegister(\n      ServiceTokens.DEBUG_EVENT_BUS,\n      () => DebugEventBus.getInstance(),\n      ServiceLifetime.Singleton\n    );\n\n    this.logger = this.resolveOrRegister(\n      ServiceTokens.PIPELINE_DEBUG_LOGGER,\n      () => new PipelineDebugLogger(null, {\n        enableConsoleLogging: true,\n        enableDebugCenter: true,\n      }),\n      ServiceLifetime.Singleton\n    );\n  }\n\n  /**\n   * Handle incoming request - must be implemented by subclasses\n   */\n  abstract handleRequest(req: Request, res: Response): Promise<void>;\n\n  /**\n   * Validate request - can be overridden by subclasses\n   */\n  protected validateRequest(req: Request): ValidationResult {\n    // Basic validation logic\n    return { isValid: true, errors: [] };\n  }\n\n  /**\n   * Build standardized error response\n   */\n  protected buildErrorResponse(error: any, requestId: string): ErrorResponse {\n    return this.buildErrorPayload(error, requestId);\n  }\n\n  /**\n   * Log helper forwarding to PipelineDebugLogger\n   */\n  protected logModule(moduleId: string, action: string, data?: Record<string, unknown>): void {\n    this.logger.logModule(moduleId, action, data);\n  }\n\n  protected logError(error: unknown, context?: Record<string, unknown>): void {\n    this.logger.logError(error, context);\n  }\n\n  /**\n   * Sanitize headers for logging/security\n   */\n  protected sanitizeHeaders(headers: any): Record<string, string> {\n    const sanitized: Record<string, string> = {};\n    const sensitiveHeaders = ['authorization', 'api-key', 'x-api-key', 'cookie'];\n\n    for (const [key, value] of Object.entries(headers)) {\n      if (sensitiveHeaders.includes(key.toLowerCase())) {\n        sanitized[key] = '[REDACTED]';\n      } else {\n        sanitized[key] = Array.isArray(value) ? value.join(', ') : String(value);\n      }\n    }\n\n    return sanitized;\n  }\n\n  /**\n   * Generate unique request ID\n   */\n  protected generateRequestId(): string {\n    return `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  /**\n   * Send JSON response with standard headers\n   */\n  protected sendJsonResponse(res: Response, data: any, requestId: string): void {\n    res.setHeader('x-request-id', requestId);\n    res.setHeader('Cache-Control', 'no-store');\n    res.setHeader('Content-Type', 'application/json; charset=utf-8');\n    res.status(200).json(data);\n  }\n\n  /**\n   * Log request completion\n   */\n  protected logCompletion(requestId: string, startTime: number, success: boolean): void {\n    const duration = Date.now() - startTime;\n    this.logger.logModule(this.constructor.name, 'request_complete', {\n      requestId,\n      duration,\n      success,\n    });\n  }\n\n  /**\n   * Handle error with proper logging and response\n   */\n  protected async handleError(error: Error, res: Response, requestId: string): Promise<void> {\n    const errorResponse = this.buildErrorResponse(error, requestId);\n\n    if (!res.headersSent) {\n      res.setHeader('x-request-id', requestId);\n      res.setHeader('Cache-Control', 'no-store');\n      res.setHeader('Content-Type', 'application/json; charset=utf-8');\n    }\n\n    res.status(errorResponse.status).json(errorResponse.body);\n  }\n\n  /**\n   * Build error payload - can be overridden by subclasses for custom error handling\n   */\n  private buildErrorPayload(error: unknown, requestId: string): ErrorResponse {\n    const e = error as Record<string, unknown>;\n\n    // Determine status code\n    const statusFromObj = typeof e?.status === 'number' ? e.status\n      : (typeof e?.statusCode === 'number' ? e.statusCode\n        : (e?.response && typeof e.response === 'object' && e.response !== null && 'status' in e.response && typeof (e.response as Record<string, unknown>).status === 'number' ? (e.response as Record<string, unknown>).status : undefined));\n    const routeCodexStatus = error instanceof RouteCodexError ? error.status : undefined;\n    const status = statusFromObj ?? routeCodexStatus ?? 500;\n\n    // Extract error message\n    const response = e?.response as any;\n    const data = e?.data as any;\n    const upstreamMsg = response?.data?.error?.message\n      || response?.data?.message\n      || data?.error?.message\n      || data?.message\n      || (typeof e?.message === 'string' ? e.message : undefined);\n\n    let message = upstreamMsg ? String(upstreamMsg) : (error instanceof Error ? error.message : String(error));\n\n    // Handle object stringification\n    if (message && /^\\[object\\s+Object\\]$/.test(message)) {\n      const serializable = e?.response && typeof e.response === 'object' && e.response !== null && 'data' in e.response && (e.response as Record<string, unknown>).data && typeof (e.response as Record<string, unknown>).data === 'object' && (e.response as Record<string, unknown>).data !== null ? (e.response as Record<string, unknown>).data\n        : e?.error ? e.error\n        : e?.data ? e.data\n        : e;\n      try {\n        message = JSON.stringify(serializable);\n      } catch {\n        message = 'Unknown error';\n      }\n    }\n\n    // Determine error type and code\n    const mapStatusToType = (s: number): string => {\n      if (s === 400) {return 'bad_request';}\n      if (s === 401) {return 'unauthorized';}\n      if (s === 403) {return 'forbidden';}\n      if (s === 404) {return 'not_found';}\n      if (s === 408) {return 'request_timeout';}\n      if (s === 409) {return 'conflict';}\n      if (s === 422) {return 'unprocessable_entity';}\n      if (s === 429) {return 'rate_limit_exceeded';}\n      if (s >= 500) {return 'server_error';}\n      return 'internal_error';\n    };\n\n    const rcxCode = error instanceof RouteCodexError ? error.code : undefined;\n    const upstreamCode = response?.data?.error?.code || (typeof e?.code === 'string' ? e.code : undefined);\n    const type = rcxCode || mapStatusToType(status as number);\n    const code = upstreamCode || type;\n\n    return {\n      status: status as number,\n      body: {\n        error: {\n          message,\n          type,\n          code,\n          param: null,\n          details: {\n            requestId,\n          },\n        },\n      },\n    };\n  }\n\n  /**\n   * Attach pipeline manager (called by router/server)\n   */\n  public attachPipelineManager(pipelineManager: unknown): void {\n    this.pipelineManager = pipelineManager;\n    if (pipelineManager) {\n      this.serviceContainer.registerInstance(ServiceTokens.PIPELINE_MANAGER, pipelineManager);\n    }\n  }\n\n  public attachRoutePools(routePools: Record<string, string[]>): void {\n    this.routePools = routePools;\n    this.serviceContainer.registerInstance(ServiceTokens.ROUTE_POOLS, routePools);\n  }\n\n  public attachRouteMeta(routeMeta: Record<string, { providerId: string; modelId: string; keyId: string }>): void {\n    this.routeMeta = routeMeta;\n    this.serviceContainer.registerInstance(ServiceTokens.ROUTE_META, routeMeta);\n  }\n\n  public attachRoutingClassifier(classifier: { classify: (payload: unknown) => Promise<unknown> }): void {\n    this.classifier = classifier;\n    this.serviceContainer.registerInstance(ServiceTokens.ROUTING_CLASSIFIER, classifier);\n  }\n\n  public attachRoutingClassifierConfig(classifierConfig: Record<string, unknown>): void {\n    this.classifierConfig = classifierConfig;\n  }\n\n  protected shouldUsePipeline(): boolean {\n    return this.config.enablePipeline ?? false;\n  }\n\n  protected getPipelineManager(): any {\n    if (this.pipelineManager) {\n      return this.pipelineManager;\n    }\n\n    const resolved = this.serviceContainer.tryResolve<any>('PipelineManager');\n    if (resolved) {\n      this.pipelineManager = resolved;\n      return resolved;\n    }\n\n    const globalPipeline = (globalThis as any)?.pipelineManager;\n    if (globalPipeline) {\n      this.pipelineManager = globalPipeline;\n      return globalPipeline;\n    }\n\n    return null;\n  }\n\n  protected getRoutePools(): Record<string, string[]> | null {\n    if (this.routePools) {\n      return this.routePools;\n    }\n\n    const resolved = this.serviceContainer.tryResolve<Record<string, string[]>>(ServiceTokens.ROUTE_POOLS);\n    if (resolved) {\n      this.routePools = resolved;\n      return resolved;\n    }\n\n    const globalPools = (globalThis as any)?.routePools;\n    if (globalPools) {\n      this.routePools = globalPools;\n      return globalPools;\n    }\n\n    return null;\n  }\n\n  protected getRouteMeta(): Record<string, { providerId: string; modelId: string; keyId?: string }> | null {\n    if (this.routeMeta) {\n      return this.routeMeta;\n    }\n\n    const resolved = this.serviceContainer.tryResolve<Record<string, { providerId: string; modelId: string; keyId?: string }>>(ServiceTokens.ROUTE_META);\n    if (resolved) {\n      this.routeMeta = resolved;\n      return resolved;\n    }\n\n    const globalMeta = (globalThis as any)?.routeMeta;\n    if (globalMeta) {\n      this.routeMeta = globalMeta;\n      return globalMeta;\n    }\n\n    return null;\n  }\n\n  protected getClassifier(): { classify: (payload: unknown) => Promise<unknown> } | null {\n    if (this.classifier) {\n      return this.classifier;\n    }\n\n    const resolved = this.serviceContainer.tryResolve<{ classify: (payload: unknown) => Promise<unknown> }>(ServiceTokens.ROUTING_CLASSIFIER);\n    if (resolved) {\n      this.classifier = resolved;\n      return resolved;\n    }\n    return null;\n  }\n\n  protected async decideRouteCategoryAsync(req: Request, defaultEndpoint: string = '/v1/chat/completions'): Promise<string> {\n    const fallback = () => {\n      const pools = this.getRoutePools();\n      if (!pools) {\n        return 'default';\n      }\n      if (pools.default) {\n        return 'default';\n      }\n      const keys = Object.keys(pools);\n      return keys[0] ?? 'default';\n    };\n\n    try {\n      const classifier = this.getClassifier();\n      if (!classifier) {\n        return fallback();\n      }\n\n      const payload = {\n        request: req.body,\n        endpoint: req.url || defaultEndpoint,\n        protocol: 'openai'\n      };\n\n      const result = await classifier.classify(payload);\n      const route = result && typeof result === 'object' && result !== null && 'route' in result\n        ? (result as Record<string, unknown>).route\n        : undefined;\n\n      if (typeof route === 'string' && this.getRoutePools()?.[route]) {\n        return route;\n      }\n\n      return fallback();\n    } catch {\n      return fallback();\n    }\n  }\n\n  protected pickPipelineId(routeName: string): string {\n    const pools = this.getRoutePools();\n    const pool = (pools && pools[routeName]) || [];\n    if (pool.length === 0) {\n      throw new Error(`No pipelines available for route ${routeName}`);\n    }\n    const idx = this.rrIndex.get(routeName) ?? 0;\n    const chosen = pool[idx % pool.length];\n    this.rrIndex.set(routeName, (idx + 1) % pool.length);\n    return chosen;\n  }\n\n  protected getClassifierConfig(): Record<string, unknown> | null {\n    return this.classifierConfig;\n  }\n\n  private ensureCoreServices(): void {\n    if (!this.serviceContainer.isRegistered(ServiceTokens.ERROR_HANDLING_CENTER)) {\n      initializeDefaultServices(this.serviceContainer);\n    }\n  }\n\n  private resolveOrRegister<T>(token: string, factory: () => T, lifetime: ServiceLifetime): T {\n    const existing = this.serviceContainer.tryResolve<T>(token);\n    if (existing) {\n      return existing;\n    }\n\n    this.serviceContainer.register(token, factory, lifetime);\n    return this.serviceContainer.resolve<T>(token);\n  }\n}\n"
        },
        "src/server/handlers/chat-completions.ts": {
          "path": "src/server/handlers/chat-completions.ts",
          "size": 17359,
          "lines": 399,
          "imports": [
            "express",
            "./base-handler.js",
            "../types.js",
            "../types.js",
            "../utils/request-validator.js",
            "../utils/response-normalizer.js",
            "../utils/streaming-manager.js",
            "../protocol/protocol-detector.js",
            "../protocol/openai-adapter.js",
            "../../modules/pipeline/utils/debug-logger.js",
            "../utils/error-context.js",
            "os",
            "path"
          ],
          "exports": [
            "ChatCompletionsHandler"
          ],
          "classes": [
            "ChatCompletionsHandler"
          ],
          "functions": [
            "currentSys",
            "snapshotDir",
            "writeSnapshot",
            "ctSummary"
          ],
          "content": "/**\n * Chat Completions Handler Implementation\n * Handles OpenAI-compatible chat completion requests\n */\n\nimport { type Request, type Response } from 'express';\nimport { BaseHandler, type ProtocolHandlerConfig } from './base-handler.js';\nimport { type RequestContext, type OpenAIChatCompletionRequest } from '../types.js';\nimport { RouteCodexError } from '../types.js';\nimport { RequestValidator } from '../utils/request-validator.js';\nimport { ResponseNormalizer } from '../utils/response-normalizer.js';\nimport { StreamingManager } from '../utils/streaming-manager.js';\nimport { ProtocolDetector } from '../protocol/protocol-detector.js';\nimport { OpenAIAdapter } from '../protocol/openai-adapter.js';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\nimport { ErrorContextBuilder, EnhancedRouteCodexError, type ErrorContext } from '../utils/error-context.js';\nimport os from 'os';\nimport path from 'path';\n\n/**\n * Chat Completions Handler\n * Handles /v1/chat/completions endpoint\n */\nexport class ChatCompletionsHandler extends BaseHandler {\n  private requestValidator: RequestValidator;\n  private responseNormalizer: ResponseNormalizer;\n  private streamingManager: StreamingManager;\n\n  constructor(config: ProtocolHandlerConfig) {\n    super(config);\n    this.requestValidator = new RequestValidator();\n    this.responseNormalizer = new ResponseNormalizer();\n    this.streamingManager = new StreamingManager(config);\n  }\n\n  /**\n   * Handle chat completions request\n  */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = this.generateRequestId();\n\n    // 记录开始时间用于错误上下文\n    (req as any).__startTime = startTime;\n    // Capture raw request for debugging\n    try {\n      const rawBody = JSON.parse(JSON.stringify((req as any).body || {}));\n      (req as any).__rawBody = rawBody;\n      const fs = await import('fs/promises');\n      const dir = path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat');\n      await (fs as any).mkdir(dir, { recursive: true });\n      const rawFile = path.join(dir, `${requestId}_raw-request.json`);\n      const payload = {\n        requestId,\n        method: req.method,\n        url: req.originalUrl || req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        body: rawBody,\n      };\n      await (fs as any).writeFile(rawFile, JSON.stringify(payload, null, 2), 'utf-8');\n    } catch (error) {\n      // 快速死亡原则 - 暴露捕获失败的原因\n      throw new EnhancedRouteCodexError(error as Error, {\n        module: 'ChatCompletionsHandler',\n        file: 'src/server/handlers/chat-completions.ts',\n        function: 'handleRequest',\n        line: 58,\n        requestId,\n        additional: {\n          operation: 'raw-request-capture',\n          payload: ErrorContextBuilder.safeStringify({\n            requestId,\n            method: req.method,\n            url: req.originalUrl || req.url\n          }, 500)\n        }\n      });\n    }\n\n    this.logModule('ChatCompletionsHandler', 'request_start', {\n      requestId,\n      model: req.body.model,\n      messageCount: req.body.messages?.length || 0,\n      streaming: req.body.stream || false,\n      tools: !!req.body.tools,\n      timestamp: startTime,\n    });\n\n    try {\n      // 可选：受控的“静态”系统提示注入（默认关闭）。\n      // 禁止样本抓取式注入；仅当明确配置 ROUTECODEX_ENABLE_SERVER_SYSTEM_PROMPT=1 且提供\n      // ROUTECODEX_SERVER_SYSTEM_PROMPT=</path/to/prompt.txt> 时，才会替换/插入一条 system。\n      try {\n        const enable = String(process.env.ROUTECODEX_ENABLE_SERVER_SYSTEM_PROMPT || '0') === '1';\n        const filePath = String(process.env.ROUTECODEX_SERVER_SYSTEM_PROMPT || '').trim();\n        if (enable && filePath && Array.isArray((req.body as any)?.messages)) {\n          const fs = await import('fs/promises');\n          const sysText = await fs.readFile(filePath, 'utf-8');\n          const messages = (req.body as any).messages as any[];\n          const currentSys = (() => {\n            try {\n              const m = messages[0];\n              return (m && typeof m === 'object' && m.role === 'system' && typeof m.content === 'string') ? String(m.content) : '';\n            } catch { return ''; }\n          })();\n          const hasMdMarkers = /\\bCLAUDE\\.md\\b|\\bAGENT(?:S)?\\.md\\b/i.test(currentSys);\n          const replaceSystemInOpenAIMessagesSimple = (msgs: any[], sys: string): any[] => {\n            if (!Array.isArray(msgs)) {return msgs;}\n            const out = msgs.slice();\n            if (out.length > 0 && out[0] && out[0].role === 'system') {\n              out[0] = { ...out[0], role: 'system', content: sys };\n            } else {\n              out.unshift({ role: 'system', content: sys });\n            }\n            return out;\n          };\n          if (!hasMdMarkers) {\n            (req.body as any).messages = replaceSystemInOpenAIMessagesSimple(messages, sysText) as any[];\n            try { res.setHeader('x-rc-system-prompt-source', 'static-file'); } catch { /* ignore */ }\n          }\n        }\n      } catch (error) {\n        // 注入失败不影响主流程\n        this.logger.logModule('ChatCompletionsHandler', 'system-prompt-static-skip', {\n          requestId,\n          reason: (error as Error)?.message || String(error)\n        });\n      }\n\n      // Strict protocol separation: Chat endpoint only accepts OpenAI Chat payload.\n      // Do NOT auto-convert Anthropic/Responses-shaped payloads here to avoid cross-path pollution.\n      try {\n        const detector = new ProtocolDetector();\n        const det = detector.detectFromRequest(req);\n        const looksAnthropicContent = Array.isArray(req.body?.messages) && (req.body.messages as any[]).some((m: any) => Array.isArray(m?.content) && m.content.some((c: any) => c && typeof c === 'object' && typeof c.type === 'string'));\n        if (det.protocol === 'anthropic' || det.protocol === 'responses' || looksAnthropicContent) {\n          throw new RouteCodexError('Chat endpoint only accepts OpenAI Chat payload (messages: string or OpenAI content parts). Use the Responses endpoint for Responses-shaped payloads.', 'invalid_protocol', 400);\n        }\n      } catch (e) {\n        if (e instanceof RouteCodexError) {throw e;}\n      }\n\n      // Tool guidance和归一化在 llmswitch-core 的 OpenAI 工具阶段统一处理\n      try { res.setHeader('x-rc-conversion-profile', 'openai-openai'); } catch { /* ignore */ }\n\n      // Skip strict request validation (preserve raw inputs)\n\n      // Process request through pipeline\n      const pipelineResponse = await this.processChatRequest(req, requestId);\n\n      // Handle streaming vs non-streaming response (also honor Accept: text/event-stream)\n      const accept = String(req.headers['accept'] || '').toLowerCase();\n      const wantsSSE = (accept.includes('text/event-stream') || req.body?.stream === true);\n      if (wantsSSE) {\n        const streamModel = (pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse)\n          ? ((pipelineResponse as any).data?.model ?? req.body.model)\n          : req.body.model;\n        await this.streamingManager.streamResponse(pipelineResponse, requestId, res, streamModel);\n        return;\n      }\n\n      // Return JSON response\n      const payload = pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse\n        ? (pipelineResponse as Record<string, unknown>).data\n        : pipelineResponse;\n      // 工具文本→tool_calls 的归一化已在 llmswitch-core (openai-openai codec) 统一完成，这里不再做重复处理\n      const normalized = this.responseNormalizer.normalizeOpenAIResponse(payload, 'chat');\n      // Chat 路径不注入 Responses 的 required_action 结构，保持协议纯净\n      this.sendJsonResponse(res, normalized, requestId);\n\n      this.logCompletion(requestId, startTime, true);\n    } catch (error) {\n      this.logCompletion(requestId, startTime, false);\n      // If the client requested SSE streaming, emit an SSE error and end the stream\n      try {\n        const accept = String(req.headers['accept'] || '').toLowerCase();\n        const wantsSSE = (accept.includes('text/event-stream') || req.body?.stream === true);\n        if (wantsSSE && !res.writableEnded) {\n          try {\n            if (!res.headersSent) {\n              res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n              res.setHeader('Cache-Control', 'no-cache, no-transform');\n              res.setHeader('Connection', 'keep-alive');\n              try { res.setHeader('X-Accel-Buffering', 'no'); } catch { /* ignore */ }\n              res.setHeader('x-request-id', requestId);\n              try { (res as any).flushHeaders?.(); } catch { /* ignore */ }\n            }\n            const model = typeof req.body?.model === 'string' ? req.body.model : 'unknown';\n            const errMsg = (error instanceof Error ? error.message : String(error)) || 'stream error';\n            const errChunk = {\n              id: `chatcmpl-${Date.now()}`,\n              object: 'chat.completion.chunk',\n              created: Math.floor(Date.now() / 1000),\n              model,\n              choices: [{ index: 0, delta: { content: `Error: ${errMsg}` }, finish_reason: 'error' }]\n            } as any;\n            res.write(`data: ${JSON.stringify(errChunk)}\\n\\n`);\n            res.write(`data: [DONE]\\n\\n`);\n          } catch { /* ignore */ }\n          try { res.end(); } catch { /* ignore */ }\n          return;\n        }\n      } catch { /* ignore */ }\n\n      await this.handleEnhancedError(error as Error, res, requestId, req);\n    }\n  }\n\n  /**\n   * 增强的错误处理函数 - 符合9大架构原则\n   * 实现快速死亡和暴露问题原则\n   */\n  private async handleEnhancedError(error: Error, res: Response, requestId: string, req: Request): Promise<void> {\n    // 构建详细的错误上下文\n    const context: ErrorContext = {\n      module: 'ChatCompletionsHandler',\n      file: 'src/server/handlers/chat-completions.ts',\n      function: 'handleRequest',\n      line: ErrorContextBuilder.extractLineNumber(error),\n      requestId,\n      additional: {\n        endpoint: '/v1/chat/completions',\n        method: req.method,\n        url: req.url,\n        model: req.body?.model,\n        messageCount: req.body?.messages?.length,\n        hasTools: !!req.body?.tools,\n        streaming: req.body?.stream,\n        processingTime: Date.now() - (req as any).__startTime || Date.now(),\n        userAgent: req.headers['user-agent'],\n        contentType: req.headers['content-type']\n      }\n    };\n\n    // 如果错误已经是EnhancedRouteCodexError，直接使用\n    if (error instanceof EnhancedRouteCodexError) {\n      this.sendDetailedErrorResponse(res, error.detailedError);\n      return;\n    }\n\n    // 否则创建增强错误\n    const enhancedError = new EnhancedRouteCodexError(error, context);\n    this.sendDetailedErrorResponse(res, enhancedError.detailedError);\n  }\n\n  /**\n   * 发送详细的错误响应\n   * 符合暴露问题原则，提供完整调试信息\n   */\n  private sendDetailedErrorResponse(res: Response, detailedError: any): void {\n    try {\n      // 设置状态码和响应头\n      res.status(500).set({\n        'Content-Type': 'application/json',\n        'x-request-id': detailedError.error.context.requestId,\n        'x-error-source': detailedError.source,\n        'x-error-code': detailedError.error.code\n      });\n\n      // 发送详细的错误响应\n      res.json(detailedError);\n    } catch (responseError) {\n      // 如果连错误响应都失败了，返回最基本的错误信息\n      res.status(500).json({\n        error: {\n          code: 'CRITICAL_ERROR_RESPONSE_FAILURE',\n          message: '系统发生严重错误，无法提供详细错误信息',\n          type: 'CriticalError'\n        },\n        source: 'RouteCodex-Critical',\n        remediation: {\n          immediate: '检查服务器日志和系统状态',\n          support: '联系技术支持团队'\n        }\n      });\n    }\n  }\n\n  /**\n   * Process chat completion request\n   */\n  private async processChatRequest(req: Request, requestId: string): Promise<any> {\n    // Use pipeline manager only; no fallback allowed\n    if (this.shouldUsePipeline() && this.getRoutePools()) {\n      return await this.processWithPipeline(req, requestId);\n    }\n    throw new RouteCodexError('Chat pipeline unavailable (no route pools or pipeline manager)', 'pipeline_unavailable', 503);\n  }\n\n  /**\n   * Process request through pipeline\n   */\n  private async processWithPipeline(req: Request, requestId: string): Promise<any> {\n    const routeName = await this.decideRouteCategoryAsync(req);\n    const pipelineId = this.pickPipelineId(routeName);\n    const routeMeta = this.getRouteMeta();\n    const meta = routeMeta ? routeMeta[pipelineId] : undefined;\n    const providerId = meta?.providerId ?? 'unknown';\n    const modelId = meta?.modelId ?? 'unknown';\n\n    // Pre-convert OpenAI Responses payload to OpenAI Chat payload\n    let chatPayload = { ...(req.body || {}) } as Record<string, unknown>;\n    // 不在入口做跨路径的内容过滤；路径隔离在转换路由与编码器中保证。\n    // 角色分布快照（llmswitch 前/后），便于诊断“tool 历史泄露”等问题\n    const snapshotDir = (() => {\n      try {\n        const os = require('os'); const path = require('path'); const fs = require('fs');\n        const dir = path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat');\n        fs.mkdirSync(dir, { recursive: true });\n        return dir;\n      } catch { return null; }\n    })();\n    const roleCounts = (msgs: any[]): Record<string, number> => {\n      const c: Record<string, number> = { system: 0, user: 0, assistant: 0, tool: 0, unknown: 0 };\n      for (const m of Array.isArray(msgs) ? msgs : []) {\n        const r = String(m?.role || '').toLowerCase();\n        if (r === 'system' || r === 'user' || r === 'assistant' || r === 'tool') {c[r] += 1;} else {c.unknown += 1;}\n      }\n      return c;\n    };\n    const writeSnapshot = (kind: 'pre-llmswitch' | 'post-llmswitch', payload: any) => {\n      try {\n        if (!snapshotDir) {return;}\n        const fs = require('fs'); const path = require('path');\n        const file = path.join(snapshotDir, `${requestId}_${kind}.json`);\n        const msgs = Array.isArray(payload?.messages) ? payload.messages : [];\n        const counts = roleCounts(msgs);\n        const tc = msgs.filter((m: any) => m && m.role === 'assistant' && Array.isArray(m.tool_calls) && m.tool_calls.length);\n        const ctSummary = (() => {\n          const sum: Record<string, number> = {};\n          for (const m of tc) {\n            const t = (m.content === null) ? 'null' : (typeof m.content);\n            sum[t] = (sum[t] || 0) + 1;\n          }\n          return sum;\n        })();\n        const stats = { kind, counts, total: msgs.length, assistant_tool_calls: tc.length, content_types: ctSummary };\n        fs.writeFileSync(file, JSON.stringify({ requestId, stats }, null, 2), 'utf-8');\n      } catch { /* ignore */ }\n    };\n    try { writeSnapshot('pre-llmswitch', chatPayload); } catch { /* ignore */ }\n    // 不在入口执行 legacy OpenAI normalizer；统一由 pipeline conversion-router + canonicalizer 处理\n    chatPayload = { ...(req.body || {}) } as Record<string, unknown>;\n    try { writeSnapshot('post-llmswitch', chatPayload); } catch { /* ignore */ }\n\n    // Ensure payload model aligns with selected route meta\n    const normalizedPayload = { ...chatPayload, ...(modelId ? { model: modelId } : {}) } as Record<string, unknown>;\n    // Inject route.requestId into payload metadata for downstream providers (capture + pairing)\n    try {\n      const meta = (normalizedPayload as any)._metadata && typeof (normalizedPayload as any)._metadata === 'object'\n        ? { ...(normalizedPayload as any)._metadata }\n        : {};\n      (normalizedPayload as any)._metadata = { ...meta, requestId };\n    } catch { /* non-blocking */ }\n\n    const pipelineRequest = {\n      data: normalizedPayload,\n      route: {\n        providerId,\n        modelId,\n        requestId,\n        timestamp: Date.now(),\n        pipelineId,\n      },\n      metadata: {\n        method: req.method,\n        url: req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        targetProtocol: 'openai',\n        endpoint: `${req.baseUrl || ''}${req.url || ''}`,\n        entryEndpoint: '/v1/chat/completions',\n      },\n      debug: {\n        enabled: this.config.enableMetrics ?? true,\n        stages: {\n          llmSwitch: true,\n          workflow: true,\n          compatibility: true,\n          provider: true,\n        },\n      },\n    };\n\n    const pipelineTimeoutMs = Number(\n      process.env.ROUTECODEX_TIMEOUT_MS ||\n      process.env.RCC_TIMEOUT_MS ||\n      process.env.ROUTECODEX_PIPELINE_MAX_WAIT_MS ||\n      300000\n    );\n    const pipelineResponse = await Promise.race([\n      this.getPipelineManager()?.processRequest?.(pipelineRequest) || Promise.reject(new Error('Pipeline manager not available')),\n      new Promise((_, reject) => setTimeout(() => reject(new Error(`Pipeline timeout after ${pipelineTimeoutMs}ms`)), Math.max(1, pipelineTimeoutMs)))\n    ]);\n\n    return pipelineResponse;\n  }\n\n}\n"
        },
        "src/server/handlers/completions.ts": {
          "path": "src/server/handlers/completions.ts",
          "size": 7339,
          "lines": 202,
          "imports": [
            "express",
            "./base-handler.js",
            "../types.js",
            "../types.js",
            "../utils/request-validator.js",
            "../utils/response-normalizer.js",
            "../utils/streaming-manager.js",
            "../protocol/protocol-detector.js",
            "../protocol/openai-adapter.js"
          ],
          "exports": [
            "CompletionsHandler"
          ],
          "classes": [
            "CompletionsHandler"
          ],
          "functions": [],
          "content": "/**\n * Completions Handler Implementation\n * Handles OpenAI-compatible text completion requests\n */\n\nimport { type Request, type Response } from 'express';\nimport { BaseHandler, type ProtocolHandlerConfig } from './base-handler.js';\nimport { type RequestContext, type OpenAICompletionRequest } from '../types.js';\nimport { RouteCodexError } from '../types.js';\nimport { RequestValidator } from '../utils/request-validator.js';\nimport { ResponseNormalizer } from '../utils/response-normalizer.js';\nimport { StreamingManager } from '../utils/streaming-manager.js';\nimport { ProtocolDetector } from '../protocol/protocol-detector.js';\nimport { OpenAIAdapter } from '../protocol/openai-adapter.js';\n\n/**\n * Completions Handler\n * Handles /v1/completions endpoint\n */\nexport class CompletionsHandler extends BaseHandler {\n  private requestValidator: RequestValidator;\n  private responseNormalizer: ResponseNormalizer;\n  private streamingManager: StreamingManager;\n\n  constructor(config: ProtocolHandlerConfig) {\n    super(config);\n    this.requestValidator = new RequestValidator();\n    this.responseNormalizer = new ResponseNormalizer();\n    this.streamingManager = new StreamingManager(config);\n  }\n\n  /**\n   * Handle completions request\n   */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = this.generateRequestId();\n\n    this.logger.logModule(this.constructor.name, 'request_start', {\n      requestId,\n      model: req.body.model,\n      prompt: req.body.prompt ? (typeof req.body.prompt === 'string' ? req.body.prompt.substring(0, 100) : 'Array') : null,\n      streaming: req.body.stream || false,\n      timestamp: startTime,\n    });\n\n    try {\n      // Forced adapter preflight: convert Anthropic/Responses-shaped payloads to OpenAI\n      try {\n        const looksAnthropicContent = Array.isArray(req.body?.messages) && (req.body.messages as any[]).some((m: any) => Array.isArray(m?.content) && m.content.some((c: any) => c && typeof c === 'object' && c.type));\n        const detector = new ProtocolDetector();\n        const det = detector.detectFromRequest(req);\n        if (looksAnthropicContent || det.protocol === 'anthropic' || det.protocol === 'responses') {\n          const adapter = new OpenAIAdapter();\n          req.body = adapter.convertFromProtocol(req.body, 'anthropic') as any;\n          try { res.setHeader('x-rc-adapter', 'anthropic->openai'); } catch { /* ignore */ }\n        }\n      } catch { /* non-blocking */ }\n\n      // Validate request\n      const validation = this.requestValidator.validateCompletion(req.body);\n      if (!validation.isValid) {\n        throw new RouteCodexError(\n          `Request validation failed: ${validation.errors.join(', ')}`,\n          'validation_error',\n          400\n        );\n      }\n\n      // Process request through pipeline\n      const pipelineResponse = await this.processCompletionRequest(req, requestId);\n\n      // Handle streaming vs non-streaming response\n      if (req.body.stream) {\n        const streamModel = (pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse)\n          ? ((pipelineResponse as any).data?.model ?? req.body.model)\n          : req.body.model;\n        await this.streamingManager.streamResponse(pipelineResponse, requestId, res, streamModel);\n        return;\n      }\n\n      // Return JSON response\n      const payload = pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse\n        ? (pipelineResponse as Record<string, unknown>).data\n        : pipelineResponse;\n      const normalized = this.responseNormalizer.normalizeOpenAIResponse(payload, 'completion');\n      this.sendJsonResponse(res, normalized, requestId);\n\n      this.logCompletion(requestId, startTime, true);\n    } catch (error) {\n      this.logCompletion(requestId, startTime, false);\n      await this.handleError(error as Error, res, requestId);\n    }\n  }\n\n  /**\n   * Process completion request\n   */\n  private async processCompletionRequest(req: Request, requestId: string): Promise<any> {\n    if (this.config.enablePipeline && this.shouldUsePipeline()) {\n      return await this.processWithPipeline(req, requestId);\n    }\n\n    throw new RouteCodexError('Completions pipeline unavailable', 'pipeline_unavailable', 503);\n  }\n\n  /**\n   * Process request through pipeline\n   */\n  private async processWithPipeline(req: Request, requestId: string): Promise<any> {\n    // Similar to ChatCompletionsHandler but for completion format\n    const pipelineRequest = {\n      data: {\n        model: req.body.model,\n        prompt: req.body.prompt,\n        max_tokens: req.body.max_tokens,\n        temperature: req.body.temperature,\n        top_p: req.body.top_p,\n        frequency_penalty: req.body.frequency_penalty,\n        presence_penalty: req.body.presence_penalty,\n        stop: req.body.stop,\n        stream: req.body.stream,\n        logprobs: req.body.logprobs,\n        echo: req.body.echo,\n        n: req.body.n,\n        best_of: req.body.best_of,\n        logit_bias: req.body.logit_bias,\n        suffix: req.body.suffix,\n      },\n      route: {\n        providerId: 'default',\n        modelId: req.body.model || 'unknown',\n        requestId,\n        timestamp: Date.now(),\n        pipelineId: 'completion',\n      },\n      metadata: {\n        method: req.method,\n        url: req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        targetProtocol: 'openai',\n        endpoint: `${req.baseUrl || ''}${req.url || ''}`,\n      },\n      debug: {\n        enabled: this.config.enableMetrics ?? true,\n        stages: {\n          llmSwitch: true,\n          workflow: true,\n          compatibility: true,\n          provider: true,\n        },\n      },\n    };\n\n    const pipelineTimeoutMs = Number(process.env.ROUTECODEX_PIPELINE_MAX_WAIT_MS || 300000);\n    const pipelineResponse = await Promise.race([\n      this.executePipelineRequest(pipelineRequest),\n      new Promise((_, reject) =>\n        setTimeout(() => reject(new Error(`Pipeline timeout after ${pipelineTimeoutMs}ms`)), Math.max(1, pipelineTimeoutMs))\n      )\n    ]);\n\n    return pipelineResponse;\n  }\n\n  /**\n   * Execute pipeline request\n   */\n  private async executePipelineRequest(request: any): Promise<any> {\n    try {\n      const pipelineManager = this.getPipelineManager();\n\n      if (!pipelineManager || typeof pipelineManager.processRequest !== 'function') {\n        throw new RouteCodexError('Pipeline manager not available', 'pipeline_unavailable', 503);\n      }\n\n      // Align requested model with selected route meta if available\n      if (this.getRouteMeta) {\n        try {\n          const routeMeta = this.getRouteMeta();\n          const pid = request?.route?.pipelineId as string;\n          const meta = pid && routeMeta ? routeMeta[pid] : undefined;\n          if (meta?.modelId) {\n            request.data = { ...(request.data || {}), model: meta.modelId };\n          }\n        } catch { /* ignore */ }\n      }\n      return await pipelineManager.processRequest(request);\n    } catch (error) {\n      this.logError(error, { context: 'executePipelineRequest' });\n      throw error;\n    }\n  }\n\n  /**\n   * Check if pipeline should be used\n   */\n  protected override shouldUsePipeline(): boolean {\n    return super.shouldUsePipeline() && typeof process !== 'undefined';\n  }\n}\n"
        },
        "src/server/handlers/embeddings.ts": {
          "path": "src/server/handlers/embeddings.ts",
          "size": 6930,
          "lines": 205,
          "imports": [
            "express",
            "./base-handler.js",
            "../types.js",
            "../utils/request-validator.js",
            "../protocol/protocol-detector.js",
            "../protocol/openai-adapter.js"
          ],
          "exports": [
            "EmbeddingData",
            "EmbeddingResponse",
            "EmbeddingsHandler"
          ],
          "classes": [
            "EmbeddingsHandler"
          ],
          "functions": [],
          "content": "/**\n * Embeddings Handler Implementation\n * Handles embedding generation requests for all protocols\n */\n\nimport { type Request, type Response } from 'express';\nimport { BaseHandler, type ProtocolHandlerConfig } from './base-handler.js';\nimport { RouteCodexError } from '../types.js';\nimport { RequestValidator } from '../utils/request-validator.js';\nimport { ProtocolDetector } from '../protocol/protocol-detector.js';\nimport { OpenAIAdapter } from '../protocol/openai-adapter.js';\n\n/**\n * Embedding data interface\n */\nexport interface EmbeddingData {\n  object: string;\n  embedding: number[];\n  index: number;\n}\n\n/**\n * Embedding response interface\n */\nexport interface EmbeddingResponse {\n  object: string;\n  data: EmbeddingData[];\n  model: string;\n  usage: {\n    prompt_tokens: number;\n    total_tokens: number;\n  };\n}\n\n/**\n * Embeddings Handler\n * Handles /v1/embeddings endpoint for all protocol compatibility\n */\nexport class EmbeddingsHandler extends BaseHandler {\n  private requestValidator: RequestValidator;\n\n  constructor(config: ProtocolHandlerConfig) {\n    super(config);\n    this.requestValidator = new RequestValidator();\n  }\n\n  /**\n   * Handle embeddings request\n   */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = this.generateRequestId();\n\n    this.logger.logModule(this.constructor.name, 'request_start', {\n      requestId,\n      model: req.body.model,\n      inputType: Array.isArray(req.body.input) ? 'array' : 'string',\n      inputLength: Array.isArray(req.body.input) ? req.body.input.length : (req.body.input?.length || 0),\n      dimensions: req.body.dimensions,\n      timestamp: startTime,\n    });\n\n    try {\n      // Forced adapter preflight: if payload is Anthropic/Responses-shaped, normalize to OpenAI embeddings-compatible\n      try {\n        const looksAnthropicContent = Array.isArray(req.body?.messages) && (req.body.messages as any[]).some((m: any) => Array.isArray(m?.content) && m.content.some((c: any) => c && typeof c === 'object' && c.type));\n        const detector = new ProtocolDetector();\n        const det = detector.detectFromRequest(req);\n        if (looksAnthropicContent || det.protocol === 'anthropic' || det.protocol === 'responses') {\n          const adapter = new OpenAIAdapter();\n          req.body = adapter.convertFromProtocol(req.body, 'anthropic') as any;\n          try { res.setHeader('x-rc-adapter', 'anthropic->openai'); } catch { /* ignore */ }\n        }\n      } catch { /* non-blocking */ }\n\n      // Validate request\n      const validation = this.requestValidator.validateEmbedding(req.body);\n      if (!validation.isValid) {\n        throw new RouteCodexError(\n          `Request validation failed: ${validation.errors.join(', ')}`,\n          'validation_error',\n          400\n        );\n      }\n\n      // Process request\n      const pipelineResponse = await this.processEmbeddingsRequest(req, requestId);\n\n      // Return JSON response\n      const payload = pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse\n        ? (pipelineResponse as Record<string, unknown>).data\n        : pipelineResponse;\n      const normalized = this.normalizeEmbeddingResponse(payload);\n      this.sendJsonResponse(res, normalized, requestId);\n\n      this.logCompletion(requestId, startTime, true);\n    } catch (error) {\n      this.logCompletion(requestId, startTime, false);\n      await this.handleError(error as Error, res, requestId);\n    }\n  }\n\n  /**\n   * Process embeddings request\n   */\n  private async processEmbeddingsRequest(req: Request, requestId: string): Promise<any> {\n    if (this.shouldUsePipeline() && this.getRoutePools()) {\n      return await this.processWithPipeline(req, requestId);\n    }\n\n    throw new RouteCodexError('Embeddings pipeline unavailable', 'pipeline_unavailable', 503);\n  }\n\n  /**\n   * Process request through pipeline\n   */\n  private async processWithPipeline(req: Request, requestId: string): Promise<any> {\n    const routeName = await this.decideRouteCategoryAsync(req, '/v1/embeddings');\n    const pipelineId = this.pickPipelineId(routeName);\n    const routeMeta = this.getRouteMeta();\n    const meta = routeMeta ? routeMeta[pipelineId] : undefined;\n    const providerId = meta?.providerId ?? 'unknown';\n    const modelId = meta?.modelId ?? 'unknown';\n\n    const pipelineRequest = {\n      data: {\n        model: req.body.model,\n        input: req.body.input,\n        dimensions: req.body.dimensions,\n        encoding_format: req.body.encoding_format,\n        user: req.body.user,\n      },\n      route: {\n        providerId,\n        modelId,\n        requestId,\n        timestamp: Date.now(),\n        pipelineId,\n      },\n      metadata: {\n        method: req.method,\n        url: req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        targetProtocol: 'openai',\n        endpoint: `${req.baseUrl || ''}${req.url || ''}`,\n      },\n      debug: {\n        enabled: this.config.enableMetrics ?? true,\n        stages: {\n          llmSwitch: true,\n          workflow: true,\n          compatibility: true,\n          provider: true,\n        },\n      },\n    };\n\n    const pipelineTimeoutMs = Number(process.env.ROUTECODEX_PIPELINE_MAX_WAIT_MS || 300000);\n    const pipelineResponse = await Promise.race([\n      this.getPipelineManager()?.processRequest?.(pipelineRequest) || Promise.reject(new Error('Pipeline manager not available')),\n      new Promise((_, reject) => setTimeout(() => reject(new Error(`Pipeline timeout after ${pipelineTimeoutMs}ms`)), Math.max(1, pipelineTimeoutMs)))\n    ]);\n\n    // Align returned payload's model with route meta if set\n    if (\n      modelId &&\n      pipelineResponse &&\n      typeof pipelineResponse === 'object' &&\n      'data' in pipelineResponse &&\n      (pipelineResponse as any).data &&\n      typeof (pipelineResponse as any).data === 'object'\n    ) {\n      try { (pipelineResponse as any).data.model = modelId; } catch { /* ignore */ }\n    }\n    return pipelineResponse;\n  }\n\n  private normalizeEmbeddingResponse(response: any): EmbeddingResponse {\n    if (response && typeof response === 'object') {\n      const data = Array.isArray((response as any).data) ? (response as any).data : [];\n      const usage = (response as any).usage || {};\n      if (data.length) {\n        return {\n          object: (response as any).object ?? 'list',\n          data,\n          model: (response as any).model ?? 'unknown',\n          usage: {\n            prompt_tokens: usage.prompt_tokens ?? usage.input_tokens ?? 0,\n            total_tokens: usage.total_tokens ?? (usage.prompt_tokens ?? usage.input_tokens ?? 0)\n          }\n        };\n      }\n    }\n\n    throw new RouteCodexError('Invalid embedding response payload', 'invalid_pipeline_response', 502);\n  }\n\n  /**\n   * Check if pipeline should be used\n   */\n  protected override shouldUsePipeline(): boolean {\n    return super.shouldUsePipeline();\n  }\n}\n"
        },
        "src/server/handlers/messages.ts": {
          "path": "src/server/handlers/messages.ts",
          "size": 7499,
          "lines": 184,
          "imports": [
            "express",
            "./base-handler.js",
            "../types.js",
            "../utils/request-validator.js",
            "../utils/response-normalizer.js",
            "../utils/streaming-manager.js",
            "../protocol/protocol-detector.js",
            "../protocol/anthropic-adapter.js"
          ],
          "exports": [
            "MessagesHandler"
          ],
          "classes": [
            "MessagesHandler"
          ],
          "functions": [],
          "content": "/**\n * Messages Handler Implementation\n * Handles Anthropic-compatible messages requests\n */\n\nimport { type Request, type Response } from 'express';\nimport { BaseHandler, type ProtocolHandlerConfig } from './base-handler.js';\nimport { RouteCodexError } from '../types.js';\nimport { RequestValidator } from '../utils/request-validator.js';\nimport { ResponseNormalizer } from '../utils/response-normalizer.js';\nimport { StreamingManager } from '../utils/streaming-manager.js';\nimport { ProtocolDetector } from '../protocol/protocol-detector.js';\nimport { AnthropicAdapter } from '../protocol/anthropic-adapter.js';\n\n/**\n * Messages Handler\n * Handles /v1/messages endpoint for Anthropic compatibility\n */\nexport class MessagesHandler extends BaseHandler {\n  private requestValidator: RequestValidator;\n  private responseNormalizer: ResponseNormalizer;\n  private streamingManager: StreamingManager;\n\n  constructor(config: ProtocolHandlerConfig) {\n    super(config);\n    this.requestValidator = new RequestValidator();\n    this.responseNormalizer = new ResponseNormalizer();\n    this.streamingManager = new StreamingManager(config);\n  }\n\n  /**\n   * Handle messages request\n   */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = this.generateRequestId();\n\n    this.logger.logModule('MessagesHandler', 'request_start', {\n      requestId,\n      model: req.body.model,\n      messageCount: req.body.messages?.length || 0,\n      maxTokens: req.body.max_tokens,\n      streaming: req.body.stream || false,\n      tools: !!req.body.tools,\n      timestamp: startTime,\n    });\n\n    try {\n      // Forced adapter preflight: convert OpenAI-shaped payloads to Anthropic\n      try {\n        const looksOpenAI = Array.isArray(req.body?.messages) && (req.body.messages as any[]).some((m: any) => typeof m?.content === 'string');\n        const detector = new ProtocolDetector();\n        const det = detector.detectFromRequest(req);\n        if (looksOpenAI || det.protocol === 'openai') {\n          const adapter = new AnthropicAdapter();\n          req.body = adapter.convertFromProtocol(req.body, 'openai') as any;\n          try { res.setHeader('x-rc-adapter', 'openai->anthropic'); } catch { /* ignore */ }\n        }\n      } catch { /* non-blocking */ }\n\n      // 对齐 OpenAI 通路：跳过严格校验，保留原始入参，避免丢参/误拒\n      // 如需启用严格校验，设置 RCC_ENABLE_MSG_VALIDATION=1\n      try {\n        const enableValidation = String(process.env.RCC_ENABLE_MSG_VALIDATION || '').trim() === '1';\n        if (enableValidation) {\n          const validation = this.requestValidator.validateMessages(req.body);\n          if (!validation.isValid) {\n            throw new RouteCodexError(\n              `Request validation failed: ${validation.errors.join(', ')}`,\n              'validation_error',\n              400\n            );\n          }\n        }\n      } catch { /* keep going on validator errors */ }\n\n      // Process request\n      const pipelineResponse = await this.processMessagesRequest(req, requestId);\n\n      // Handle streaming vs non-streaming response\n      if (req.body.stream) {\n        const streamModel = (pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse)\n          ? ((pipelineResponse as any).data?.model ?? req.body.model)\n          : req.body.model;\n\n        // 默认开启真流式：未设置时视为启用；仅当显式为 '0' 或 'false' 时关闭\n        // 与 OpenAI 通路保持一致：默认不用特定 O2A 流转换，除非显式开启\n        const _flag = process.env.RCC_O2A_STREAM;\n        const useO2AStream = (_flag === '1') || (_flag?.toLowerCase?.() === 'true');\n        try {\n          // Prefer true streaming conversion when upstream is a Readable and RCC_O2A_STREAM=1\n          const topReadable = pipelineResponse && typeof (pipelineResponse as any).pipe === 'function' ? (pipelineResponse as any) : null;\n          const nestedReadable = (!topReadable && pipelineResponse && typeof (pipelineResponse as any).data?.pipe === 'function') ? (pipelineResponse as any).data : null;\n          const readable = topReadable || nestedReadable;\n          if (useO2AStream && readable) {\n            const core = await import('rcc-llmswitch-core/conversion');\n            const windowMs = Number(process.env.RCC_O2A_COALESCE_MS || 1000) || 1000;\n            await (core as any).transformOpenAIStreamToAnthropic(readable, res, { requestId, model: streamModel, windowMs, useEventHeaders: true });\n            return;\n          }\n        } catch {\n          // fall through to generic streamer on failure\n        }\n\n        await this.streamingManager.streamAnthropicResponse(pipelineResponse, requestId, res, streamModel);\n        return;\n      }\n\n      // Return JSON response\n      const payload = pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse\n        ? (pipelineResponse as Record<string, unknown>).data\n        : pipelineResponse;\n      const normalized = this.responseNormalizer.normalizeAnthropicResponse(payload);\n      this.sendJsonResponse(res, normalized, requestId);\n\n      this.logCompletion(requestId, startTime, true);\n    } catch (error) {\n      this.logCompletion(requestId, startTime, false);\n      await this.handleError(error as Error, res, requestId);\n    }\n  }\n\n  /**\n   * Process messages request\n   */\n  private async processMessagesRequest(req: Request, requestId: string): Promise<any> {\n    if (this.shouldUsePipeline() && this.getRoutePools()) {\n      return await this.processWithPipeline(req, requestId);\n    }\n\n    throw new RouteCodexError('Messages pipeline unavailable', 'pipeline_unavailable', 503);\n  }\n\n  /**\n   * Process request through pipeline\n   */\n  private async processWithPipeline(req: Request, requestId: string): Promise<any> {\n    const routeName = await this.decideRouteCategoryAsync(req, '/v1/messages');\n    const pipelineId = this.pickPipelineId(routeName);\n    const routeMeta = this.getRouteMeta();\n    const meta = routeMeta ? routeMeta[pipelineId] : undefined;\n    const providerId = meta?.providerId ?? 'unknown';\n    const modelId = meta?.modelId ?? 'unknown';\n\n    // Convert Anthropic messages to standard format\n    const payload = { ...(req.body || {}), ...(modelId ? { model: modelId } : {}) };\n    const pipelineRequest = {\n      data: payload,\n      route: {\n        providerId,\n        modelId,\n        requestId,\n        timestamp: Date.now(),\n        pipelineId,\n      },\n      metadata: {\n        method: req.method,\n        url: req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        targetProtocol: 'anthropic',\n        endpoint: `${req.baseUrl || ''}${req.url || ''}`,\n        entryEndpoint: '/v1/messages',\n      },\n      debug: {\n        enabled: this.config.enableMetrics ?? true,\n        stages: {\n          llmSwitch: true,\n          workflow: true,\n          compatibility: true,\n          provider: true,\n        },\n      },\n    };\n\n    const pipelineTimeoutMs = Number(process.env.ROUTECODEX_PIPELINE_MAX_WAIT_MS || 300000);\n    const pipelineResponse = await Promise.race([\n      this.getPipelineManager()?.processRequest?.(pipelineRequest) || Promise.reject(new Error('Pipeline manager not available')),\n      new Promise((_, reject) => setTimeout(() => reject(new Error(`Pipeline timeout after ${pipelineTimeoutMs}ms`)), Math.max(1, pipelineTimeoutMs)))\n    ]);\n\n    return pipelineResponse;\n  }\n}\n"
        },
        "src/server/handlers/models.ts": {
          "path": "src/server/handlers/models.ts",
          "size": 10405,
          "lines": 412,
          "imports": [
            "express",
            "./base-handler.js",
            "../types.js"
          ],
          "exports": [
            "ModelInfo",
            "ModelsHandler"
          ],
          "classes": [
            "ModelsHandler"
          ],
          "functions": [],
          "content": "/**\n * Models Handler Implementation\n * Handles model listing requests for all protocols\n */\n\nimport { type Request, type Response } from 'express';\nimport { BaseHandler, type ProtocolHandlerConfig } from './base-handler.js';\nimport { RouteCodexError } from '../types.js';\n\n/**\n * Model information interface\n */\nexport interface ModelInfo {\n  id: string;\n  object: string;\n  created: number;\n  owned_by: string;\n  permission?: any[];\n  root?: string;\n  parent?: string;\n  capabilities?: {\n    text_completion?: boolean;\n    chat_completion?: boolean;\n    embedding?: boolean;\n    image_generation?: boolean;\n    vision?: boolean;\n    tool_calling?: boolean;\n    streaming?: boolean;\n  };\n  pricing?: {\n    prompt: number;\n    completion: number;\n    currency?: string;\n  };\n  context_window?: number;\n  max_output_tokens?: number;\n  training_cutoff?: string;\n}\n\n/**\n * Models Handler\n * Handles /v1/models endpoint for all protocol compatibility\n */\nexport class ModelsHandler extends BaseHandler {\n  private availableModels: ModelInfo[];\n\n  constructor(config: ProtocolHandlerConfig, models?: ModelInfo[]) {\n    super(config);\n    this.availableModels = models || this.getDefaultModels();\n  }\n\n  /**\n   * Handle models request\n   */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = this.generateRequestId();\n\n    this.logger.logModule('ModelsHandler', 'request_start', {\n      requestId,\n      method: req.method,\n      url: req.url,\n      timestamp: startTime,\n    });\n\n    try {\n      // Validate request\n      // Process request\n      const response = await this.processModelsRequest(req, requestId);\n\n      // Return JSON response\n      this.sendJsonResponse(res, response, requestId);\n\n      this.logCompletion(requestId, startTime, true);\n    } catch (error) {\n      this.logCompletion(requestId, startTime, false);\n      await this.handleError(error as Error, res, requestId);\n    }\n  }\n\n  /**\n   * Process models request\n   */\n  private async processModelsRequest(req: Request, requestId: string): Promise<{ object: string; data: ModelInfo[] } | ModelInfo> {\n    // Check if requesting specific model\n    if (req.params.model) {\n      return this.getSpecificModel(req.params.model);\n    }\n\n    // Return all available models\n    return this.getAllModels();\n  }\n\n  /**\n   * Get specific model information\n   */\n  private getSpecificModel(modelId: string): ModelInfo {\n    const model = this.availableModels.find(m => m.id === modelId);\n\n    if (!model) {\n      throw new RouteCodexError(\n        `Model '${modelId}' not found`,\n        'model_not_found',\n        404\n      );\n    }\n\n    return model;\n  }\n\n  /**\n   * Get all available models\n   */\n  private getAllModels(): { data: ModelInfo[]; object: string } {\n    return {\n      object: 'list',\n      data: this.availableModels\n    };\n  }\n\n  /**\n   * Get default models\n   */\n  private getDefaultModels(): ModelInfo[] {\n    const now = Math.floor(Date.now() / 1000);\n\n    return [\n      // OpenAI-compatible models\n      {\n        id: 'gpt-4o',\n        object: 'model',\n        created: now,\n        owned_by: 'openai',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: true,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 128000,\n        max_output_tokens: 4096,\n        training_cutoff: '2024-06',\n      },\n      {\n        id: 'gpt-4o-mini',\n        object: 'model',\n        created: now,\n        owned_by: 'openai',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: false,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 128000,\n        max_output_tokens: 16384,\n        training_cutoff: '2024-06',\n      },\n      {\n        id: 'gpt-3.5-turbo',\n        object: 'model',\n        created: now,\n        owned_by: 'openai',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: false,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 16385,\n        max_output_tokens: 4096,\n        training_cutoff: '2024-06',\n      },\n      {\n        id: 'text-davinci-003',\n        object: 'model',\n        created: now,\n        owned_by: 'openai',\n        capabilities: {\n          text_completion: true,\n          chat_completion: false,\n          embedding: false,\n          image_generation: false,\n          vision: false,\n          tool_calling: false,\n          streaming: true,\n        },\n        context_window: 4097,\n        max_output_tokens: 4096,\n        training_cutoff: '2024-06',\n      },\n      // Anthropic-compatible models\n      {\n        id: 'claude-3-5-sonnet-20241022',\n        object: 'model',\n        created: now,\n        owned_by: 'anthropic',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: true,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 200000,\n        max_output_tokens: 8192,\n        training_cutoff: '2024-04',\n      },\n      {\n        id: 'claude-3-5-haiku-20241022',\n        object: 'model',\n        created: now,\n        owned_by: 'anthropic',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: true,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 200000,\n        max_output_tokens: 8192,\n        training_cutoff: '2024-04',\n      },\n      {\n        id: 'claude-3-opus-20240229',\n        object: 'model',\n        created: now,\n        owned_by: 'anthropic',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: true,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 200000,\n        max_output_tokens: 4096,\n        training_cutoff: '2024-04',\n      },\n      // Local/Custom models\n      {\n        id: 'llama-3.1-8b-instruct',\n        object: 'model',\n        created: now,\n        owned_by: 'meta',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: false,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 128000,\n        max_output_tokens: 4096,\n        training_cutoff: '2024-06',\n      },\n      {\n        id: 'llama-3.1-70b-instruct',\n        object: 'model',\n        created: now,\n        owned_by: 'meta',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: false,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 128000,\n        max_output_tokens: 8192,\n        training_cutoff: '2024-06',\n      },\n      {\n        id: 'qwen2.5-7b-instruct',\n        object: 'model',\n        created: now,\n        owned_by: 'alibaba',\n        capabilities: {\n          text_completion: false,\n          chat_completion: true,\n          embedding: false,\n          image_generation: false,\n          vision: false,\n          tool_calling: true,\n          streaming: true,\n        },\n        context_window: 32768,\n        max_output_tokens: 8192,\n        training_cutoff: '2024-06',\n      },\n      // Embedding models\n      {\n        id: 'text-embedding-3-small',\n        object: 'model',\n        created: now,\n        owned_by: 'openai',\n        capabilities: {\n          text_completion: false,\n          chat_completion: false,\n          embedding: true,\n          image_generation: false,\n          vision: false,\n          tool_calling: false,\n          streaming: false,\n        },\n        context_window: 8191,\n        max_output_tokens: 0,\n        training_cutoff: '2024-06',\n      },\n      {\n        id: 'text-embedding-3-large',\n        object: 'model',\n        created: now,\n        owned_by: 'openai',\n        capabilities: {\n          text_completion: false,\n          chat_completion: false,\n          embedding: true,\n          image_generation: false,\n          vision: false,\n          tool_calling: false,\n          streaming: false,\n        },\n        context_window: 8191,\n        max_output_tokens: 0,\n        training_cutoff: '2024-06',\n      },\n    ];\n  }\n\n  /**\n   * Update available models\n   */\n  public updateModels(models: ModelInfo[]): void {\n    this.availableModels = models;\n    this.logger.logModule('ModelsHandler', 'models_updated', {\n      count: models.length,\n      modelIds: models.map(m => m.id),\n    });\n  }\n\n  /**\n   * Add a new model\n   */\n  public addModel(model: ModelInfo): void {\n    const existingIndex = this.availableModels.findIndex(m => m.id === model.id);\n    if (existingIndex >= 0) {\n      this.availableModels[existingIndex] = model;\n    } else {\n      this.availableModels.push(model);\n    }\n\n    this.logger.logModule('ModelsHandler', 'model_added', {\n      modelId: model.id,\n      ownedBy: model.owned_by,\n    });\n  }\n\n  /**\n   * Remove a model\n   */\n  public removeModel(modelId: string): boolean {\n    const initialLength = this.availableModels.length;\n    this.availableModels = this.availableModels.filter(m => m.id !== modelId);\n    const removed = this.availableModels.length < initialLength;\n\n    if (removed) {\n      this.logger.logModule('ModelsHandler', 'model_removed', {\n        modelId,\n      });\n    }\n\n    return removed;\n  }\n\n  /**\n   * Get models by capability\n   */\n  public getModelsByCapability(capability: keyof ModelInfo['capabilities']): ModelInfo[] {\n    return this.availableModels.filter(model =>\n      model.capabilities && model.capabilities[capability] === true\n    );\n  }\n\n  /**\n   * Get models by owner\n   */\n  public getModelsByOwner(owner: string): ModelInfo[] {\n    return this.availableModels.filter(model => model.owned_by === owner);\n  }\n}\n"
        },
        "src/server/handlers/responses.ts": {
          "path": "src/server/handlers/responses.ts",
          "size": 52634,
          "lines": 954,
          "imports": [
            "express",
            "./base-handler.js",
            "../types.js",
            "../utils/request-validator.js",
            "../utils/response-normalizer.js",
            "../utils/streaming-manager.js",
            "../config/responses-config.js",
            "../conversion/responses-converter.js",
            "../conversion/responses-mapper.js",
            "rcc-llmswitch-core/conversion/responses/responses-openai-bridge",
            "rcc-llmswitch-core/conversion/shared/args-mapping",
            "../../modules/pipeline/utils/debug-logger.js"
          ],
          "exports": [
            "ResponsesHandler"
          ],
          "classes": [
            "ResponsesHandler"
          ],
          "functions": [
            "ensureDirs",
            "capture",
            "startHeartbeat",
            "writeEvt",
            "toResponsesShape",
            "push",
            "toCleanObject",
            "now",
            "inProgressResp",
            "knownServers",
            "toArgsStr",
            "usage2",
            "mapUsage",
            "pick"
          ],
          "content": "/**\n * OpenAI Responses Handler\n * Handles OpenAI /v1/responses endpoint (stream + json)\n */\n\nimport express, { type Request, type Response } from 'express';\nimport { BaseHandler, type ProtocolHandlerConfig } from './base-handler.js';\nimport { RouteCodexError } from '../types.js';\nimport { RequestValidator } from '../utils/request-validator.js';\nimport { ResponseNormalizer } from '../utils/response-normalizer.js';\nimport { StreamingManager } from '../utils/streaming-manager.js';\nimport { ResponsesConfigUtil } from '../config/responses-config.js';\nimport { ResponsesConverter } from '../conversion/responses-converter.js';\nimport { ResponsesMapper } from '../conversion/responses-mapper.js';\nimport { buildResponsesPayloadFromChat } from 'rcc-llmswitch-core/conversion/responses/responses-openai-bridge';\nimport { normalizeTools } from 'rcc-llmswitch-core/conversion/shared/args-mapping';\n// Protocol conversion is handled downstream by llmswitch (Responses→Chat) for providers.\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\n\n/**\n * Responses Handler\n * Handles /v1/responses endpoint (OpenAI Responses)\n */\nexport class ResponsesHandler extends BaseHandler {\n  private requestValidator: RequestValidator;\n  private responseNormalizer: ResponseNormalizer;\n  private streamingManager: StreamingManager;\n\n  constructor(config: ProtocolHandlerConfig) {\n    super(config);\n    this.requestValidator = new RequestValidator();\n    this.responseNormalizer = new ResponseNormalizer();\n    this.streamingManager = new StreamingManager(config);\n  }\n\n  /**\n   * Handle responses request\n   */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const moduleConfig = await ResponsesConfigUtil.load();\n    const startTime = Date.now();\n    const requestId = this.generateRequestId();\n    // Capture original request body and headers for later diff/debug\n    try {\n      const rawBody = JSON.parse(JSON.stringify((req as any).body || {}));\n      (req as any).__rawBody = rawBody;\n      const fs = await import('fs/promises');\n      const os = await import('os');\n      const path = await import('path');\n      const dir = path.join((os as any).homedir(), '.routecodex', 'codex-samples', 'responses-replay');\n      await (fs as any).mkdir(dir, { recursive: true });\n      const rawFile = path.join(dir, `raw-request_${requestId}.json`);\n      const payload = { requestId, method: req.method, url: req.originalUrl || req.url, headers: this.sanitizeHeaders(req.headers), body: rawBody };\n      await (fs as any).writeFile(rawFile, JSON.stringify(payload, null, 2), 'utf-8');\n    } catch { /* ignore capture failures */ }\n\n    this.logger.logModule(this.constructor.name, 'request_start', {\n      requestId,\n      model: req.body.model,\n      messageCount: req.body.messages?.length || 0,\n      maxTokens: req.body.max_tokens,\n      streaming: req.body.stream || false,\n      tools: !!req.body.tools,\n      timestamp: startTime,\n    });\n\n    try {\n      try { res.setHeader('x-rc-conversion-profile', 'responses-openai'); } catch { /* ignore */ }\n      // 可选：受控的“静态”系统提示注入（默认关闭），优先写入 instructions 字段\n      // 禁止样本抓取式注入；仅当明确配置 ROUTECODEX_ENABLE_SERVER_SYSTEM_PROMPT=1 且提供\n      // ROUTECODEX_SERVER_SYSTEM_PROMPT=</path/to/prompt.txt> 时，才在 instructions 为空时写入。\n      try {\n        const enable = String(process.env.ROUTECODEX_ENABLE_SERVER_SYSTEM_PROMPT || '0') === '1';\n        const filePath = String(process.env.ROUTECODEX_SERVER_SYSTEM_PROMPT || '').trim();\n        if (enable && filePath) {\n          const fs = await import('fs/promises');\n          const sysText = await fs.readFile(filePath, 'utf-8');\n          const instr = typeof (req.body as any)?.instructions === 'string' ? String((req.body as any).instructions) : '';\n          const hasMdMarkers = /\\bCLAUDE\\.md\\b|\\bAGENT(?:S)?\\.md\\b/i.test(instr);\n          if (!hasMdMarkers && (!instr || !instr.trim())) {\n            (req.body as any).instructions = sysText;\n            try { res.setHeader('x-rc-system-prompt-source', 'static-file'); } catch { /* ignore */ }\n          }\n        }\n      } catch { /* non-blocking */ }\n\n      // 仅改变格式，不清洗/丢弃字段：基于原始请求体派生 Responses 关键字段并回写，保留全部原字段\n      try {\n        const b: any = req.body || {};\n        const merged: any = { ...b };\n        // model：如缺失则置为 unknown（不强行覆盖已有值）\n        if (typeof merged.model !== 'string' || !merged.model) {\n          merged.model = 'unknown';\n        }\n        // stream：仅当未显式给出时，按 Accept 头推断\n        if (typeof merged.stream !== 'boolean') {\n          const inferred = ResponsesConverter.inferStreamingFlag(b, req);\n          if (typeof inferred === 'boolean') {merged.stream = inferred;}\n        }\n        // 若原本没有 instructions/input，则尝试从 Chat 形状派生\n        if (typeof merged.instructions !== 'string' || merged.instructions.trim() === '') {\n          if (Array.isArray(b.messages)) {\n            const sys: string[] = [];\n            for (const m of b.messages) {\n              if (!m || typeof m !== 'object') {continue;}\n              const role = (m as any).role;\n              const content = (m as any).content;\n              if (role === 'system' && typeof content === 'string' && content.trim()) {sys.push(content.trim());}\n            }\n            if (sys.length) {merged.instructions = sys.join('\\n\\n');}\n          }\n        }\n        if (typeof merged.input === 'undefined' || (typeof merged.input === 'string' && merged.input.trim() === '')) {\n          if (Array.isArray(b.messages)) {\n            const userTexts: string[] = [];\n            for (const m of b.messages) {\n              if (!m || typeof m !== 'object') {continue;}\n              const role = (m as any).role;\n              const content = (m as any).content;\n              if (role === 'user' && typeof content === 'string' && content.trim()) {userTexts.push(content.trim());}\n              if (role === 'user' && Array.isArray(content)) {\n                for (const part of content) {\n                  if (part && typeof part === 'object' && typeof (part as any).text === 'string' && (part as any).text.trim()) {\n                    userTexts.push((part as any).text.trim());\n                  }\n                }\n              }\n            }\n            if (userTexts.length) {merged.input = userTexts.join('\\n');}\n          }\n          // Anthropic content 兼容\n          if (typeof merged.input === 'undefined' && Array.isArray(b.content)) {\n            const texts: string[] = [];\n            for (const c of b.content) {\n              if (c && typeof c === 'object' && (c.type === 'text' || c.type === 'output_text') && typeof c.text === 'string' && c.text.trim()) {\n                texts.push(c.text.trim());\n              }\n            }\n            if (texts.length) {merged.input = texts.join('\\n');}\n          }\n        }\n\n        req.body = merged; // 不移除任何原字段\n        try { res.setHeader('x-rc-adapter', 'normalized:chat|anthropic->responses:non-lossy'); } catch { /* ignore */ }\n      } catch { /* non-blocking */ }\n\n      // Tool guidance and normalization are handled in llmswitch-core (openai tooling stage)\n\n      // Remove strict request validation for Responses-shaped requests (preserve raw body)\n\n      // Preserve original normalized request before pipeline mutates it\n      const originalRequestSnapshot = JSON.parse(JSON.stringify(req.body || {}));\n\n      // Process request through pipeline\n      const response = await this.processResponseRequest(req, requestId);\n\n      // Handle streaming vs non-streaming response（保留“合成流”以兼容 Responses 客户端）\n      if (req.body.stream) {\n        // Prefer true streaming bridge when upstream returns a Readable and RCC_R2C_STREAM is enabled (default on)\n        try {\n          const _flag = process.env.RCC_R2C_STREAM;\n          const useBridge = (_flag === undefined) || _flag === '1' || (_flag?.toLowerCase?.() === 'true');\n          const topReadable = response && typeof (response as any).pipe === 'function' ? (response as any) : null;\n          const nestedReadable = (!topReadable && response && typeof (response as any).data?.pipe === 'function') ? (response as any).data : null;\n          const readable = topReadable || nestedReadable;\n          if (useBridge && readable) {\n            const core = await import('rcc-llmswitch-core/conversion');\n            const windowMs = Number(process.env.RCC_R2C_COALESCE_MS || 1000) || 1000;\n            await (core as any).transformOpenAIStreamToResponses(readable, res, { requestId, model: req.body.model, windowMs, tools: (req.body as any)?.tools });\n            return;\n          }\n        } catch { /* fall through to synthetic SSE */ }\n\n        await this.streamResponsesSSE(\n          response,\n          requestId,\n          res,\n          req.body.model,\n          (req.body as any)?.tools,\n          (req.body as any)?.tool_choice,\n          (req.body as any)?.parallel_tool_calls,\n          (req.body as any)?.instructions,\n          originalRequestSnapshot\n        );\n        return;\n      }\n\n      // Return JSON response in OpenAI Responses format（确定性路径，无流时只返回 JSON）\n      const normalized = await this.buildResponsesJson(\n        response,\n        {\n          tools: (req.body as any)?.tools,\n          tool_choice: (req.body as any)?.tool_choice,\n          parallel_tool_calls: (req.body as any)?.parallel_tool_calls,\n        }\n      );\n      try { res.setHeader('Content-Type', 'application/json'); } catch { /* ignore */ }\n      this.sendJsonResponse(res, normalized, requestId);\n\n      this.logCompletion(requestId, startTime, true);\n    } catch (error) {\n      this.logCompletion(requestId, startTime, false);\n      // If client requested SSE, emit SSE error and close the stream instead of JSON\n      try {\n        const accept = String(req.headers['accept'] || '').toLowerCase();\n        const wantsSSE = (accept.includes('text/event-stream') || req.body?.stream === true);\n        if (wantsSSE && !res.writableEnded) {\n          try {\n            if (!res.headersSent) {\n              res.setHeader('Content-Type', 'text/event-stream');\n              res.setHeader('Cache-Control', 'no-cache');\n              res.setHeader('Connection', 'keep-alive');\n              res.setHeader('x-request-id', requestId);\n            }\n            const errMsg = (error instanceof Error ? error.message : String(error)) || 'stream error';\n            const evt = { type: 'response.error', error: { message: errMsg, type: 'streaming_error', code: 'STREAM_FAILED' }, requestId } as Record<string, unknown>;\n            res.write(`event: response.error\\n`);\n            res.write(`data: ${JSON.stringify(evt)}\\n\\n`);\n          } catch { /* ignore */ }\n          try { res.end(); } catch { /* ignore */ }\n          return;\n        }\n      } catch { /* ignore */ }\n\n      await this.handleError(error as Error, res, requestId);\n    }\n  }\n\n  /**\n   * Process response request\n   */\n  private async processResponseRequest(req: Request, requestId: string): Promise<any> {\n    if (this.shouldUsePipeline() && this.getRoutePools()) {\n      return await this.processWithPipeline(req, requestId);\n    }\n\n    throw new RouteCodexError('Responses pipeline unavailable', 'pipeline_unavailable', 503);\n  }\n\n  /**\n   * Process request through pipeline\n   */\n  private async processWithPipeline(req: Request, requestId: string): Promise<any> {\n    const routeName = await this.decideRouteCategoryAsync(req);\n    const pipelineId = this.pickPipelineId(routeName);\n    const routeMeta = this.getRouteMeta();\n    const meta = routeMeta ? routeMeta[pipelineId] : undefined;\n    const providerId = meta?.providerId ?? 'unknown';\n    const modelId = meta?.modelId ?? 'unknown';\n\n    const pipelineRequest = {\n      data: { ...(req.body as any || {}), ...(modelId ? { model: modelId } : {}) },\n      route: {\n        providerId,\n        modelId,\n        requestId,\n        timestamp: Date.now(),\n        pipelineId,\n      },\n      metadata: {\n        method: req.method,\n        url: req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        // Send OpenAI Chat to provider after local synth; targetProtocol=openai for safety\n        targetProtocol: 'openai',\n        endpoint: `${req.baseUrl || ''}${req.url || ''}`,\n        entryEndpoint: '/v1/responses',\n      },\n      debug: {\n        enabled: this.config.enableMetrics ?? true,\n        stages: {\n          llmSwitch: true,\n          workflow: true,\n          compatibility: true,\n          provider: true,\n        },\n      },\n    } as any;\n\n    // Inject route.requestId into payload metadata for downstream providers (capture + pairing)\n    try {\n      const dataObj = pipelineRequest.data as Record<string, unknown>;\n      const meta = (dataObj as any)._metadata && typeof (dataObj as any)._metadata === 'object'\n        ? { ...(dataObj as any)._metadata }\n        : {};\n      (dataObj as any)._metadata = { ...meta, requestId };\n    } catch { /* ignore */ }\n\n    // Debug: capture final request payload before entering pipeline (to diagnose client tolerance issues)\n    try {\n      const fs = await import('fs/promises');\n      const os = await import('os');\n      const path = await import('path');\n      const dir = path.join((os as any).homedir(), '.routecodex', 'codex-samples', 'openai-responses');\n      await (fs as any).mkdir(dir, { recursive: true });\n      const file = path.join(dir, `${requestId}_pre-pipeline.json`);\n      await (fs as any).writeFile(file, JSON.stringify({ requestId, payload: pipelineRequest.data }, null, 2), 'utf-8');\n    } catch { /* ignore */ }\n\n    const pipelineTimeoutMs = Number(\n      process.env.ROUTECODEX_TIMEOUT_MS ||\n      process.env.RCC_TIMEOUT_MS ||\n      process.env.ROUTECODEX_PIPELINE_MAX_WAIT_MS ||\n      300000\n    );\n    const pipelineResponse = await Promise.race([\n      this.getPipelineManager()?.processRequest?.(pipelineRequest) || Promise.reject(new Error('Pipeline manager not available')),\n      new Promise((_, reject) => setTimeout(() => reject(new Error(`Pipeline timeout after ${pipelineTimeoutMs}ms`)), Math.max(1, pipelineTimeoutMs)))\n    ]);\n    const firstResp = (pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse)\n      ? (pipelineResponse as Record<string, unknown>).data\n      : pipelineResponse;\n\n    // Debug: persist provider response for offline analysis\n    try {\n      const fs = await import('fs/promises');\n      const os = await import('os');\n      const path = await import('path');\n      const dir = path.join((os as any).homedir(), '.routecodex', 'codex-samples', 'openai-responses');\n      await (fs as any).mkdir(dir, { recursive: true });\n      const file = path.join(dir, `${requestId}_provider-response.json`);\n      await (fs as any).writeFile(file, JSON.stringify(firstResp, null, 2), 'utf-8');\n    } catch { /* ignore */ }\n\n    // 工具调用仅由客户端执行；服务端不再尝试本地执行或发起第二轮。\n\n    return firstResp;\n  }\n\n  /**\n   * Stream SSE for Anthropic Responses endpoint with proper completion signal\n   */\n  private async streamResponsesSSE(\n    response: any,\n    requestId: string,\n    res: Response,\n    model?: string,\n    reqTools?: unknown,\n    reqToolChoice?: unknown,\n    reqParallel?: unknown,\n    reqInstructions?: unknown,\n    reqMeta?: Record<string, unknown>\n  ): Promise<void> {\n    let heartbeatTimer: NodeJS.Timeout | null = null;\n    const streamAborted = false;\n    // audit writer stub (initialized after fs paths are ready)\n    let auditWrite: (kind: 'event' | 'meta' | 'end' | 'error', payload: Record<string, unknown>) => Promise<void> = async () => Promise.resolve();\n    try {\n      const fs = await import('fs/promises');\n      const path = await import('path');\n      const home = process.env.HOME || process.env.USERPROFILE || '';\n      const baseDir = path.join(home || '', '.routecodex', 'codex-samples');\n      const subDir = path.join(baseDir, 'openai-responses');\n      const sseFile = path.join(subDir, `${requestId}_sse-events.log`);\n      const sseAuditFile = path.join(subDir, `${requestId}_sse-audit.log`);\n      const ensureDirs = async () => { try { await fs.mkdir(subDir, { recursive: true }); } catch { /* ignore */ } };\n      const capture = async (event: string, data: unknown) => {\n        try {\n          await ensureDirs();\n          const line = `${JSON.stringify({ ts: Date.now(), requestId, event, data })  }\\n`;\n          await fs.appendFile(sseFile, line, 'utf-8');\n        } catch { /* ignore */ }\n      };\n      auditWrite = async (kind: 'event' | 'meta' | 'end' | 'error', payload: Record<string, unknown>) => {\n        try {\n          await ensureDirs();\n          const now = new Date().toISOString();\n          if (kind === 'event') {\n            const ev = String((payload as any).event || 'unknown');\n            const data = (payload as any).data !== undefined ? JSON.stringify((payload as any).data) : '{}';\n            const chunk = `# ${now} requestId=${requestId}\\n` +\n                         `event: ${ev}\\n` +\n                         `data: ${data}\\n\\n`;\n            await fs.appendFile(sseAuditFile, chunk, 'utf-8');\n          } else if (kind === 'meta') {\n            const chunk = `# ${now} requestId=${requestId} META ${JSON.stringify(payload)}\\n`;\n            await fs.appendFile(sseAuditFile, chunk, 'utf-8');\n          } else if (kind === 'end') {\n            const chunk = `# ${now} requestId=${requestId} SSE_END ${JSON.stringify(payload)}\\n`;\n            await fs.appendFile(sseAuditFile, chunk, 'utf-8');\n          } else if (kind === 'error') {\n            const chunk = `# ${now} requestId=${requestId} SSE_ERROR ${JSON.stringify(payload)}\\n`;\n            await fs.appendFile(sseAuditFile, chunk, 'utf-8');\n          }\n        } catch { /* ignore */ }\n      };\n      const startHeartbeat = async () => {\n        const cfg = await ResponsesConfigUtil.load();\n        const intervalMs = cfg.sse.heartbeatMs;\n        if (!Number.isFinite(intervalMs) || intervalMs <= 0) {\n          return;\n        }\n        heartbeatTimer = setInterval(() => {\n          if (res.writableEnded) {\n            if (heartbeatTimer) {\n              clearInterval(heartbeatTimer);\n              heartbeatTimer = null;\n            }\n            return;\n          }\n          try {\n            res.write(`: heartbeat ${Date.now()}\\n\\n`);\n            void capture('heartbeat', { ts: Date.now() });\n          } catch {\n            if (heartbeatTimer) {\n              clearInterval(heartbeatTimer);\n              heartbeatTimer = null;\n            }\n          }\n        }, intervalMs);\n      };\n      // Headers\n      res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n      res.setHeader('Cache-Control', 'no-cache, no-transform');\n      res.setHeader('Connection', 'keep-alive');\n      try { res.setHeader('X-Accel-Buffering', 'no'); } catch { /* ignore */ }\n      res.setHeader('x-request-id', requestId);\n      try { (res as any).flushHeaders?.(); } catch { /* ignore */ }\n      await auditWrite('meta', { phase: 'headers', headers: {\n        'Content-Type': 'text/event-stream; charset=utf-8', 'Cache-Control': 'no-cache, no-transform', 'Connection': 'keep-alive', 'X-Accel-Buffering': 'no', 'x-request-id': requestId\n      }});\n      await startHeartbeat();\n\n      // Minimal writer (OpenAI Responses standard) with sequence_number (start at 0)\n      let seq = 0;\n      const writeEvt = async (event: string, data: Record<string, unknown>) => {\n        const payload = { ...(data || {}), sequence_number: seq++ } as Record<string, unknown>;\n        res.write(`event: ${event}\\n`);\n        res.write(`data: ${JSON.stringify(payload)}\\n\\n`);\n        try { await capture(event, payload); } catch { /* ignore */ }\n        try { await auditWrite('event', { event, data: payload }); } catch { /* ignore */ }\n      };\n\n      const requestMeta = { ...(reqMeta ?? {}) } as Record<string, unknown>;\n      // Normalize tools declaration for Responses SSE metadata (align with other routes)\n      try {\n        const rawTools = (requestMeta as any).tools;\n        if (Array.isArray(rawTools)) {\n          const nt = normalizeTools(rawTools as any[]);\n          (requestMeta as any).tools = (nt as any[]).map((t: any) => (t && t.type === 'function' && t.function && typeof t.function === 'object') ? { ...t, function: { ...t.function, strict: true } } : t);\n        }\n      } catch { /* ignore tools normalize errors */ }\n      const requestInstructions = typeof reqInstructions === 'string' ? reqInstructions : undefined;\n      if (requestInstructions && !requestMeta.instructions) {\n        requestMeta.instructions = requestInstructions;\n      }\n      if (reqTools !== undefined && requestMeta.tools === undefined) {requestMeta.tools = reqTools;}\n      if (reqToolChoice !== undefined && requestMeta.tool_choice === undefined) {requestMeta.tool_choice = reqToolChoice;}\n      if (reqParallel !== undefined && requestMeta.parallel_tool_calls === undefined) {requestMeta.parallel_tool_calls = reqParallel;}\n      // If tool-followup is present, prefer __initial for tool events and __final for text/usage\n      const rawInitial = (response && typeof response === 'object' && (response as any).__initial) ? (response as any).__initial : response;\n      const rawFinal = (response && typeof response === 'object' && (response as any).__final) ? (response as any).__final : response;\n\n      // Normalize both initial/final payloads into OpenAI Responses JSON (passthrough if already Responses)\n      const toResponsesShape = async (payload: any) => {\n        // Always use core bridge to ensure canonical sanitation (e.g., strip tool result envelopes)\n        return buildResponsesPayloadFromChat(payload, undefined) as Record<string, unknown>;\n      };\n      const initialResp = await toResponsesShape(rawInitial);\n      const finalResp = await toResponsesShape(rawFinal);\n\n      try {\n        const fs = await import('fs/promises');\n        const os = await import('os');\n        const path = await import('path');\n      const dir = path.join((os as any).homedir(), '.routecodex', 'codex-samples', 'openai-responses');\n      await (fs as any).mkdir(dir, { recursive: true });\n      await (fs as any).writeFile(path.join(dir, `${requestId}_responses-initial.json`), JSON.stringify(initialResp, null, 2), 'utf-8');\n      await (fs as any).writeFile(path.join(dir, `${requestId}_responses-final.json`), JSON.stringify(finalResp, null, 2), 'utf-8');\n      } catch { /* ignore */ }\n\n      const extractText = (resp: any): string => {\n        const texts: string[] = [];\n        const push = (s?: string) => { if (typeof s === 'string') { const t = s.trim(); if (t) {texts.push(t);} } };\n        const walkBlocks = (blocks: any[]): void => {\n          for (const b of blocks || []) {\n            if (!b || typeof b !== 'object') {continue;}\n            const t = (b as any).type;\n            if ((t === 'text' || t === 'output_text') && typeof (b as any).text === 'string') { push((b as any).text); continue; }\n            if (t === 'message' && Array.isArray((b as any).content)) { walkBlocks((b as any).content); continue; }\n            if (Array.isArray((b as any).content)) { walkBlocks((b as any).content); }\n          }\n        };\n        try {\n          if (resp && typeof resp.output_text === 'string') {push(resp.output_text);}\n          if (resp && Array.isArray(resp.output)) {walkBlocks(resp.output);}\n          if (resp && Array.isArray(resp.content)) {walkBlocks(resp.content);}\n          // Support Chat content as string or array-of-parts\n          const chat = resp?.choices?.[0]?.message?.content;\n          if (typeof chat === 'string') {\n            push(chat);\n          } else if (Array.isArray(chat)) {\n            for (const part of chat) {\n              if (part && typeof part === 'object') {\n                if (typeof (part as any).text === 'string') { push((part as any).text); }\n                else if (typeof (part as any).content === 'string') { push((part as any).content); }\n              } else if (typeof part === 'string') { push(part); }\n            }\n          }\n        } catch { /* ignore */ }\n        // Preserve paragraph/line breaks when aggregating text parts\n        return texts.join('\\n');\n      };\n\n      // Use final response text if available; fallback to initial\n      let textOut = extractText(finalResp) || extractText(initialResp);\n      // Collapse excessive consecutive newlines to at most two (paragraph break)\n      if (typeof textOut === 'string') {\n        textOut = textOut.replace(/\\n{3,}/g, '\\n\\n');\n      }\n      // Preserve newlines during streaming by splitting into newline-aware segments\n      const words = (textOut || '')\n        .split(/(\\n+)/g)\n        .map((s: string) => (/^\\n+$/.test(s) ? '\\n' : s))\n        .filter((s: string) => s.length > 0);\n\n      const toCleanObject = (obj: Record<string, unknown>) => {\n        for (const key of Object.keys(obj)) {\n          if (obj[key] === undefined) {delete obj[key];}\n        }\n        return obj;\n      };\n\n      const assembleSnapshot = (): Record<string, unknown> => {\n        const snap: Record<string, unknown> = { ...(finalResp || {}) };\n        snap.id = snap.id || finalResp?.id || `resp_${requestId}`;\n        snap.object = snap.object || 'response';\n        const createdAt = (typeof snap.created_at === 'number' ? snap.created_at : undefined)\n          ?? (typeof (snap as any).created === 'number' ? (snap as any).created : undefined)\n          ?? Math.floor(Date.now() / 1000);\n        snap.created_at = createdAt;\n        delete (snap as any).created;\n        snap.status = snap.status || 'completed';\n        if (!('background' in snap)) {snap.background = false;}\n        if (!('error' in snap)) {snap.error = null;}\n        if (!('incomplete_details' in snap)) {snap.incomplete_details = null;}\n        if (requestMeta.instructions && !snap.instructions) {snap.instructions = requestMeta.instructions;}\n        if (requestMeta.reasoning && !snap.reasoning) {snap.reasoning = requestMeta.reasoning;}\n        if (requestMeta.tool_choice && !snap.tool_choice) {snap.tool_choice = requestMeta.tool_choice;}\n        if (requestMeta.parallel_tool_calls !== undefined && !snap.parallel_tool_calls) {snap.parallel_tool_calls = requestMeta.parallel_tool_calls;}\n        if (requestMeta.tools && !snap.tools) {snap.tools = requestMeta.tools;}\n        if (requestMeta.store !== undefined && !snap.store) {snap.store = requestMeta.store;}\n        if (requestMeta.include && !snap.include) {snap.include = requestMeta.include;}\n        if (requestMeta.prompt_cache_key && !snap.prompt_cache_key) {snap.prompt_cache_key = requestMeta.prompt_cache_key;}\n        // Prefer provider-reported model; only fallback to request model when missing\n        snap.model = (snap.model as string) || (model as string) || 'unknown';\n        if (!snap.metadata) {snap.metadata = {};}\n        if (!Array.isArray(snap.output) && Array.isArray(finalResp?.output)) {snap.output = finalResp?.output;}\n        if (!('output_text' in snap)) {snap.output_text = finalResp?.output_text ?? textOut ?? '';}\n        if (finalResp?.usage && !snap.usage) {snap.usage = finalResp.usage;}\n        return toCleanObject(snap);\n      };\n\n      const baseSnapshot = assembleSnapshot();\n      const baseResp = await ResponsesMapper.enrichResponsePayload(baseSnapshot, finalResp as Record<string, unknown>, requestMeta);\n      const now = () => Math.floor(Date.now() / 1000);\n      // Text strict Azure mode removed per rollback request; keep default text path\n\n      // response.created / in_progress (OpenAI Responses shape)\n      const createdTs = now();\n      const inProgressResp = (() => {\n        const snap = { ...baseResp, status: 'in_progress' } as Record<string, unknown>;\n        delete snap.output;\n        delete snap.output_text;\n        delete snap.usage;\n        delete snap.required_action;\n        // Avoid sending giant instructions block in early SSE events to prevent client buffering/stalls\n        if (typeof (snap as any).instructions === 'string') {\n          delete (snap as any).instructions;\n        }\n        return toCleanObject(snap);\n      })();\n\n      await writeEvt('response.created', {\n        type: 'response.created',\n        response: { ...inProgressResp, created_at: createdTs }\n      });\n      await writeEvt('response.in_progress', {\n        type: 'response.in_progress',\n        response: { ...inProgressResp, created_at: createdTs }\n      });\n\n      // 如果是工具调用场景：按透传方式流 function_call 参数增量并最终发送 completed（SSE 不发送 required_action）\n      const raExisting = (initialResp as any)?.required_action || (finalResp as any)?.required_action;\n      const getFunctionCalls = (resp: any): any[] => {\n        const out = Array.isArray(resp?.output) ? resp.output : [];\n        return out.filter((it: any) => it && (it.type === 'function_call' || it.type === 'tool_call'));\n      };\n      const deriveRequiredAction = (funcCalls: any[]): Record<string, unknown> | null => {\n        if (!Array.isArray(funcCalls) || funcCalls.length === 0) {return null;}\n        const toStringArgs = (v: any): string => {\n          if (typeof v === 'string') {return v;}\n          try { return JSON.stringify(v ?? {}); } catch { return '{}'; }\n        };\n        const calls = funcCalls.map((it: any) => {\n          const id = typeof it.id === 'string' ? it.id : (typeof it.call_id === 'string' ? it.call_id : `call_${Math.random().toString(36).slice(2,8)}`);\n          const name = typeof it.name === 'string' ? it.name : (typeof it.tool_name === 'string' ? it.tool_name : (it?.function?.name || 'tool'));\n          const argsVal = it.arguments ?? it?.function?.arguments ?? {};\n          const argsStr = toStringArgs(argsVal);\n          return { id, type: 'function', function: { name: String(name), arguments: String(argsStr) } };\n        });\n        return { type: 'submit_tool_outputs', submit_tool_outputs: { tool_calls: calls } } as Record<string, unknown>;\n      };\n      // 主动“按优先级”提取工具调用：\n      // 1) final.output 中的 function_call/tool_call\n      // 2) initial.output 中的 function_call/tool_call\n      // 3) final.required_action.submit_tool_outputs.tool_calls\n      // 4) initial.required_action.submit_tool_outputs.tool_calls\n      const getFuncCallsFromRequiredAction = (resp: any): any[] => {\n        try {\n          const ra = resp?.required_action;\n          const arr = (ra && ra.type === 'submit_tool_outputs') ? (ra.submit_tool_outputs?.tool_calls || []) : [];\n          if (!Array.isArray(arr) || !arr.length) {return [];}\n          return arr.map((c: any) => ({\n            type: 'function_call',\n            id: c?.id,\n            call_id: c?.id || c?.call_id,\n            name: c?.name || c?.function?.name,\n            arguments: c?.arguments || c?.function?.arguments || '{}',\n            status: 'in_progress'\n          }));\n        } catch { return []; }\n      };\n      const pickFuncCalls = (): any[] => {\n        const f1 = getFunctionCalls(finalResp); if (Array.isArray(f1) && f1.length) {return f1;}\n        const f2 = getFunctionCalls(initialResp); if (Array.isArray(f2) && f2.length) {return f2;}\n        const f3 = getFuncCallsFromRequiredAction(finalResp); if (Array.isArray(f3) && f3.length) {return f3;}\n        const f4 = getFuncCallsFromRequiredAction(initialResp); if (Array.isArray(f4) && f4.length) {return f4;}\n        return [];\n      };\n      let funcCalls = pickFuncCalls();\n      // 过滤无效函数调用（缺少 name 或 name 为 'tool'），避免发出不可执行的 function_call 事件\n      funcCalls = Array.isArray(funcCalls) ? funcCalls.filter((it: any) => {\n        const nm = (typeof it?.name === 'string' ? it.name : (typeof it?.function?.name === 'string' ? it.function.name : '')).trim();\n        return nm.length > 0 && nm.toLowerCase() !== 'tool';\n      }) : [];\n\n      // Build known MCP servers (env + discovered from original request snapshot)\n      const knownServers = (() => {\n        const set = new Set<string>();\n        try {\n          const serversRaw = String(process.env.ROUTECODEX_MCP_SERVERS || '').trim();\n          if (serversRaw) { for (const s of serversRaw.split(',').map(x => x.trim()).filter(Boolean)) {set.add(s);} }\n          const snap = (reqMeta && typeof reqMeta === 'object') ? reqMeta : {};\n          const messages = Array.isArray((snap as any).messages) ? ((snap as any).messages as any[]) : [];\n          for (const m of messages) {\n            try {\n              if (m && m.role === 'tool' && typeof m.content === 'string') {\n                const o = JSON.parse(m.content); const sv = o?.arguments?.server; if (typeof sv === 'string' && sv.trim()) {set.add(sv.trim());}\n              }\n              if (m && m.role === 'assistant' && Array.isArray(m.tool_calls)) {\n                for (const tc of m.tool_calls) {\n                  const argStr = String(tc?.function?.arguments ?? '');\n                  try { const parsed = JSON.parse(argStr); const sv = parsed?.server; if (typeof sv === 'string' && sv.trim()) {set.add(sv.trim());} } catch {}\n                }\n              }\n            } catch { /* ignore */ }\n          }\n        } catch { /* ignore */ }\n        return set;\n      })();\n      if (Array.isArray(funcCalls) && funcCalls.length > 0) {\n        // Stream function_call lifecycle and arguments deltas\n        const toArgsStr = (v: any) => (typeof v === 'string' ? v : (() => { try { return JSON.stringify(v ?? {}); } catch { return '{}'; } })());\n        const canonicalizeTool = (name: string | undefined, argsStr: string): { name: string; args: string } => {\n          const whitelist = new Set(['read_mcp_resource', 'list_mcp_resources', 'list_mcp_resource_templates']);\n          if (!name || typeof name !== 'string') {return { name: 'tool', args: argsStr };}\n          const dot = name.indexOf('.');\n          if (dot <= 0) {return { name, args: argsStr };}\n          const base = name.slice(dot + 1).trim();\n          if (!whitelist.has(base)) {return { name, args: argsStr };}\n          // Drop the dotted prefix unconditionally; do not inject server from prefix\n          return { name: base, args: argsStr };\n        };\n        // Reserve 0 for text outputs; tool_call items start at 1\n        let outIndex = 1;\n        // Deduplicate short-window identical function_call (name + arguments)\n        const WINDOW = Math.max(1, Math.min(16, Number(process.env.ROUTECODEX_TOOL_CALL_DEDUPE_WINDOW || 4)));\n        const lastKeys: string[] = [];\n        const lastSet = new Set<string>();\n        for (const it of funcCalls) {\n          const id = typeof it.id === 'string' ? it.id : (typeof it.call_id === 'string' ? it.call_id : `call_${Math.random().toString(36).slice(2, 8)}`);\n          const call_id = typeof it.call_id === 'string' ? it.call_id : id;\n          const rawName = typeof it.name === 'string' ? it.name : (typeof it.tool_name === 'string' ? it.tool_name : (it?.function?.name || 'tool'));\n          const rawArgs = toArgsStr(it.arguments ?? it?.function?.arguments ?? {});\n          const { name, args: argsStr } = canonicalizeTool(rawName, rawArgs);\n          const currKey = `${String(name || '')}\\n${String(argsStr || '')}`;\n          if (lastSet.has(currKey)) { continue; }\n          lastSet.add(currKey);\n          lastKeys.push(currKey);\n          if (lastKeys.length > WINDOW) {\n            const old = lastKeys.shift();\n            if (old) {lastSet.delete(old);}\n          }\n          // Align item.type with stable schema\n          await writeEvt('response.output_item.added', { type: 'response.output_item.added', output_index: outIndex, item: { id, type: 'tool_call', status: 'in_progress', arguments: '', call_id, name } });\n          // Early required_action (idempotent per call id)\n          try {\n            const responseMeta = { id: (baseResp as any)?.id || `resp_${requestId}`, object: 'response', created_at: createdTs, model: (baseResp as any)?.model || (model || 'unknown') } as Record<string, unknown>;\n            const tool_calls_submit = [ { id, type: 'function', function: { name: String(name), arguments: '' } } ];\n            const tool_calls_simple = [ { id, type: 'function', name: String(name) } ];\n            await writeEvt('response.required_action', { type: 'response.required_action', response: responseMeta, required_action: { type: 'submit_tool_outputs', submit_tool_outputs: { tool_calls: tool_calls_submit } } });\n            // compatibility variant for clients expecting type='tool_calls' (top-level name)\n            await writeEvt('response.required_action', { type: 'response.required_action', response: responseMeta, required_action: { type: 'tool_calls', tool_calls: tool_calls_simple } });\n          } catch { /* ignore early RA */ }\n          await writeEvt('response.content_part.added', { type: 'response.content_part.added', item_id: id, output_index: outIndex, content_index: 0, part: { type: 'input_json', partial_json: '' } });\n          // arguments deltas\n          const parts = Math.max(3, Math.min(12, Math.ceil(argsStr.length / 8)));\n          const step = Math.max(1, Math.ceil(argsStr.length / parts));\n          for (let i = 0; i < argsStr.length; i += step) {\n            const d = argsStr.slice(i, i + step);\n            await writeEvt('response.tool_call.delta', { type: 'response.tool_call.delta', item_id: id, output_index: outIndex, delta: { arguments: d } });\n            await writeEvt('response.function_call_arguments.delta', { type: 'response.function_call_arguments.delta', item_id: id, output_index: outIndex, delta: d });\n          }\n          await writeEvt('response.function_call_arguments.done', { type: 'response.function_call_arguments.done', item_id: id, output_index: outIndex, arguments: String(argsStr) });\n          await writeEvt('response.output_item.done', { type: 'response.output_item.done', output_index: outIndex, item: { id, type: 'tool_call', status: 'completed', arguments: String(argsStr), call_id, name } });\n          outIndex += 1;\n        }\n        // After function_call streaming, emit required_action for clients to submit tool outputs\n        try {\n          const ra = deriveRequiredAction(funcCalls);\n          if (ra) {\n            const responseMeta = { id: (baseResp as any)?.id || `resp_${requestId}`, object: 'response', created_at: createdTs, model: (baseResp as any)?.model || (model || 'unknown') } as Record<string, unknown>;\n            await writeEvt('response.required_action', { type: 'response.required_action', response: responseMeta, required_action: ra });\n            // compatibility: emit tool_calls variant as well (top-level name)\n            if ((ra as any)?.type === 'submit_tool_outputs') {\n              const calls = (ra as any)?.submit_tool_outputs?.tool_calls || [];\n              const tool_calls_simple = calls.map((c: any) => ({ id: (c?.id || c?.call_id), type: 'function', name: (c?.function?.name || c?.name) }));\n              await writeEvt('response.required_action', { type: 'response.required_action', response: responseMeta, required_action: { type: 'tool_calls', tool_calls: tool_calls_simple } });\n            }\n          }\n        } catch { /* ignore */ }\n\n        // After function_call streaming, emit completed and close\n        const srcUsage2 = finalResp?.usage ? finalResp.usage : (initialResp?.usage ? initialResp.usage : undefined);\n        const usage2 = (() => {\n          if (!srcUsage2 || typeof srcUsage2 !== 'object') {return undefined as unknown as Record<string, number> | undefined;}\n          const input = (typeof (srcUsage2 as any).input_tokens === 'number') ? (srcUsage2 as any).input_tokens : (typeof (srcUsage2 as any).prompt_tokens === 'number' ? (srcUsage2 as any).prompt_tokens : 0);\n          const output = (typeof (srcUsage2 as any).output_tokens === 'number') ? (srcUsage2 as any).output_tokens : (typeof (srcUsage2 as any).completion_tokens === 'number' ? (srcUsage2 as any).completion_tokens : 0);\n          const total = (typeof (srcUsage2 as any).total_tokens === 'number') ? (srcUsage2 as any).total_tokens : (input + output);\n          return { input_tokens: input, output_tokens: output, total_tokens: total } as Record<string, number>;\n        })();\n        const completedSnapshot2 = { ...baseResp, status: 'completed' } as Record<string, unknown>;\n        completedSnapshot2.created_at = completedSnapshot2.created_at ?? createdTs;\n        if (usage2) {completedSnapshot2.usage = usage2;}\n        const completed2 = { type: 'response.completed', response: toCleanObject(completedSnapshot2) } as Record<string, unknown>;\n        await writeEvt('response.completed', completed2);\n        // Emit terminal done event to signal clients to stop reconnecting\n        await writeEvt('response.done', { type: 'response.done' } as unknown as Record<string, unknown>);\n        try { res.end(); } catch { /* ignore */ }\n        await auditWrite('end', { phase: 'completed_after_function_call' });\n        if (heartbeatTimer) { clearInterval(heartbeatTimer); heartbeatTimer = null; }\n        return;\n      }\n\n      // 文本直出：仅输出 output_text.delta/done（不做额外 reasoning/message 合成）\n      if (words.length > 0) {\n        for (const w of words) {\n          const delta = /^\\n+$/.test(w) ? '\\n' : w;\n          await writeEvt('response.output_text.delta', { type: 'response.output_text.delta', output_index: 0, content_index: 0, delta, logprobs: [] });\n          await new Promise(r => setTimeout(r, 12));\n        }\n        await writeEvt('response.output_text.done', { type: 'response.output_text.done', output_index: 0, content_index: 0, logprobs: [] });\n      }\n\n      // response.completed (attach total output_text when available)\n      // Map usage from final turn if available\n      const srcUsage = finalResp?.usage ? finalResp.usage : (initialResp?.usage ? initialResp.usage : undefined);\n      const mapUsage = (u: any) => {\n        if (!u || typeof u !== 'object') {return undefined as unknown;}\n        const input = (typeof u.input_tokens === 'number') ? u.input_tokens\n          : (typeof u.prompt_tokens === 'number') ? u.prompt_tokens\n          : 0;\n        const output = (typeof u.output_tokens === 'number') ? u.output_tokens\n          : (typeof u.completion_tokens === 'number') ? u.completion_tokens\n          : 0;\n        const total = (typeof u.total_tokens === 'number') ? u.total_tokens : (input + output);\n        return { input_tokens: input, output_tokens: output, total_tokens: total } as Record<string, number>;\n      };\n      const usage = mapUsage(srcUsage) as Record<string, number> | undefined;\n      const completedSnapshot = { ...baseResp, status: 'completed' } as Record<string, unknown>;\n      completedSnapshot.created_at = completedSnapshot.created_at ?? createdTs;\n      if (!('output_text' in completedSnapshot) && textOut) {\n        completedSnapshot.output_text = textOut;\n      }\n      if (!textOut) {\n        delete completedSnapshot.output_text;\n      }\n      if (usage) {completedSnapshot.usage = usage;}\n      // Ensure completed.output 仅包含聚合文本（如存在）；不注入 reasoning/function_call 条目\n      try {\n        const list: any[] = [];\n        if (textOut && textOut.trim()) {\n          list.push({ type: 'message', message: { role: 'assistant', content: [ { type: 'output_text', text: textOut } ] } });\n        }\n        (completedSnapshot as any).output = list;\n      } catch { /* ignore */ }\n      const completed = { type: 'response.completed', response: toCleanObject(completedSnapshot) } as Record<string, unknown>;\n      if (streamAborted) { return; }\n      await writeEvt('response.completed', completed);\n      await writeEvt('response.done', { type: 'response.done' } as unknown as Record<string, unknown>);\n      // Compatibility: some clients expect a final [DONE] line\n      try { res.write(`data: [DONE]\\n\\n`); } catch { /* ignore */ }\n      // no SSE audit summary when Azure artifacts removed\n      try { res.end(); } catch { /* ignore */ }\n      await auditWrite('end', { phase: 'completed' });\n      if (heartbeatTimer) {\n        clearInterval(heartbeatTimer);\n        heartbeatTimer = null;\n      }\n    } catch (err) {\n      try {\n        const e = { type: 'response.error', error: { message: (err as Error).message || 'stream error', type: 'streaming_error', code: 'STREAM_FAILED' }, requestId } as Record<string, unknown>;\n        res.write(`event: response.error\\n`);\n        res.write(`data: ${JSON.stringify(e)}\\n\\n`);\n      } catch { /* ignore */ }\n      // Ensure clients always receive a terminal event\n      try {\n        const doneEvt = { type: 'response.done' } as Record<string, unknown>;\n        res.write(`event: response.done\\n`);\n        res.write(`data: ${JSON.stringify(doneEvt)}\\n\\n`);\n      } catch { /* ignore */ }\n      await auditWrite('error', { message: (err as Error).message || String(err) });\n      try { res.end(); } catch { /* ignore */ }\n      if (heartbeatTimer) {\n        clearInterval(heartbeatTimer);\n        heartbeatTimer = null;\n      }\n    }\n  }\n\n  /**\n   * Build OpenAI Responses JSON for non-stream requests.\n   * - Prefer __final if two-turn tool flow was used; else use single payload\n   * - If payload already looks like Responses (object: 'response' or has output_text/output[]), return as-is\n   * - Otherwise, convert Chat-style payload to Responses via llmswitch-response-chat\n   */\n  private async buildResponsesJson(raw: any, reqMeta?: { tools?: unknown; tool_choice?: unknown; parallel_tool_calls?: unknown }): Promise<any> {\n    const pick = (v: any) => (v && typeof v === 'object' && v.__final) ? v.__final : v;\n    const payload = pick(raw);\n    try {\n      if (payload && typeof payload === 'object') {\n        if (payload.object === 'response' || Array.isArray((payload as any).output) || typeof (payload as any).output_text === 'string') {\n          // 不合并任何服务器端工具结果；仅透传请求工具定义到 metadata\n          if (reqMeta && typeof reqMeta === 'object' && payload && typeof payload === 'object') {\n            const base: any = { ...(payload as any) };\n            const meta: Record<string, unknown> = { ...(base.metadata || {}) };\n            if (typeof reqMeta.tools !== 'undefined') {\n              try {\n                const nt = Array.isArray((reqMeta as any).tools) ? normalizeTools((reqMeta as any).tools as any[]) : (reqMeta as any).tools;\n                const toolsStrict = Array.isArray(nt) ? (nt as any[]).map((t: any) => (t && t.type === 'function' && t.function && typeof t.function === 'object') ? { ...t, function: { ...t.function, strict: true } } : t) : nt;\n                meta.tools = toolsStrict;\n              } catch {\n                meta.tools = reqMeta.tools as unknown as any[];\n              }\n              try {\n                const crypto = await import('crypto');\n                const str = JSON.stringify(meta.tools);\n                const hash = crypto.createHash('sha256').update(str).digest('hex');\n                (meta as any).tools_hash = hash;\n                if (Array.isArray(meta.tools)) {(meta as any).tools_count = (meta.tools as any[]).length;}\n              } catch { /* ignore */ }\n            }\n            if (typeof reqMeta.tool_choice !== 'undefined') {meta.tool_choice = reqMeta.tool_choice;}\n            if (typeof reqMeta.parallel_tool_calls !== 'undefined') {meta.parallel_tool_calls = reqMeta.parallel_tool_calls;}\n            base.metadata = meta;\n            return await ResponsesMapper.enrichResponsePayload(base, payload as Record<string, unknown>, reqMeta as Record<string, unknown>);\n          }\n          return await ResponsesMapper.enrichResponsePayload(payload as Record<string, unknown>, payload as Record<string, unknown>, reqMeta as Record<string, unknown>);\n        }\n      }\n    } catch { /* ignore */ }\n    try {\n      // Core codec: Chat JSON → Responses JSON（严格、按 schema 归一），并传入请求工具上下文以统一出参行为\n      let toolsNorm: Array<Record<string, unknown>> = [];\n      const reqTools = reqMeta?.tools as unknown;\n      if (Array.isArray(reqTools)) {\n        try { toolsNorm = normalizeTools(reqTools as any[]); } catch { /* ignore */ }\n      }\n      const ctx = {\n        metadata: toolsNorm.length ? { tools: toolsNorm } : undefined,\n        toolsRaw: Array.isArray(reqTools) ? (reqTools as any[]) : undefined,\n        toolsNormalized: toolsNorm,\n        toolChoice: (reqMeta as any)?.tool_choice,\n        parallelToolCalls: (reqMeta as any)?.parallel_tool_calls,\n      } as any;\n      const converted0 = buildResponsesPayloadFromChat(payload, ctx) as Record<string, unknown>;\n\n      // Synthesize required_action for non-stream JSON when存在 function_call 输出\n      const outputs = Array.isArray((converted0 as any)?.output) ? ((converted0 as any).output as any[]) : [];\n      const funcCalls = outputs.filter(it => it && (it.type === 'function_call' || it.type === 'tool_call'));\n      let converted = converted0;\n      if (funcCalls.length > 0) {\n        const toStringJson = (v: unknown): string => {\n          if (typeof v === 'string') {return v;}\n          try { return JSON.stringify(v ?? {}); } catch { return '{}'; }\n        };\n        const calls = funcCalls.map((it: any) => {\n          const id = typeof it.id === 'string' ? it.id : (typeof it.call_id === 'string' ? it.call_id : `call_${Math.random().toString(36).slice(2,8)}`);\n          const nm = typeof it.name === 'string' ? it.name : (typeof it?.function?.name === 'string' ? it.function.name : 'tool');\n          const rawArgs = it.arguments ?? it?.function?.arguments ?? {};\n          const argsStr = toStringJson(rawArgs);\n          return { id, type: 'function', function: { name: String(nm), arguments: String(argsStr) } };\n        });\n        // Validation warnings for unsafe write redirection (cat > without heredoc)\n        const warnings: Array<Record<string, unknown>> = [];\n        const detectWriteRedirection = (name: string | undefined, args: string): boolean => {\n          try {\n            if (!name || name.toLowerCase() !== 'shell') {return false;}\n            const obj = JSON.parse(args);\n            const cmd = obj?.command;\n            const getScript = (): string => {\n              if (typeof cmd === 'string') {return cmd;}\n              if (Array.isArray(cmd)) {return cmd.map((x: any) => String(x)).join(' ');}\n              return '';\n            };\n            const s = getScript().toLowerCase();\n            if (!s) {return false;}\n            const hasBash = /\\bbash\\b\\s+-lc\\b/.test(s);\n            const hasCatWrite = /cat\\s*>\\s*[^\\s<]+/.test(s);\n            const hasHeredoc = /<</.test(s);\n            return hasBash && hasCatWrite && !hasHeredoc;\n          } catch { return false; }\n        };\n        for (const c of calls) {\n          try {\n            const nm = (c as any)?.function?.name as string | undefined;\n            const argStr = (c as any)?.function?.arguments as string || '{}';\n            if (detectWriteRedirection(nm, argStr)) {\n              warnings.push({\n                call_id: (c as any)?.id,\n                name: nm || 'tool',\n                kind: 'write_redirection_without_heredoc',\n                message: '禁止使用 cat 重定向写文件，请改用 apply_patch。',\n                suggestion: '使用统一 diff 补丁：\\n*** Begin Patch\\n*** Update File: path/to/file\\n@@\\n- old line\\n+ new line\\n*** End Patch'\n              });\n            }\n          } catch { /* ignore */ }\n        }\n        const required_action: any = { type: 'submit_tool_outputs', submit_tool_outputs: { tool_calls: calls } };\n        if (warnings.length) {(required_action as any).validation = { warnings };}\n        converted = { ...(converted0 as any), required_action };\n      }\n\n      return await ResponsesMapper.enrichResponsePayload(converted as Record<string, unknown>, payload as Record<string, unknown>, reqMeta as Record<string, unknown>);\n    } catch (e) {\n      // 不允许 fallback：严格报错，便于定位转换问题\n      const msg = (e as Error)?.message || 'Responses conversion failed (Chat→Responses)';\n      const err = new RouteCodexError(msg, 'conversion_error', 400);\n      throw err;\n    }\n  }\n}\n"
        },
        "src/server/http-server.ts": {
          "path": "src/server/http-server.ts",
          "size": 57848,
          "lines": 1648,
          "imports": [
            "express",
            "fs/promises",
            "cors",
            "helmet",
            "rcc-basemodule",
            "rcc-errorhandling",
            "rcc-debugcenter",
            "../utils/error-handling-utils.js",
            "../utils/module-config-reader.js",
            "./protocol-handler.js",
            "../core/request-handler.js",
            "../core/provider-manager.js",
            "../modules/pipeline/core/pipeline-manager.js",
            "../types/common-types.js",
            "../modules/debug/debug-file-logger.js",
            "../modules/virtual-router/classifiers/config-request-classifier.js",
            "./core/service-container.js"
          ],
          "exports": [
            "HttpServerConfig",
            "HttpServer"
          ],
          "classes": [
            "HttpServer"
          ],
          "functions": [
            "onBound",
            "onError",
            "done",
            "send",
            "captureAnthropicResponse2",
            "digest",
            "shutdown"
          ],
          "content": "/**\n * HTTP Server Implementation\n * Express.js-based HTTP server with middleware setup, health checks, and error handling\n */\n\nimport express, { type Application, type Request, type Response, type NextFunction } from 'express';\nimport fs from 'fs/promises';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { BaseModule, type ModuleInfo } from 'rcc-basemodule';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport { ErrorHandlingUtils } from '../utils/error-handling-utils.js';\nimport { ModuleConfigReader } from '../utils/module-config-reader.js';\nimport { ProtocolHandler } from './protocol-handler.js';\nimport { RequestHandler } from '../core/request-handler.js';\nimport { ProviderManager } from '../core/provider-manager.js';\nimport { PipelineManager } from '../modules/pipeline/core/pipeline-manager.js';\nimport {\n  type ServerConfig,\n  type HealthStatus,\n  type ProviderHealth,\n  type IHttpServer,\n  type ServerModuleInfo,\n} from './types.js';\nimport type { UnknownObject } from '../types/common-types.js';\nimport { DebugFileLogger } from '../modules/debug/debug-file-logger.js';\nimport { ConfigRequestClassifier } from '../modules/virtual-router/classifiers/config-request-classifier.js';\nimport { ServiceContainer, ServiceTokens } from './core/service-container.js';\n\n/**\n * HTTP Server configuration interface\n */\nexport interface HttpServerConfig {\n  port: number;\n  host: string;\n  cors?: {\n    origin: string | string[];\n    credentials?: boolean;\n  };\n  timeout?: number;\n  bodyLimit?: string;\n  enableMetrics?: boolean;\n  enableHealthChecks?: boolean;\n}\n\n/**\n * HTTP Server class\n */\nexport class HttpServer extends BaseModule implements IHttpServer {\n  private app: Application;\n  private server?: unknown;\n  private servers: unknown[] = [];\n  private moduleConfigReader: ModuleConfigReader;\n  private protocolHandler: ProtocolHandler;\n  private requestHandler: RequestHandler;\n  private providerManager: ProviderManager;\n  private errorHandling: ErrorHandlingCenter;\n  private debugEventBus: DebugEventBus;\n  private errorUtils: ReturnType<typeof ErrorHandlingUtils.createModuleErrorHandler>;\n  private _isInitialized: boolean = false;\n  private _isRunning: boolean = false;\n  private healthStatus: HealthStatus;\n  private startupTime: number;\n  private config: UnknownObject = {};\n  private mergedConfig: UnknownObject | null = null;\n  private pipelineManager: PipelineManager | null = null;\n  private routePools: Record<string, string[]> | null = null;\n  private routeMeta: Record<string, { providerId: string; modelId: string; keyId: string }> | null = null;\n  private classifier: ConfigRequestClassifier | null = null;\n  private classifierConfig: UnknownObject | null = null;\n  private serviceContainer: ServiceContainer;\n\n  // Debug enhancement properties\n  private isDebugEnhanced = false;\n  private serverMetrics: Map<string, { values: UnknownObject[]; lastUpdated: number }> = new Map();\n  private requestHistory: UnknownObject[] = [];\n  private errorHistory: UnknownObject[] = [];\n  private maxHistorySize = 100;\n\n  /** Generate a session id that embeds the current port for DebugCenter port-level separation */\n  private async sessionId(tag: string): Promise<string> {\n    try {\n      const cfg = await this.getServerConfig();\n      return `p${cfg.server.port}_${tag}_${Date.now()}`;\n    } catch {\n      return `p_unknown_${tag}_${Date.now()}`;\n    }\n  }\n\n  constructor(modulesConfigPath: string = './config/modules.json') {\n    const moduleInfo: ModuleInfo = {\n      id: 'http-server',\n      name: 'HttpServer',\n      version: '0.50.1',\n      description: 'Express.js HTTP server for RouteCodex',\n      type: 'server',\n    };\n\n    super(moduleInfo);\n\n    // Store module info for debug access\n    const moduleInfoForDebug = moduleInfo;\n    (this as UnknownObject).moduleInfo = moduleInfoForDebug;\n\n    this.moduleConfigReader = new ModuleConfigReader(modulesConfigPath);\n    this.errorHandling = new ErrorHandlingCenter();\n    try {\n      this.debugEventBus = (String(process.env.ROUTECODEX_ENABLE_DEBUGCENTER || '0') === '1') ? DebugEventBus.getInstance() : (null as any);\n    } catch { this.debugEventBus = null as any; }\n    this.errorUtils = ErrorHandlingUtils.createModuleErrorHandler('http-server');\n    this.app = express();\n    this.serviceContainer = ServiceContainer.getInstance();\n    this.startupTime = Date.now();\n\n    // Initialize health status\n    this.healthStatus = {\n      status: 'healthy',\n      timestamp: new Date().toISOString(),\n      uptime: 0,\n      memory: process.memoryUsage(),\n      providers: {},\n    };\n\n    // Initialize components with default configuration - will be properly initialized in initialize() method\n    this.providerManager = new ProviderManager(this.getDefaultServerConfig());\n    this.requestHandler = new RequestHandler(this.providerManager, this.getDefaultServerConfig());\n    this.protocolHandler = new ProtocolHandler(this.requestHandler, this.providerManager, this.moduleConfigReader);\n\n    // Initialize debug enhancements\n    this.initializeDebugEnhancements();\n  }\n\n  /**\n   * Get default server configuration for initialization\n   */\n  private getDefaultServerConfig(): ServerConfig {\n    return {\n      server: {\n        port: 5506,\n        host: 'localhost',\n        cors: {\n          origin: '*',\n          credentials: true,\n        },\n        timeout: 30000,\n        bodyLimit: '10mb',\n      },\n      logging: {\n        level: 'info',\n        enableConsole: true,\n        enableFile: false,\n        categories: ['server', 'api', 'request', 'config', 'error', 'message'],\n      },\n      providers: {},\n      routing: {\n        strategy: 'round-robin',\n        timeout: 30000,\n        retryAttempts: 3,\n      },\n    };\n  }\n\n  /**\n   * Get server configuration from modules config\n   */\n  private async getServerConfig(): Promise<ServerConfig> {\n    // Require merged configuration injected via initializeWithMergedConfig\n    if (!this.mergedConfig) {\n      throw new Error('Server configuration missing. Ensure merged-config is initialized.');\n    }\n\n    const mc: UnknownObject = this.mergedConfig as UnknownObject;\n    const httpCfg = (((mc.modules as UnknownObject) || {} as UnknownObject)['httpserver'] as UnknownObject)?.['config'] as UnknownObject | undefined;\n    const rootCfg: UnknownObject = (this.config as UnknownObject) || {};\n\n    // Port field translation: support multiple configuration field names\n    // Priority: httpserver.port > server.port > port (root level)\n    let resolvedPort: number | undefined;\n\n    // 1. Try httpserver.config.port from merged config\n    if (typeof (httpCfg as any)?.port === 'number' && (httpCfg as any).port > 0) {\n      resolvedPort = (httpCfg as any).port;\n    }\n    // 2. Try server.port from root config\n    else if (typeof (rootCfg as any)?.server?.port === 'number' && (rootCfg as any)?.server?.port > 0) {\n      resolvedPort = (rootCfg as any).server.port;\n    }\n    // 3. Try port from root config (user's current format)\n    else if (typeof (rootCfg as any)?.port === 'number' && (rootCfg as any)?.port > 0) {\n      resolvedPort = (rootCfg as any).port;\n    }\n\n    if (!(resolvedPort && resolvedPort > 0)) {\n      throw new Error('HTTP server port is missing. Please set port, server.port, or httpserver.port in your user config (~/.routecodex/config.json).');\n    }\n\n    // Host field translation: support multiple configuration field names\n    // Priority: httpserver.host > server.host > host (root level)\n    let resolvedHost: string | undefined;\n\n    // 1. Try httpserver.config.host from merged config\n    if (typeof (httpCfg as any)?.host === 'string' && (httpCfg as any).host.trim()) {\n      resolvedHost = (httpCfg as any).host.trim();\n    }\n    // 2. Try server.host from root config\n    else if (typeof (rootCfg as any)?.server?.host === 'string' && (rootCfg as any)?.server?.host.trim()) {\n      resolvedHost = (rootCfg as any).server.host.trim();\n    }\n    // 3. Try host from root config (user's current format)\n    else if (typeof (rootCfg as any)?.host === 'string' && (rootCfg as any)?.host.trim()) {\n      resolvedHost = (rootCfg as any).host.trim();\n    }\n\n    // Fallback to IPv4 localhost if no host found; avoid IPv6 by default\n    resolvedHost = resolvedHost || '127.0.0.1';\n    // Normalize host to IPv4-friendly values\n    try {\n      const lower = String(resolvedHost).toLowerCase();\n      if (lower === 'localhost') { resolvedHost = '127.0.0.1'; }\n      if (lower === '::' || lower === '::1') { resolvedHost = '127.0.0.1'; }\n      if (lower === '0.0.0.0') { resolvedHost = '127.0.0.1'; }\n    } catch { /* ignore normalization errors */ }\n    const resolvedCors = (httpCfg as any)?.cors || (rootCfg as any)?.cors || { origin: '*', credentials: true };\n    // Unified timeout override via env (in ms)\n    const unifiedTimeout = Number(process.env.ROUTECODEX_TIMEOUT_MS || process.env.RCC_TIMEOUT_MS || NaN);\n    const resolvedTimeout = !Number.isNaN(unifiedTimeout)\n      ? unifiedTimeout\n      : ((httpCfg as any)?.timeout ?? (rootCfg as any)?.timeout ?? 300000);\n    const resolvedBodyLimit = (httpCfg as any)?.bodyLimit ?? (rootCfg as any)?.bodyLimit ?? '10mb';\n\n    const logging = (rootCfg.logging as UnknownObject) || {};\n    return {\n      server: {\n        port: resolvedPort as number,\n        host: resolvedHost,\n        cors: resolvedCors as any,\n        timeout: Number(resolvedTimeout),\n        bodyLimit: String(resolvedBodyLimit),\n      },\n      logging: {\n        level: ((logging.level as string) || 'info') as 'debug' | 'info' | 'warn' | 'error',\n        enableConsole: (logging.enableConsole as boolean) !== false,\n        enableFile: (logging.enableFile as boolean) === true,\n        filePath: logging.filePath as string,\n        categories: (logging.categories as string[]) || [\n          'server',\n          'api',\n          'request',\n          'config',\n          'error',\n          'message',\n        ],\n      },\n      providers: {},\n      routing: {\n        strategy: 'round-robin',\n        timeout: 30000,\n        retryAttempts: 3,\n      },\n    };\n  }\n\n  /**\n   * Initialize the HTTP server with merged configuration\n   */\n  public async initializeWithMergedConfig(mergedConfig: UnknownObject): Promise<void> {\n    try {\n      console.log('🔄 Initializing HTTP server with merged configuration...');\n\n      // Set up the server with merged configuration\n      this.mergedConfig = mergedConfig;\n      // Use the merged config directly for server configuration (port, host, etc.)\n      this.config = mergedConfig;\n\n      // Initialize DebugCenter file logging if configured\n      const debugCenterConfig = (mergedConfig.modules as any)?.debugcenter?.config;\n      if (debugCenterConfig?.enableFile && debugCenterConfig.filePath) {\n        DebugFileLogger.initialize({\n          filePath: debugCenterConfig.filePath,\n          enabled: true,\n        });\n      }\n\n      // Continue with normal initialization\n      await this.initialize();\n\n      console.log('✅ HTTP server initialized with merged configuration successfully');\n\n      // Attach authMappings to protocol handler if available in merged config\n      try {\n        const pac = (this.mergedConfig as any)?.pipeline_assembler?.config;\n        let authMappings = pac?.authMappings as Record<string, string> | undefined;\n        // Fallback: derive from compatibilityConfig.keyMappings.global when assembler does not expose authMappings\n        if (!authMappings) {\n          const cc = (this.mergedConfig as any)?.compatibilityConfig;\n          const km = cc?.keyMappings;\n          const globalMap = km && typeof km === 'object' ? (km as any).global : undefined;\n          if (globalMap && typeof globalMap === 'object') {\n            authMappings = { ...globalMap } as Record<string, string>;\n          }\n        }\n        if (authMappings && this.protocolHandler && typeof (this.protocolHandler as any).attachAuthMappings === 'function') {\n          (this.protocolHandler as any).attachAuthMappings(authMappings);\n        }\n      } catch { /* ignore */ }\n    } catch (error) {\n      console.error('❌ Failed to initialize HTTP server with merged configuration:', error);\n      throw error;\n    }\n  }\n\n  /**\n   * Initialize the HTTP server\n   */\n  public async initialize(): Promise<void> {\n    try {\n      // Load modules configuration\n      await this.moduleConfigReader.load();\n\n      // Initialize error handling utilities\n      await ErrorHandlingUtils.initialize();\n\n      // Initialize error handling\n      await this.errorHandling.initialize();\n\n      // Re-initialize components with actual configuration\n      const serverConfig = await this.getServerConfig();\n      this.providerManager = new ProviderManager(serverConfig);\n      this.requestHandler = new RequestHandler(this.providerManager, serverConfig);\n      // Load router config from modules.json if available\n      const protocolHandlerModule = this.moduleConfigReader.getModuleConfigValue<UnknownObject>(\n        'protocolhandler',\n        {}\n      );\n      this.protocolHandler = new ProtocolHandler(this.requestHandler, this.providerManager, this.moduleConfigReader, (protocolHandlerModule || {}) as UnknownObject);\n\n      // Initialize request handler\n      await this.requestHandler.initialize();\n\n      // Initialize provider manager\n      await this.providerManager.initialize();\n\n      // Initialize OpenAI router\n      await this.protocolHandler.initialize();\n\n      // Setup Express middleware\n      await this.setupMiddleware();\n\n      // Setup routes\n      this.setupRoutes();\n\n      // Setup error handling\n      this.setupErrorHandling();\n\n      // Setup graceful shutdown\n      this.setupGracefulShutdown();\n\n      // Register error messages for HTTP server\n      this.errorUtils.registerMessage(\n        'server_start_error',\n        'Failed to start HTTP server',\n        'critical',\n        'server',\n        'HTTP server failed to start on specified port',\n        'Check port availability and permissions'\n      );\n\n      this.errorUtils.registerMessage(\n        'server_stop_error',\n        'Failed to stop HTTP server',\n        'high',\n        'server',\n        'HTTP server failed to stop gracefully',\n        'Force kill process and check for resource leaks'\n      );\n\n      this.errorUtils.registerMessage(\n        'middleware_error',\n        'Express middleware error',\n        'medium',\n        'server',\n        'Error in Express middleware setup',\n        'Check middleware configuration and dependencies'\n      );\n\n      this.errorUtils.registerMessage(\n        'route_error',\n        'Route handling error',\n        'medium',\n        'server',\n        'Error processing HTTP route',\n        'Check route configuration and request format'\n      );\n\n      this.errorUtils.registerMessage(\n        'initialization_error',\n        'Server initialization error',\n        'critical',\n        'system',\n        'Failed to initialize server components',\n        'Check logs and ensure all dependencies are available'\n      );\n\n      // Register error handlers for HTTP server\n      this.errorUtils.registerHandler(\n        'server_start_error',\n        async context => {\n          console.error(`Server start error: ${context.error}`);\n          // Could implement automatic port change or retry logic\n        },\n        1,\n        'Handle server startup errors'\n      );\n\n      this.errorUtils.registerHandler(\n        'initialization_error',\n        async context => {\n          console.error(`Critical initialization error: ${context.error}`);\n          // Could implement component restart logic\n        },\n        0,\n        'Handle critical initialization errors'\n      );\n\n      this._isInitialized = true;\n\n      try {\n        if (process.env.ROUTECODEX_ENABLE_DEBUGCENTER === '1') {\n          this.debugEventBus.publish({\n            sessionId: await this.sessionId('http_server_initialized'),\n            moduleId: 'http-server',\n            operationId: 'http_server_initialized',\n            timestamp: Date.now(),\n            type: 'start',\n            position: 'middle',\n            data: {\n              configPath: './config/modules.json',\n              port: (await this.getServerConfig()).server.port,\n              host: (await this.getServerConfig()).server.host,\n            },\n          });\n        }\n      } catch { /* ignore */ }\n    } catch (error) {\n      await this.handleError(error as Error, 'initialization');\n      throw error;\n    }\n  }\n\n  /**\n   * Attach a pipeline manager to the OpenAI router (enables HTTP → Router → Pipeline layering)\n   */\n  public attachPipelineManager(pipelineManager: PipelineManager): void {\n    this.pipelineManager = pipelineManager as PipelineManager;\n    try {\n      this.serviceContainer.registerInstance(ServiceTokens.PIPELINE_MANAGER, pipelineManager);\n    } catch { /* ignore registration errors */ }\n    try {\n      if (this.protocolHandler && (this.protocolHandler as any).attachPipelineManager) {\n        (this.protocolHandler as any).attachPipelineManager(pipelineManager);\n      }\n    } catch (error) {\n      // Do not break server if attachment fails\n      console.error('Failed to attach pipeline manager to OpenAI router:', error);\n    }\n    // Router handles its own route selection; no local resolver\n  }\n\n  /** Attach static route pools for round-robin dispatch */\n  public attachRoutePools(routePools: Record<string, string[]>): void {\n    this.routePools = routePools;\n    try {\n      this.serviceContainer.registerInstance(ServiceTokens.ROUTE_POOLS, routePools);\n    } catch { /* ignore registration errors */ }\n    try {\n      if (this.protocolHandler && (this.protocolHandler as any).attachRoutePools) {\n        (this.protocolHandler as any).attachRoutePools(routePools);\n      }\n    } catch (error) {\n      console.error('Failed to attach route pools to OpenAI router:', error);\n    }\n\n    // Consistency check: every bound pipeline id must exist in PipelineManager\n    try {\n      if (this.pipelineManager && routePools) {\n        const existing = new Set<string>(this.pipelineManager.getPipelineIds());\n        const missing: Array<{ route: string; id: string }> = [];\n        for (const [route, ids] of Object.entries(routePools)) {\n          for (const id of ids || []) {\n            if (!existing.has(id)) missing.push({ route, id });\n          }\n        }\n        if (missing.length) {\n          // Fail fast with detailed message\n          const details = missing.map(m => `${m.route}→${m.id}`).join(', ');\n          const err = new Error(`RoutePools reference unknown pipelines: ${details}`);\n          (err as any).code = 'route_pools_inconsistent';\n          throw err;\n        }\n      }\n    } catch (e) {\n      // Surface the problem and stop boot/attachment\n      console.error('Route pools consistency check failed:', e);\n      throw e;\n    }\n  }\n\n  public attachRouteMeta(routeMeta: Record<string, { providerId: string; modelId: string; keyId: string }>): void {\n    this.routeMeta = routeMeta;\n    try {\n      this.serviceContainer.registerInstance(ServiceTokens.ROUTE_META, routeMeta);\n    } catch { /* ignore */ }\n    try {\n      if (this.protocolHandler && (this.protocolHandler as any).attachRouteMeta) {\n        (this.protocolHandler as any).attachRouteMeta(routeMeta);\n      }\n    } catch (error) {\n      console.error('Failed to attach route meta to OpenAI router:', error);\n    }\n  }\n\n  /** Attach classification config to router */\n  public attachRoutingClassifierConfig(classifierConfig: UnknownObject): void {\n    this.classifierConfig = classifierConfig;\n    try {\n      if (this.protocolHandler && (this.protocolHandler as any).attachRoutingClassifierConfig) {\n        (this.protocolHandler as any).attachRoutingClassifierConfig(classifierConfig);\n      }\n    } catch (error) {\n      console.error('Failed to attach classifier config to OpenAI router:', error);\n    }\n\n    try {\n      if (this.classifier) {\n        this.serviceContainer.registerInstance(ServiceTokens.ROUTING_CLASSIFIER, this.classifier);\n      }\n    } catch { /* ignore */ }\n  }\n\n  /**\n   * Initialize debug enhancements\n   */\n  private initializeDebugEnhancements(): void {\n    try {\n      this.isDebugEnhanced = true;\n      console.log('HTTP Server debug enhancements initialized');\n    } catch (error) {\n      console.warn('Failed to initialize HTTP Server debug enhancements:', error);\n      this.isDebugEnhanced = false;\n    }\n  }\n\n  /**\n   * Safely publish a DebugCenter event when enabled\n   */\n  private safePublishDebug(event: UnknownObject): void {\n    try {\n      if (String(process.env.ROUTECODEX_ENABLE_DEBUGCENTER || '0') === '1' && this.debugEventBus && typeof (this.debugEventBus as any).publish === 'function') {\n        this.debugEventBus.publish(event as any);\n      }\n    } catch {\n      // no-op when DebugCenter is disabled or unavailable\n    }\n  }\n\n  /**\n   * Record server metric\n   */\n  private recordServerMetric(operation: string, data: UnknownObject): void {\n    if (!this.serverMetrics.has(operation)) {\n      this.serverMetrics.set(operation, { values: [], lastUpdated: Date.now() });\n    }\n\n    const metric = this.serverMetrics.get(operation)!;\n    metric.values.push(data);\n    metric.lastUpdated = Date.now();\n\n    // Keep only last 50 measurements\n    if (metric.values.length > 50) {\n      metric.values.shift();\n    }\n  }\n\n  /**\n   * Add to request history\n   */\n  private addToRequestHistory(request: UnknownObject): void {\n    this.requestHistory.push(request);\n\n    // Keep only recent history\n    if (this.requestHistory.length > this.maxHistorySize) {\n      this.requestHistory.shift();\n    }\n  }\n\n  /**\n   * Add to error history\n   */\n  private addToErrorHistory(error: UnknownObject): void {\n    this.errorHistory.push(error);\n\n    // Keep only recent history\n    if (this.errorHistory.length > this.maxHistorySize) {\n      this.errorHistory.shift();\n    }\n  }\n\n  /**\n   * Publish debug event\n   */\n  private publishDebugEvent(type: string, data: UnknownObject): void {\n    if (!this.isDebugEnhanced) {\n      return;\n    }\n\n    try {\n      this.debugEventBus.publish({\n        sessionId: `session_${Date.now()}`,\n        moduleId: 'http-server',\n        operationId: type,\n        timestamp: Date.now(),\n        type: 'start',\n        position: 'middle',\n        data: {\n          ...data,\n          serverId: ((this as UnknownObject).moduleInfo as any).id,\n          source: 'http-server',\n        },\n      });\n    } catch (error) {\n      // Silent fail if debug event bus is not available\n    }\n  }\n\n  /**\n   * Get debug status with enhanced information\n   */\n  getDebugStatus(): UnknownObject {\n    const moduleInfo = (this as any).moduleInfo as any;\n    const baseStatus = {\n      serverId: moduleInfo.id,\n      isInitialized: this._isInitialized,\n      isRunning: this._isRunning,\n      type: moduleInfo.type,\n      isEnhanced: this.isDebugEnhanced,\n    };\n\n    if (!this.isDebugEnhanced) {\n      return baseStatus;\n    }\n\n    return {\n      ...baseStatus,\n      debugInfo: this.getDebugInfo(),\n      serverMetrics: this.getServerMetrics(),\n      healthStatus: this.healthStatus,\n      requestHistory: [...this.requestHistory.slice(-10)], // Last 10 requests\n      errorHistory: [...this.errorHistory.slice(-10)], // Last 10 errors\n    };\n  }\n\n  /**\n   * Get detailed debug information\n   */\n  private getDebugInfo(): UnknownObject {\n    const moduleInfo = (this as any).moduleInfo as any;\n    return {\n      serverId: moduleInfo.id,\n      serverType: moduleInfo.type,\n      enhanced: this.isDebugEnhanced,\n      eventBusAvailable: !!this.debugEventBus,\n      requestHistorySize: this.requestHistory.length,\n      errorHistorySize: this.errorHistory.length,\n      uptime: Date.now() - this.startupTime,\n      hasPipelineManager: !!this.pipelineManager,\n      hasRoutePools: !!this.routePools,\n      serverConfig: this.config?.server || null,\n    };\n  }\n\n  /**\n   * Get server metrics\n   */\n  private getServerMetrics(): UnknownObject {\n    const metrics: UnknownObject = {};\n\n    for (const [operation, metric] of this.serverMetrics.entries()) {\n      metrics[operation] = {\n        count: metric.values.length,\n        lastUpdated: metric.lastUpdated,\n        recentValues: metric.values.slice(-5), // Last 5 values\n      };\n    }\n\n    return metrics;\n  }\n\n  // Pipeline assembly removed: handled externally via merged-config assembler.\n\n  /**\n   * Start the HTTP server\n   */\n  public async start(): Promise<void> {\n    if (!this._isInitialized) {\n      await this.initialize();\n    }\n\n    const config = await this.getServerConfig();\n    const rootCfg = (this.config as UnknownObject) || {};\n    const dualStack: boolean = Boolean((rootCfg as any).dualStack ?? (rootCfg as any)?.server?.dualStack);\n    const requestedHost = config.server.host;\n    const hostsToBind: string[] = (() => {\n      if (!dualStack) { return [requestedHost]; }\n      const set = new Set<string>();\n      const lower = String(requestedHost || '').toLowerCase();\n      if (lower === 'localhost') { set.add('127.0.0.1'); set.add('::1'); }\n      else if (lower === '127.0.0.1') { set.add('127.0.0.1'); set.add('::1'); }\n      else if (lower === '::1') { set.add('::1'); set.add('127.0.0.1'); }\n      else { set.add(requestedHost); }\n      return Array.from(set.values());\n    })();\n\n    return new Promise((resolve, reject) => {\n      const total = hostsToBind.length;\n      let successes = 0;\n      let errors = 0;\n      let firstError: Error | null = null;\n\n      const onBound = (host: string, srv: any) => {\n        successes += 1;\n        this._isRunning = true;\n        this.startupTime = Date.now();\n        this.servers.push(srv);\n        if (!this.server) { this.server = srv; }\n        this.safePublishDebug({\n          sessionId: `p${config.server.port}_http_server_started_${Date.now()}`,\n          moduleId: 'http-server',\n          operationId: 'http_server_started',\n          timestamp: Date.now(),\n          type: 'start',\n          position: 'middle',\n          data: { port: config.server.port, host, uptime: this.getUptime() },\n        });\n        console.log(`🚀 RouteCodex HTTP Server started on http://${host}:${config.server.port}`);\n        if (successes === 1) { resolve(); }\n      };\n\n      const onError = async (error: Error) => {\n        errors += 1;\n        if (!firstError) { firstError = error; }\n        await this.handleError(error as Error, 'server_start');\n        if (errors >= total && successes === 0) {\n          reject(firstError || error);\n        }\n      };\n\n      for (const h of hostsToBind) {\n        try {\n          const srv: any = this.app.listen(config.server.port, h, () => onBound(h, srv));\n          srv.on('error', onError);\n        } catch (e) {\n          // synchronous error\n          errors += 1;\n          if (!firstError) { firstError = e as Error; }\n        }\n      }\n\n      // If all attempts failed synchronously\n      if (errors >= total && successes === 0) {\n        reject(firstError || new Error('Failed to bind any host'));\n      }\n    });\n  }\n\n  /**\n   * Stop the HTTP server\n   */\n  public async stop(): Promise<void> {\n    try {\n      const toClose: unknown[] = this.servers && this.servers.length ? this.servers : (this.server ? [this.server] : []);\n      if (toClose.length) {\n        await new Promise<void>((resolveAll) => {\n          let remaining = toClose.length;\n          const done = () => { remaining -= 1; if (remaining <= 0) {resolveAll();} };\n          for (const srv of toClose) {\n            try { (srv as any).close(() => done()); } catch { done(); }\n          }\n        });\n        this._isRunning = false;\n        this.servers = [];\n\n        // Stop all components\n        await this.protocolHandler.stop();\n        await this.providerManager.stop();\n        await this.requestHandler.stop();\n        // Config manager doesn't need to be closed in new architecture\n        await this.errorHandling.destroy();\n\n        this.safePublishDebug({\n          sessionId: `session_${Date.now()}`,\n          moduleId: 'http-server',\n          operationId: 'http_server_stopped',\n          timestamp: Date.now(),\n          type: 'end',\n          position: 'middle',\n          data: {\n            uptime: this.getUptime(),\n          },\n        });\n\n        console.log('🛑 RouteCodex HTTP Server stopped');\n        return;\n      }\n    } catch (error) {\n      await this.handleError(error as Error, 'server_stop');\n      throw error;\n    }\n  }\n\n  /**\n   * Get current health status\n   */\n  public getStatus(): HealthStatus {\n    // const config = this.getDefaultServerConfig(); // Reserved for future use\n\n    const providers = this.getUnifiedProvidersHealth();\n    return {\n      status: this._isRunning ? 'healthy' : 'unhealthy',\n      timestamp: new Date().toISOString(),\n      uptime: this.getUptime(),\n      memory: process.memoryUsage(),\n      providers,\n    };\n  }\n\n  /**\n   * Get module info\n   */\n  public getModuleInfo(): ServerModuleInfo {\n    return {\n      id: 'http-server',\n      name: 'HttpServer',\n      version: '0.0.1',\n      description: 'Express.js HTTP server for RouteCodex',\n      type: 'server',\n      capabilities: ['http-server', 'openai-api', 'health-checks', 'metrics'],\n      dependencies: ['config-manager', 'request-handler', 'provider-manager', 'protocol-handler'],\n    };\n  }\n\n  /**\n   * Setup Express middleware\n   */\n  private async setupMiddleware(): Promise<void> {\n    const config = await this.getServerConfig();\n\n    // Security middleware\n    this.app.use(\n      helmet({\n        contentSecurityPolicy: {\n          directives: {\n            defaultSrc: [\"'self'\"],\n            styleSrc: [\"'self'\", \"'unsafe-inline'\"],\n            scriptSrc: [\"'self'\"],\n            imgSrc: [\"'self'\", 'data:', 'https:'],\n          },\n        },\n      })\n    );\n\n    // CORS middleware\n    if (config.server.cors) {\n      this.app.use(cors(config.server.cors));\n    } else {\n      this.app.use(\n        cors({\n          origin: '*',\n          credentials: true,\n        })\n      );\n    }\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: '10mb' }));\n    this.app.use(express.urlencoded({ extended: true, limit: '10mb' }));\n\n    // Request logging middleware\n    this.app.use((req: Request, res: Response, next: NextFunction) => {\n      const start = Date.now();\n\n      res.on('finish', () => {\n        const duration = Date.now() - start;\n\n        this.safePublishDebug({\n          sessionId: `session_${Date.now()}`,\n          moduleId: 'http-server',\n          operationId: 'request_completed',\n          timestamp: Date.now(),\n          type: 'end',\n          position: 'middle',\n          data: {\n            method: req.method,\n            url: req.url,\n            status: res.statusCode,\n            duration,\n            userAgent: req.get('user-agent'),\n            ip: req.ip,\n          },\n        });\n      });\n\n      next();\n    });\n\n    // Request timeout middleware\n    if (config.server.timeout) {\n      this.app.use((req: Request, res: Response, next: NextFunction) => {\n        req.setTimeout(config.server.timeout!);\n        next();\n      });\n    }\n\n    // Compression middleware (if needed)\n    this.app.use((req: Request, res: Response, next: NextFunction) => {\n      res.setHeader('X-Powered-By', 'RouteCodex');\n      next();\n    });\n  }\n\n  /**\n   * Setup routes\n   */\n  private setupRoutes(): void {\n    // Health check endpoints\n    this.app.get('/health', this.handleHealthCheck.bind(this));\n    this.app.get('/healthz', this.handleHealthCheck.bind(this));\n    this.app.get('/ready', this.handleReadinessCheck.bind(this));\n    this.app.get('/live', this.handleLivenessCheck.bind(this));\n\n    // Metrics endpoint\n    this.app.get('/metrics', async (req, res) => this.handleMetrics(req, res));\n\n    // Defer mounting OpenAI /v1 catch-all until after Anthropic endpoints to avoid shadowing\n\n    // Anthropic API canonical endpoint /v1/messages (Anthropic SDK default)\n    this.app.post('/v1/messages', async (req: Request, res: Response) => {\n      try {\n        if (!this.pipelineManager || !this.routePools) {\n          return res.status(503).json({ error: { message: 'Pipeline not ready', code: 'pipeline_not_ready' } });\n        }\n\n        const requestId = `anth_${Date.now()}`;\n        const pickFirst = (): string | null => {\n          const pools = this.routePools || {} as Record<string, string[]>;\n          // Prefer non-empty anthropic pool; otherwise fallback to default\n          const anth = Array.isArray(pools['anthropic']) ? pools['anthropic'] : [];\n          const def = Array.isArray(pools['default']) ? pools['default'] : [];\n          const list = anth.length > 0 ? anth : def;\n          if (list.length > 0) { return list[0]; }\n          // As a last resort, pick the first pipeline id from any non-empty pool\n          for (const v of Object.values(pools)) {\n            if (Array.isArray(v) && v.length > 0) { return v[0]; }\n          }\n          return null;\n        };\n\n        let codexCaptured = false;\n        const maybeCapture = async (payload: UnknownObject): Promise<void> => {\n          if (codexCaptured) { return; }\n          try {\n            const ua = (req.get('user-agent') || '').toLowerCase();\n            if (!(ua.includes('codex') || ua.includes('claude'))) { return; }\n            const baseDir = `${process.env.HOME || ''}/.routecodex/codex-samples`;\n            await fs.mkdir(baseDir, { recursive: true });\n            const sample = {\n              requestId,\n              endpoint: '/v1/messages',\n              timestamp: Date.now(),\n              headers: req.headers,\n              data: payload,\n            };\n            await fs.writeFile(`${baseDir}/pipeline-in-${requestId}.json`, JSON.stringify(sample, null, 2));\n            // Also save into dedicated replay subfolder for Anthropic validation\n            try {\n        const subDir = `${baseDir}/anth-replay`;\n              await fs.mkdir(subDir, { recursive: true });\n              await fs.writeFile(`${subDir}/anthropic-request-${requestId}.json`, JSON.stringify(sample, null, 2));\n              // Capture tool_result blocks (if present) as separate snapshot(s)\n              try {\n                const body = (payload || {}) as any;\n                const messages = Array.isArray(body?.messages) ? body.messages : [];\n                const toolResults: any[] = [];\n                for (const m of messages) {\n                  const content = Array.isArray(m?.content) ? m.content : [];\n                  for (const c of content) {\n                    if (c && typeof c === 'object' && c.type === 'tool_result') {\n                      toolResults.push({\n                        tool_use_id: c.tool_use_id,\n                        content: c.content ?? c.output ?? null,\n                        is_error: !!c.is_error,\n                        role: m.role,\n                      });\n                    }\n                  }\n                }\n                if (toolResults.length > 0) {\n                  const toolResCapture = {\n                    requestId,\n                    endpoint: '/v1/messages',\n                    timestamp: Date.now(),\n                    count: toolResults.length,\n                    tool_results: toolResults,\n                  };\n                  await fs.writeFile(`${subDir}/anthropic-tool-results-${requestId}.json`, JSON.stringify(toolResCapture, null, 2));\n                }\n              } catch { /* ignore tool_result capture errors */ }\n            } catch { /* ignore */ }\n            codexCaptured = true;\n          } catch { /* ignore */ }\n        };\n\n        await maybeCapture(req.body as UnknownObject);\n\n        const pipelineId = pickFirst();\n        if (!pipelineId) {\n          return res.status(503).json({ error: { message: 'No pipelines available', code: 'no_pipelines' } });\n        }\n\n        // Use assembler-supplied meta to avoid parsing pipelineId\n        const m = (this.routeMeta && this.routeMeta[pipelineId]) || null;\n        try {\n          console.log('[HTTP] anthropic /v1/messages pick', { pipelineId, hasMeta: !!m });\n        } catch { /* ignore */ }\n        const providerId = m?.providerId || 'unknown';\n        const modelId = m?.modelId || 'unknown';\n\n        // Optional: Replace only system prompt (tools untouched) if selector active\n        try {\n          const { shouldReplaceSystemPrompt, SystemPromptLoader } = await import('../utils/system-prompt-loader.js');\n          const sel = shouldReplaceSystemPrompt();\n          if (sel) {\n            const loader = SystemPromptLoader.getInstance();\n            const sys = await loader.getPrompt(sel);\n            const currentSys = (req.body && typeof req.body === 'object' && (req.body as any).system) ? String((req.body as any).system) : '';\n            const hasMdMarkers = /\\bCLAUDE\\.md\\b|\\bAGENT(?:S)?\\.md\\b/i.test(currentSys);\n            if (sys && req.body && typeof req.body === 'object' && !hasMdMarkers) {\n              req.body = { ...(req.body as Record<string, unknown>), system: sys } as any;\n              try { res.setHeader('x-rc-system-prompt-source', sel); } catch { /* ignore */ }\n            }\n          }\n          // Tool guidance and normalization are handled downstream in llmswitch-core\n        } catch { /* non-blocking */ }\n\n        const pipelineRequest = {\n          data: req.body,\n          route: {\n            providerId,\n            modelId,\n            requestId,\n            timestamp: Date.now(),\n            pipelineId,\n          },\n          metadata: {\n            method: req.method,\n            url: '/v1/messages',\n            headers: req.headers,\n            targetProtocol: 'anthropic',\n            endpoint: '/v1/messages'\n          },\n          debug: { enabled: true, stages: { llmSwitch: true, workflow: true, compatibility: true, provider: true } },\n        } as UnknownObject;\n\n        // Pre-SSE heartbeat for anthropic when waiting on pipeline (disabled when ROUTECODEX_DISABLE_ANTHROPIC_STREAM=1)\n        let preHeartbeat: NodeJS.Timeout | null = null;\n        const jsonHeartbeatEnabled2 = false;\n        const disableAnthropicStreamB = process.env.ROUTECODEX_DISABLE_ANTHROPIC_STREAM === '1';\n        if (!disableAnthropicStreamB && (req.body as any)?.stream && !res.headersSent) {\n          try {\n            res.setHeader('Content-Type', 'text/event-stream');\n            res.setHeader('Cache-Control', 'no-cache');\n            res.setHeader('Connection', 'keep-alive');\n            res.setHeader('Transfer-Encoding', 'chunked');\n            res.setHeader('x-request-id', requestId);\n            const interval = Math.max(5000, Number(process.env.ROUTECODEX_SSE_HEARTBEAT_MS || 15000));\n            const send = () => {\n              try { res.write(`event: ping\\n` + `data: ${JSON.stringify({ type: 'ping', stage: 'pre', requestId, ts: Date.now() })}\\n\\n`); } catch { /* ignore */ }\n            };\n            send();\n            preHeartbeat = setInterval(send, interval);\n          } catch { /* ignore */ }\n        }\n        // (no JSON heartbeat for non-stream)\n\n        const pipelineResponse = await this.pipelineManager.processRequest(pipelineRequest as any);\n        const data = (pipelineResponse as any)?.data ?? pipelineResponse;\n        // Capture final Anthropic JSON response (non-stream)\n        const captureAnthropicResponse2 = async (payload: unknown) => {\n          try {\n            const baseDir = `${process.env.HOME || ''}/.routecodex/codex-samples`;\n            const subDir = `${baseDir}/anth-replay`;\n            await fs.mkdir(subDir, { recursive: true });\n            // No backfill: do not store tool_use for future mutation\n            const digest = (() => {\n              try {\n                const obj = (payload && typeof payload === 'object') ? payload as any : {};\n                const content = Array.isArray(obj.content) ? obj.content : [];\n                const tools = content.filter((b: any) => b && b.type === 'tool_use');\n                const first = tools.length ? tools[0] : null;\n                return {\n                  tool_use_count: tools.length,\n                  first_tool_use: first ? {\n                    id: first.id || null,\n                    name: first.name || null,\n                    input_keys: first.input && typeof first.input === 'object' ? Object.keys(first.input) : []\n                  } : null,\n                  stop_reason: obj.stop_reason || null\n                };\n              } catch { return { tool_use_count: 0, first_tool_use: null, stop_reason: null }; }\n            })();\n            const out = {\n              requestId,\n              endpoint: '/v1/messages',\n              timestamp: Date.now(),\n              response: { status: 200, headers: { 'x-rc-target-protocol': 'anthropic' } },\n              data: payload,\n              mappingDigest: digest,\n            };\n            await fs.writeFile(`${subDir}/anthropic-response-${requestId}.json`, JSON.stringify(out, null, 2));\n          } catch { /* ignore */ }\n        };\n        if (!disableAnthropicStreamB && (req.body as any)?.stream) {\n          res.status(200);\n          const protocolHandlerAny = this.protocolHandler as unknown as { streamFromPipeline?: (response: unknown, requestId: string, res: Response, model?: string, protocol?: 'openai' | 'anthropic') => Promise<void> };\n          if (protocolHandlerAny?.streamFromPipeline) {\n            try { if (preHeartbeat) { clearInterval(preHeartbeat); preHeartbeat = null; } } catch { /* ignore */ }\n            // no JSON heartbeat cleanup needed\n            await protocolHandlerAny.streamFromPipeline(pipelineResponse, requestId, res, (req.body as any)?.model as string | undefined, 'anthropic');\n            return;\n          }\n        }\n        try { if (preHeartbeat) { clearInterval(preHeartbeat); preHeartbeat = null; } } catch { /* ignore */ }\n        if (!res.headersSent) {\n          try { res.setHeader('x-rc-target-protocol', 'anthropic'); } catch { /* ignore */ }\n          try { res.setHeader('anthropic-version', '2023-06-01'); } catch { /* ignore */ }\n        }\n        await captureAnthropicResponse2(data);\n        if (!res.headersSent) {\n          res.status(200).json(data);\n        } else {\n          try { res.write(typeof data === 'string' ? data : JSON.stringify(data)); } catch { /* ignore */ }\n          try { res.end(); } catch { /* ignore */ }\n        }\n      } catch (err) {\n        if (!res.headersSent) {\n          res.status(500).json({ error: { message: (err as Error).message || 'Anthropic handler error' } });\n        } else {\n          try { res.end(); } catch { /* ignore */ }\n        }\n      }\n    });\n\n    // OpenAI API endpoints (mounted after Anthropic endpoints to prevent route shadowing)\n    const protocolRouter = this.protocolHandler.getRouter();\n    const protocolHandlerConfig = this.moduleConfigReader.getModuleConfigValue<UnknownObject>(\n      'protocolhandler',\n      {}\n    );\n    const basePath = (protocolHandlerConfig as any)?.basePath || '/v1/openai';\n\n    // Mount router at configured base path and at /v1 for compatibility\n    // Only mount at /v1 if basePath is not already /v1 to avoid conflicts\n    this.app.use(basePath, protocolRouter);\n    if (basePath !== '/v1') {\n      this.app.use('/v1', protocolRouter);\n    }\n\n    // Lightweight debug endpoint to inspect routing state\n    this.app.get('/debug/route-pools', async (_req: Request, res: Response) => {\n      try {\n        const ids = this.pipelineManager ? Array.from((this.pipelineManager as any).pipelines?.keys?.() || []) : [];\n        return res.status(200).json({\n          routePools: this.routePools,\n          routeMeta: this.routeMeta ? Object.keys(this.routeMeta) : [],\n          managerPipelines: ids,\n        });\n      } catch (e) {\n        return res.status(500).json({ error: { message: (e as Error).message } });\n      }\n    });\n\n    // Status endpoint\n    this.app.get('/status', this.handleStatus.bind(this));\n\n    // Configuration endpoint\n    this.app.get('/config', async (req, res) => this.handleConfig(req, res));\n\n    // Graceful shutdown endpoint (local control)\n    this.app.post('/shutdown', async (_req: Request, res: Response) => {\n      try {\n        res.status(200).json({ ok: true, message: 'Shutting down' });\n      } catch { /* ignore */ }\n      // Defer actual stop to avoid tearing down before response flush\n      setTimeout(async () => {\n        try {\n          await this.stop();\n        } catch { /* ignore */ }\n        try { process.exit(0); } catch { /* ignore */ }\n      }, 50);\n    });\n\n    // Merged configuration endpoint (raw merged-config for current instance)\n    this.app.get('/merged-config', async (_req: Request, res: Response) => {\n      try {\n        const merged = this.mergedConfig || null;\n        res.status(200).json({\n          mergedConfig: merged,\n          note: 'This is the in-memory merged configuration for this server instance.',\n        });\n      } catch (err) {\n        res.status(500).json({\n          error: { message: err instanceof Error ? err.message : 'Failed to read merged config' },\n        });\n      }\n    });\n\n    // Debug endpoint\n    this.app.get('/debug', this.handleDebug.bind(this));\n\n    // Test endpoint for error handling\n    this.app.get('/test-error', this.handleTestError.bind(this));\n\n    // Root endpoint\n    this.app.get('/', (req: Request, res: Response) => {\n      res.json({\n        name: 'RouteCodex',\n        version: this.getModuleInfo().version,\n        description: 'Multi-provider AI API proxy server',\n        status: this.getStatus(),\n        endpoints: {\n          health: '/health',\n          metrics: '/metrics',\n          status: '/status',\n          openai: '/v1/openai/*',\n          anthropic: '/v1/messages',\n          config: '/config',\n        },\n      });\n    });\n\n    // 404 handler\n    this.app.use('*', (req: Request, res: Response) => {\n      res.status(404).json({\n        error: {\n          message: `Route ${req.method} ${req.originalUrl} not found`,\n          type: 'not_found_error',\n          code: 'not_found',\n        },\n      });\n    });\n  }\n\n  /**\n   * Setup error handling\n   */\n  private setupErrorHandling(): void {\n    this.app.use(async (error: UnknownObject, req: Request, res: Response, _next: NextFunction) => {\n      // Friendly JSON parse error mapping\n      if (error?.type === 'entity.parse.failed') {\n        res.status(400).json({\n          error: {\n            message: `Invalid JSON body: ${error?.message || 'parse failed'}`,\n            type: 'bad_request',\n            code: 'invalid_json',\n          },\n        });\n        return;\n      }\n\n      await this.handleError(error as unknown as Error, 'request_handler');\n\n      // Normalize sandbox/permission errors to explicit 500 with sandbox_denied\n      const errObj = error as any;\n      let status = (errObj.status as number) || (errObj.statusCode as number) || 500;\n      let code = (errObj.code as string) || 'internal_error';\n      let type = error.type || 'internal_error';\n      let message = error.message || 'Internal Server Error';\n      try {\n        const det = ErrorHandlingUtils.detectSandboxPermissionError(error);\n        if (det.isSandbox) {\n          status = 500;\n          code = 'sandbox_denied';\n          type = 'server_error';\n          if (!message || message === 'Internal Server Error') {\n            message = 'Operation denied by sandbox or permission policy';\n          }\n        }\n      } catch { /* ignore */ }\n\n      res.status(status).json({\n        error: {\n          message,\n          type,\n          code,\n          ...(process.env.NODE_ENV === 'development' && { stack: error.stack }),\n        },\n      });\n    });\n  }\n\n  /**\n   * Setup graceful shutdown\n   */\n  private setupGracefulShutdown(): void {\n    const shutdown = async (signal: string) => {\n      console.log(`🛑 Received ${signal}, shutting down gracefully...`);\n\n      try {\n        await this.stop();\n        process.exit(0);\n      } catch (error) {\n        console.error('Error during graceful shutdown:', error);\n        process.exit(1);\n      }\n    };\n\n    process.on('SIGTERM', () => shutdown('SIGTERM'));\n    process.on('SIGINT', () => shutdown('SIGINT'));\n\n    // Handle uncaught exceptions\n    process.on('uncaughtException', async error => {\n      try {\n        console.error('Uncaught Exception:', error);\n        await this.handleError(error, 'uncaught_exception');\n        // Do NOT exit; keep the server alive to avoid interrupting in-flight sessions.\n      } catch { /* ignore */ }\n    });\n\n    // Handle unhandled promise rejections (log and continue; do NOT exit process)\n    process.on('unhandledRejection', async (reason, promise) => {\n      try {\n        console.error('Unhandled Rejection at:', promise, 'reason:', reason);\n        await this.handleError(\n          reason instanceof Error ? reason : new Error(String(reason)),\n          'unhandled_rejection'\n        );\n      } catch { /* ignore */ }\n    });\n  }\n\n  /**\n   * Handle health check\n   */\n  private handleHealthCheck(req: Request, res: Response): void {\n    const status = this.getStatus();\n\n    // Update health status\n    this.healthStatus = {\n      status: status.status,\n      timestamp: new Date().toISOString(),\n      uptime: status.uptime,\n      memory: status.memory,\n      providers: status.providers,\n    };\n\n    const httpStatus = status.status === 'healthy' ? 200 : 503;\n\n    res.status(httpStatus).json({\n      status: status.status,\n      timestamp: this.healthStatus.timestamp,\n      uptime: status.uptime,\n      memory: status.memory,\n      version: this.getModuleInfo().version,\n      providers: status.providers,\n    });\n  }\n\n  /**\n   * Handle readiness check\n   */\n  private handleReadinessCheck(req: Request, res: Response): void {\n    const hasManager = !!this.pipelineManager;\n    const hasRoutePools = !!this.routePools && Object.keys(this.routePools || {}).length > 0;\n    const isReady = this._isInitialized && this._isRunning && hasManager && hasRoutePools;\n    const status = isReady ? 'ready' : 'not_ready';\n\n    res.status(isReady ? 200 : 503).json({\n      status,\n      timestamp: new Date().toISOString(),\n      checks: {\n        initialized: this._isInitialized,\n        running: this._isRunning,\n        pipelineManagerAttached: hasManager,\n        routePoolsAttached: hasRoutePools,\n        routePoolsCount: hasRoutePools ? Object.values(this.routePools || {}).reduce((a:number,b:any)=>a+((b||[]).length),0) : 0,\n        providers: this.getUnifiedProvidersHealth(),\n      },\n    });\n  }\n\n  /**\n   * Merge provider health from ProviderManager with pipeline-derived provider health\n   * so that /health reflects active pipelines in the new architecture.\n   */\n  private getUnifiedProvidersHealth(): Record<string, ProviderHealth> {\n    const pm = this.providerManager?.getAllProvidersHealth?.() || {};\n    const out: Record<string, ProviderHealth> = { ...pm };\n\n    try {\n      if (this.pipelineManager && typeof (this.pipelineManager as any).getPipelineStatus === 'function') {\n        const statusMap = (this.pipelineManager as any).getPipelineStatus() as Record<string, any>;\n        const meta = (this.routeMeta || {}) as Record<string, { providerId: string; modelId: string; keyId: string }>;\n        for (const [pipelineId, st] of Object.entries(statusMap || {})) {\n          const providerKey = meta[pipelineId]?.providerId || pipelineId;\n          const mod = (st as any)?.modules?.provider as { type?: string; state?: string } | undefined;\n          const ok = (mod?.state || '').toLowerCase() === 'ready';\n          if (!out[providerKey]) {\n            out[providerKey] = {\n              status: ok ? 'healthy' : 'unknown',\n              consecutiveFailures: 0,\n              lastCheck: new Date().toISOString(),\n            };\n          }\n        }\n      }\n    } catch {\n      // non-fatal: keep pm-only map\n    }\n    return out;\n  }\n\n  /**\n   * Handle liveness check\n   */\n  private handleLivenessCheck(req: Request, res: Response): void {\n    const isAlive = this._isRunning;\n\n    res.status(isAlive ? 200 : 503).json({\n      status: isAlive ? 'alive' : 'not_alive',\n      timestamp: new Date().toISOString(),\n      uptime: this.getUptime(),\n    });\n  }\n\n  /**\n   * Handle metrics\n   */\n  private async handleMetrics(req: Request, res: Response): Promise<void> {\n    const config = await this.getServerConfig();\n    const metrics = {\n      server: {\n        uptime: this.getUptime(),\n        memory: process.memoryUsage(),\n        cpu: process.cpuUsage(),\n        version: this.getModuleInfo().version,\n        node_version: process.version,\n      },\n      requests: {\n        // Add request metrics here if needed\n      },\n      providers: this.providerManager.getMetrics(),\n      config: {\n        provider_count: Object.keys(config.providers).length,\n        enabled_providers: Object.values(config.providers).filter((p: UnknownObject) => (p as UnknownObject).enabled).length,\n      },\n    };\n\n    res.json(metrics);\n  }\n\n  /**\n   * Handle status\n   */\n  private handleStatus(req: Request, res: Response): void {\n    res.json(this.getStatus());\n  }\n\n  /**\n   * Handle configuration\n   */\n  private async handleConfig(req: Request, res: Response): Promise<void> {\n    const config = await this.getServerConfig();\n\n    // Return sanitized configuration (without sensitive data)\n    const sanitizedConfig = {\n      server: config.server,\n      logging: config.logging,\n      routing: config.routing,\n      providers: Object.keys(config.providers).reduce((acc: UnknownObject, key: string) => {\n        acc[key] = {\n          type: config.providers[key].type,\n          enabled: config.providers[key].enabled,\n          models: Object.keys(config.providers[key].models || {}),\n        };\n        return acc;\n      }, {}),\n    };\n\n    res.json(sanitizedConfig);\n  }\n\n  /**\n   * Handle debug information\n   */\n  private handleDebug(req: Request, res: Response): void {\n    if (process.env.NODE_ENV !== 'development') {\n      res.status(403).json({\n        error: {\n          message: 'Debug endpoint only available in development mode',\n          type: 'forbidden_error',\n          code: 'debug_forbidden',\n        },\n      });\n      return;\n    }\n\n    const debugInfo = {\n      server: this.getStatus(),\n      modules: {\n        httpserver: {\n          id: 'http-server',\n          name: 'HTTP Server',\n          version: '1.0.0',\n          status: this.getStatus(),\n        },\n        requestHandler: this.requestHandler.getInfo(),\n        providerManager: this.providerManager.getInfo(),\n        protocolRouter: this.protocolHandler.getInfo(),\n      },\n      environment: {\n        node_version: process.version,\n        platform: process.platform,\n        arch: process.arch,\n        memory_usage: process.memoryUsage(),\n        uptime: process.uptime(),\n      },\n      configuration: this.getServerConfig(),\n    };\n\n    res.json(debugInfo);\n  }\n\n  /**\n   * Handle test error\n   */\n  private async handleTestError(req: Request, res: Response): Promise<void> {\n    try {\n      // Simulate an error for testing\n      const testError = new Error('This is a test error for error handling validation');\n      await this.handleError(testError, 'test_error_endpoint');\n\n      res.json({\n        message: 'Test error processed successfully',\n        error: testError.message,\n        timestamp: new Date().toISOString(),\n      });\n    } catch (error) {\n      res.status(500).json({\n        error: {\n          message: 'Failed to process test error',\n          type: 'test_error_failed',\n        },\n      });\n    }\n  }\n\n  /**\n   * Get server uptime\n   */\n  private getUptime(): number {\n    return this._isRunning ? Math.floor((Date.now() - this.startupTime) / 1000) : 0;\n  }\n\n  /**\n   * Handle error with enhanced error handling system\n   */\n  private async handleError(error: Error, context: string): Promise<void> {\n    try {\n      // Use enhanced error handling utilities\n      await this.errorUtils.handle(error, context, {\n        severity: this.getErrorSeverity(context),\n        category: this.getErrorCategory(context),\n        additionalContext: {\n          uptime: this.getUptime(),\n          memory: process.memoryUsage(),\n          isRunning: this._isRunning,\n          isInitialized: this._isInitialized,\n        },\n      });\n    } catch (handlerError) {\n      console.error('Failed to handle error:', handlerError);\n      console.error('Original error:', error);\n    }\n  }\n\n  /**\n   * Get error severity based on context\n   */\n  private getErrorSeverity(context: string): 'low' | 'medium' | 'high' | 'critical' {\n    const criticalContexts = [\n      'initialization',\n      'server_start',\n      'server_stop',\n      'uncaught_exception',\n      'unhandled_rejection',\n    ];\n    const highContexts = ['configuration', 'provider_health', 'memory'];\n    const mediumContexts = ['request_handler', 'middleware', 'route'];\n\n    if (criticalContexts.some(c => context.includes(c))) {\n      return 'critical';\n    }\n    if (highContexts.some(c => context.includes(c))) {\n      return 'high';\n    }\n    if (mediumContexts.some(c => context.includes(c))) {\n      return 'medium';\n    }\n    return 'low';\n  }\n\n  /**\n   * Get error category based on context\n   */\n  private getErrorCategory(context: string): string {\n    const categories: Record<string, string> = {\n      initialization: 'system',\n      server_start: 'server',\n      server_stop: 'server',\n      request_handler: 'request',\n      middleware: 'server',\n      route: 'request',\n      configuration: 'configuration',\n      provider_health: 'provider',\n      memory: 'system',\n      uncaught_exception: 'system',\n      unhandled_rejection: 'system',\n    };\n\n    for (const [key, category] of Object.entries(categories)) {\n      if (context.includes(key)) {\n        return category;\n      }\n    }\n    return 'general';\n  }\n}\n"
        },
        "src/server/protocol/anthropic-adapter.ts": {
          "path": "src/server/protocol/anthropic-adapter.ts",
          "size": 3346,
          "lines": 74,
          "imports": [
            "./protocol-detector.js"
          ],
          "exports": [
            "AnthropicAdapter"
          ],
          "classes": [
            "AnthropicAdapter"
          ],
          "functions": [],
          "content": "import type { IProtocolAdapter, ProtocolAdapterConfig } from './protocol-detector.js';\n\nexport class AnthropicAdapter implements IProtocolAdapter {\n  private readonly config: ProtocolAdapterConfig;\n\n  constructor(config: ProtocolAdapterConfig = {}) {\n    this.config = {\n      name: 'anthropic',\n      version: '1.0.0',\n      supportedEndpoints: ['/v1/messages', '/v1/responses'],\n      defaultHeaders: { 'Content-Type': 'application/json', 'anthropic-version': '2023-06-01' },\n      ...config\n    };\n  }\n\n  detectRequest(request: unknown): boolean {\n    if (!request || typeof request !== 'object') {return false;}\n    const r = request as Record<string, unknown>;\n    return typeof r.system === 'string' || (Array.isArray(r.messages) && this.isAnthropicMessageArray(r.messages as any[]));\n  }\n\n  detectResponse(response: unknown): boolean {\n    if (!response || typeof response !== 'object') {return false;}\n    const r = response as Record<string, unknown>;\n    return r.type === 'message' || Array.isArray(r.content);\n  }\n\n  normalizeRequest(request: unknown): unknown {\n    if (!request || typeof request !== 'object') {return {};}\n    return { ...(request as Record<string, unknown>) };\n  }\n\n  normalizeResponse(response: unknown): unknown {\n    if (!response || typeof response !== 'object') {return {};}\n    return { ...(response as Record<string, unknown>) };\n  }\n\n  // Convert OpenAI → Anthropic\n  convertFromProtocol(request: unknown, sourceProtocol: string): unknown {\n    if (sourceProtocol !== 'openai') {return this.normalizeRequest(request);}\n    try {\n      const { AnthropicOpenAIConverter } = require('rcc-llmswitch-core/llmswitch/anthropic-openai-converter');\n      const { PipelineDebugLogger } = require('../../modules/pipeline/utils/debug-logger.js');\n      const logger = new PipelineDebugLogger(null, { enableConsoleLogging: false, enableDebugCenter: false });\n      const deps = { errorHandlingCenter: {}, debugCenter: {}, logger } as any;\n      const conv = new AnthropicOpenAIConverter({ type: 'llmswitch-anthropic-openai', config: {} }, deps);\n      if (typeof conv.initialize === 'function') { (conv as any).initialize(); }\n      return (conv as any).convertOpenAIRequestToAnthropic(request);\n    } catch {\n      return this.normalizeRequest(request);\n    }\n  }\n\n  // Convert Anthropic → OpenAI\n  convertToProtocol(request: unknown, targetProtocol: string): unknown {\n    if (targetProtocol !== 'openai') {return this.normalizeRequest(request);}\n    try {\n      const { AnthropicOpenAIConverter } = require('rcc-llmswitch-core/llmswitch/anthropic-openai-converter');\n      const { PipelineDebugLogger } = require('../../modules/pipeline/utils/debug-logger.js');\n      const logger = new PipelineDebugLogger(null, { enableConsoleLogging: false, enableDebugCenter: false });\n      const deps = { errorHandlingCenter: {}, debugCenter: {}, logger } as any;\n      const conv = new AnthropicOpenAIConverter({ type: 'llmswitch-anthropic-openai', config: {} }, deps);\n      if (typeof conv.initialize === 'function') { (conv as any).initialize(); }\n      return (conv as any).convertAnthropicRequestToOpenAI(request);\n    } catch {\n      return this.normalizeRequest(request);\n    }\n  }\n\n  private isAnthropicMessageArray(messages: any[]): boolean {\n    return messages.some((m) => m && Array.isArray(m.content));\n  }\n}\n"
        },
        "src/server/protocol/openai-adapter.ts": {
          "path": "src/server/protocol/openai-adapter.ts",
          "size": 3519,
          "lines": 86,
          "imports": [
            "./protocol-detector.js"
          ],
          "exports": [
            "OpenAIAdapter"
          ],
          "classes": [
            "OpenAIAdapter"
          ],
          "functions": [],
          "content": "import type { IProtocolAdapter, ProtocolAdapterConfig } from './protocol-detector.js';\n\n// Use the pipeline converter to perform robust mapping between OpenAI and Anthropic\nexport class OpenAIAdapter implements IProtocolAdapter {\n  private readonly config: ProtocolAdapterConfig;\n\n  constructor(config: ProtocolAdapterConfig = {}) {\n    this.config = {\n      name: 'openai',\n      version: '1.0.0',\n      supportedEndpoints: ['/v1/chat/completions', '/v1/completions', '/v1/embeddings'],\n      defaultHeaders: { 'Content-Type': 'application/json' },\n      ...config\n    };\n  }\n\n  detectRequest(request: unknown): boolean {\n    if (!request || typeof request !== 'object') {return false;}\n    const r = request as Record<string, unknown>;\n    return Array.isArray(r.messages) || typeof r.prompt === 'string';\n  }\n\n  detectResponse(response: unknown): boolean {\n    if (!response || typeof response !== 'object') {return false;}\n    const r = response as Record<string, unknown>;\n    return Array.isArray(r.choices) || r.object === 'list' || r.object === 'chat.completion';\n  }\n\n  normalizeRequest(request: unknown): unknown {\n    if (!request || typeof request !== 'object') {return {};}\n    const r = request as Record<string, unknown>;\n    // Ensure assistant tool_calls use stringified args\n    if (Array.isArray(r.messages)) {\n      const msgs = (r.messages as any[]).map((m) => {\n        if (m && m.role === 'assistant' && Array.isArray(m.tool_calls)) {\n          const tcs = m.tool_calls.map((tc: any) => {\n            if (tc && tc.function && typeof tc.function === 'object' && typeof tc.function.arguments !== 'string') {\n              try { tc.function.arguments = JSON.stringify(tc.function.arguments); } catch { tc.function.arguments = String(tc.function.arguments); }\n            }\n            return tc;\n          });\n          return { ...m, tool_calls: tcs };\n        }\n        return m;\n      });\n      return { ...r, messages: msgs };\n    }\n    return { ...r };\n  }\n\n  normalizeResponse(response: unknown): unknown {\n    if (!response || typeof response !== 'object') {return {};}\n    return { ...(response as Record<string, unknown>) };\n  }\n\n  // Convert Anthropic → OpenAI (sourceProtocol='anthropic')\n  convertFromProtocol(request: unknown, sourceProtocol: string): unknown {\n    if (sourceProtocol !== 'anthropic') {return this.normalizeRequest(request);}\n    const r = (request && typeof request === 'object') ? (request as Record<string, unknown>) : {};\n    const out: any = { ...r };\n    if (Array.isArray(out.messages)) {\n      out.messages = (out.messages as any[]).map((m: any) => {\n        if (!m || typeof m !== 'object') {return m;}\n        const role = typeof m.role === 'string' ? m.role : 'user';\n        const c = (m as any).content;\n        if (Array.isArray(c)) {\n          const text = c\n            .map((p: any) => (p && typeof p === 'object' && typeof p.text === 'string') ? p.text : (typeof p === 'string' ? p : ''))\n            .filter((s: string) => !!String(s).trim())\n            .join('\\n');\n          return { role, content: text };\n        }\n        return m;\n      });\n    }\n    return this.normalizeRequest(out);\n  }\n\n  // Convert OpenAI → Anthropic (targetProtocol='anthropic')\n  convertToProtocol(request: unknown, targetProtocol: string): unknown {\n    if (targetProtocol !== 'anthropic') {return this.normalizeRequest(request);}\n    // Minimal passthrough: no-op since we primarily consume OpenAI Chat internally\n    return this.normalizeRequest(request);\n  }\n}\n"
        },
        "src/server/protocol/protocol-detector.ts": {
          "path": "src/server/protocol/protocol-detector.ts",
          "size": 13387,
          "lines": 495,
          "imports": [
            "express"
          ],
          "exports": [
            "ProtocolAdapterConfig",
            "IProtocolAdapter",
            "DetectionResult",
            "ProtocolDetectorConfig",
            "ProtocolDetector"
          ],
          "classes": [
            "ProtocolDetector"
          ],
          "functions": [],
          "content": "/**\n * Protocol Detector Implementation\n * Automatically detects the protocol format of incoming requests\n */\n\nimport type { Request } from 'express';\n\n/**\n * Protocol type enumeration\n */\nexport type ProtocolType = 'openai' | 'anthropic' | 'responses' | 'unknown';\n\nexport interface ProtocolAdapterConfig {\n  name?: string;\n  version?: string;\n  supportedEndpoints?: string[];\n  defaultHeaders?: Record<string, string>;\n}\n\nexport interface IProtocolAdapter {\n  detectRequest(payload: unknown): boolean;\n  detectResponse(payload: unknown): boolean;\n  normalizeRequest(payload: unknown): unknown;\n  normalizeResponse(payload: unknown): unknown;\n  convertToProtocol(payload: unknown, targetProtocol: ProtocolType): unknown;\n  convertFromProtocol(payload: unknown, sourceProtocol: ProtocolType): unknown;\n}\n\n/**\n * Detection result interface\n */\nexport interface DetectionResult {\n  protocol: ProtocolType;\n  confidence: number;\n  endpoint?: string;\n  indicators: string[];\n}\n\nexport interface ProtocolDetectorConfig {\n  enableStrictDetection?: boolean;\n  minConfidenceThreshold?: number;\n  customPatterns?: Record<string, RegExp>;\n}\n\n/**\n * Protocol Detector\n * Automatically detects request protocol format based on content and headers\n */\nexport class ProtocolDetector {\n  private config: ProtocolDetectorConfig;\n\n  constructor(config: ProtocolDetectorConfig = {}) {\n    this.config = {\n      enableStrictDetection: false,\n      minConfidenceThreshold: 0.7,\n      customPatterns: {},\n      ...config,\n    };\n  }\n\n  /**\n   * Detect protocol from Express request\n   */\n  public detectFromRequest(req: Request): DetectionResult {\n    const body = req.body || {};\n    const headers = req.headers || {};\n    const url = req.url || '';\n    const method = req.method || '';\n\n    return this.detectProtocol(body, headers, url, method);\n  }\n\n  /**\n   * Detect protocol from request data\n   */\n  public detectProtocol(\n    body: any,\n    headers: Record<string, any> = {},\n    url: string = '',\n    method: string = ''\n  ): DetectionResult {\n    const indicators: string[] = [];\n    let confidence = 0;\n    let protocol: ProtocolType = 'unknown';\n\n    // Check URL-based indicators\n    const urlIndicators = this.analyzeUrl(url);\n    indicators.push(...urlIndicators.indicators);\n    confidence += urlIndicators.confidence;\n\n    // Check header-based indicators\n    const headerIndicators = this.analyzeHeaders(headers);\n    indicators.push(...headerIndicators.indicators);\n    confidence += headerIndicators.confidence;\n\n    // Check body-based indicators\n    const bodyIndicators = this.analyzeBody(body);\n    indicators.push(...bodyIndicators.indicators);\n    confidence += bodyIndicators.confidence;\n\n    // Determine protocol based on strongest indicators\n    const protocolGuess = this.determineProtocol(indicators);\n\n    // Normalize confidence\n    confidence = Math.min(confidence, 1.0);\n    protocol = confidence >= (this.config.minConfidenceThreshold || 0.7)\n      ? protocolGuess\n      : 'unknown';\n\n    return {\n      protocol,\n      confidence,\n      endpoint: url,\n      indicators: [...new Set(indicators)], // Remove duplicates\n    };\n  }\n\n  /**\n   * Analyze URL for protocol indicators\n   */\n  private analyzeUrl(url: string): { indicators: string[]; confidence: number } {\n    const indicators: string[] = [];\n    let confidence = 0;\n\n    if (!url) {\n      return { indicators, confidence };\n    }\n\n    // OpenAI endpoints\n    if (url.includes('/v1/chat/completions')) {\n      indicators.push('openai_chat_completions_endpoint');\n      confidence += 0.4;\n    } else if (url.includes('/v1/completions')) {\n      indicators.push('openai_completions_endpoint');\n      confidence += 0.4;\n    } else if (url.includes('/v1/embeddings')) {\n      indicators.push('openai_embeddings_endpoint');\n      confidence += 0.3;\n    } else if (url.includes('/v1/models')) {\n      indicators.push('openai_models_endpoint');\n      confidence += 0.2;\n    }\n\n    // Anthropic endpoints\n    if (url.includes('/v1/messages')) {\n      indicators.push('anthropic_messages_endpoint');\n      confidence += 0.4;\n    } else if (url.includes('/v1/responses')) {\n      indicators.push('anthropic_responses_endpoint');\n      confidence += 0.4;\n    }\n\n    // Generic API indicators\n    if (url.startsWith('/v1/')) {\n      indicators.push('openai_style_api');\n      confidence += 0.1;\n    }\n\n    return { indicators, confidence };\n  }\n\n  /**\n   * Analyze headers for protocol indicators\n   */\n  private analyzeHeaders(headers: Record<string, any>): { indicators: string[]; confidence: number } {\n    const indicators: string[] = [];\n    let confidence = 0;\n\n    // Content-Type headers\n    const contentType = headers['content-type'] || headers['Content-Type'] || '';\n    if (contentType.includes('application/json')) {\n      indicators.push('json_content_type');\n      confidence += 0.1;\n    }\n\n    // Authorization headers\n    const authHeader = headers.authorization || headers.Authorization || '';\n    if (authHeader.startsWith('Bearer sk-')) {\n      indicators.push('openai_style_auth');\n      confidence += 0.2;\n    } else if (authHeader.startsWith('Bearer sk-ant-')) {\n      indicators.push('anthropic_style_auth');\n      confidence += 0.3;\n    }\n\n    // User-Agent headers\n    const userAgent = headers['user-agent'] || headers['User-Agent'] || '';\n    if (userAgent.includes('OpenAI') || userAgent.includes('GPT')) {\n      indicators.push('openai_user_agent');\n      confidence += 0.1;\n    } else if (userAgent.includes('Anthropic') || userAgent.includes('Claude')) {\n      indicators.push('anthropic_user_agent');\n      confidence += 0.1;\n    }\n\n    // Custom headers\n    if (headers['anthropic-version']) {\n      indicators.push('anthropic_version_header');\n      confidence += 0.3;\n    }\n\n    if (headers['x-api-version'] || headers['api-version']) {\n      indicators.push('api_version_header');\n      confidence += 0.1;\n    }\n\n    return { indicators, confidence };\n  }\n\n  /**\n   * Analyze request body for protocol indicators\n   */\n  private analyzeBody(body: any): { indicators: string[]; confidence: number } {\n    const indicators: string[] = [];\n    let confidence = 0;\n\n    if (!body || typeof body !== 'object') {\n      return { indicators, confidence };\n    }\n\n    const bodyKeys = Object.keys(body);\n\n    // OpenAI-specific indicators\n    if (body.messages && Array.isArray(body.messages)) {\n      indicators.push('openai_messages_array');\n      confidence += 0.3;\n    }\n\n    if (body.model && typeof body.model === 'string') {\n      indicators.push('model_field_present');\n      confidence += 0.1;\n    }\n\n    if (body.temperature !== undefined || body.top_p !== undefined) {\n      indicators.push('openai_sampling_params');\n      confidence += 0.2;\n    }\n\n    if (body.max_tokens !== undefined) {\n      indicators.push('max_tokens_field');\n      confidence += 0.1;\n    }\n\n    if (body.tools && Array.isArray(body.tools)) {\n      indicators.push('openai_tools_array');\n      confidence += 0.2;\n    }\n\n    if (body.tool_choice !== undefined) {\n      indicators.push('openai_tool_choice');\n      confidence += 0.1;\n    }\n\n    if (body.stream !== undefined) {\n      indicators.push('stream_field_present');\n      confidence += 0.1;\n    }\n\n    // Anthropic-specific indicators\n    if (body.system !== undefined) {\n      indicators.push('anthropic_system_field');\n      confidence += 0.2;\n    }\n\n    if (body.max_tokens !== undefined && typeof body.max_tokens === 'number') {\n      // Anthropic requires max_tokens\n      confidence += 0.1;\n    }\n\n    // Check for Anthropic-style message structure\n    if (body.messages && Array.isArray(body.messages)) {\n      const hasAnthropicStructure = body.messages.some((msg: any) =>\n        msg && typeof msg === 'object' &&\n        msg.role &&\n        Array.isArray(msg.content) &&\n        msg.content.some((content: any) =>\n          content && typeof content === 'object' &&\n          content.type\n        )\n      );\n\n      if (hasAnthropicStructure) {\n        indicators.push('anthropic_content_structure');\n        confidence += 0.3;\n      }\n    }\n\n    // Completion-specific indicators\n    if (body.prompt !== undefined) {\n      indicators.push('prompt_field_present');\n      confidence += 0.2;\n    }\n\n    if (body.echo !== undefined) {\n      indicators.push('completion_echo_field');\n      confidence += 0.1;\n    }\n\n    if (body.logprobs !== undefined) {\n      indicators.push('completion_logprobs_field');\n      confidence += 0.1;\n    }\n\n    // Responses-specific indicators\n    if (body.response_format !== undefined) {\n      indicators.push('response_format_field');\n      confidence += 0.1;\n    }\n\n    if (body.tools && body.tools.some((tool: any) => tool.input_schema)) {\n      indicators.push('anthropic_tool_schema');\n      confidence += 0.2;\n    }\n\n    return { indicators, confidence };\n  }\n\n  /**\n   * Determine protocol based on indicators\n   */\n  private determineProtocol(indicators: string[]): ProtocolType {\n    const openaiIndicators = [\n      'openai_chat_completions_endpoint',\n      'openai_completions_endpoint',\n      'openai_embeddings_endpoint',\n      'openai_models_endpoint',\n      'openai_style_auth',\n      'openai_user_agent',\n      'openai_messages_array',\n      'openai_sampling_params',\n      'openai_tools_array',\n      'openai_tool_choice',\n    ];\n\n    const anthropicIndicators = [\n      'anthropic_messages_endpoint',\n      'anthropic_responses_endpoint',\n      'anthropic_style_auth',\n      'anthropic_user_agent',\n      'anthropic_version_header',\n      'anthropic_system_field',\n      'anthropic_content_structure',\n      'anthropic_tool_schema',\n    ];\n\n    const responsesIndicators = [\n      'anthropic_responses_endpoint',\n      'response_format_field',\n    ];\n\n    const openaiScore = indicators.filter(i => openaiIndicators.includes(i)).length;\n    const anthropicScore = indicators.filter(i => anthropicIndicators.includes(i)).length;\n    const responsesScore = indicators.filter(i => responsesIndicators.includes(i)).length;\n\n    // Determine protocol with highest score\n    if (responsesScore > 0 && responsesScore >= Math.max(openaiScore, anthropicScore)) {\n      return 'responses';\n    } else if (anthropicScore > openaiScore) {\n      return 'anthropic';\n    } else if (openaiScore > 0) {\n      return 'openai';\n    }\n\n    return 'unknown';\n  }\n\n  /**\n   * Validate detected protocol\n   */\n  public validateDetection(result: DetectionResult, body: any): boolean {\n    if (result.protocol === 'unknown') {\n      return false;\n    }\n\n    try {\n      switch (result.protocol) {\n        case 'openai':\n          return this.validateOpenAIFormat(body);\n        case 'anthropic':\n          return this.validateAnthropicFormat(body);\n        case 'responses':\n          return this.validateResponsesFormat(body);\n        default:\n          return false;\n      }\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Validate OpenAI format\n   */\n  private validateOpenAIFormat(body: any): boolean {\n    if (!body || typeof body !== 'object') {\n      return false;\n    }\n\n    // Check for required fields based on endpoint type\n    if (body.messages) {\n      // Chat completions format\n      return Array.isArray(body.messages) &&\n             body.messages.length > 0 &&\n             body.messages.every((msg: any) =>\n               msg &&\n               typeof msg === 'object' &&\n               typeof msg.role === 'string' &&\n               msg.content !== undefined\n             );\n    } else if (body.prompt) {\n      // Completions format\n      return typeof body.prompt === 'string' || Array.isArray(body.prompt);\n    }\n\n    return false;\n  }\n\n  /**\n   * Validate Anthropic format\n   */\n  private validateAnthropicFormat(body: any): boolean {\n    if (!body || typeof body !== 'object') {\n      return false;\n    }\n\n    // Check for required Anthropic fields\n    if (!Array.isArray(body.messages) || body.messages.length === 0) {\n      return false;\n    }\n\n    if (typeof body.max_tokens !== 'number' || body.max_tokens <= 0) {\n      return false;\n    }\n\n    // Validate message structure\n    return body.messages.every((msg: any) =>\n      msg &&\n      typeof msg === 'object' &&\n      typeof msg.role === 'string' &&\n      ['user', 'assistant', 'system'].includes(msg.role) &&\n      msg.content !== undefined\n    );\n  }\n\n  /**\n   * Validate Responses format\n   */\n  private validateResponsesFormat(body: any): boolean {\n    if (!body || typeof body !== 'object') {\n      return false;\n    }\n\n    // Responses format is similar to Anthropic but may have additional fields\n    if (!Array.isArray(body.messages) || body.messages.length === 0) {\n      return false;\n    }\n\n    if (typeof body.max_tokens !== 'number' || body.max_tokens <= 0) {\n      return false;\n    }\n\n    return true;\n  }\n\n  /**\n   * Get protocol statistics\n   */\n  public getDetectionStats(results: DetectionResult[]): {\n    total: number;\n    byProtocol: Record<ProtocolType, number>;\n    avgConfidence: number;\n  } {\n    const total = results.length;\n    const byProtocol = {\n      openai: 0,\n      anthropic: 0,\n      responses: 0,\n      unknown: 0,\n    };\n\n    let totalConfidence = 0;\n\n    results.forEach(result => {\n      byProtocol[result.protocol]++;\n      totalConfidence += result.confidence;\n    });\n\n    return {\n      total,\n      byProtocol,\n      avgConfidence: total > 0 ? totalConfidence / total : 0,\n    };\n  }\n}\n"
        },
        "src/server/protocol-handler.ts": {
          "path": "src/server/protocol-handler.ts",
          "size": 37618,
          "lines": 810,
          "imports": [
            "express",
            "axios",
            "rcc-basemodule",
            "../core/request-handler.js",
            "../core/provider-manager.js",
            "../utils/module-config-reader.js",
            "./handlers/chat-completions.js",
            "./handlers/completions.js",
            "./handlers/embeddings.js",
            "./handlers/models.js",
            "./handlers/messages.js",
            "./handlers/responses.js",
            "./streaming/openai-streamer.js",
            "./streaming/anthropic-streamer.js",
            "./streaming/responses-streamer.js",
            "../modules/monitoring/monitor-config.js",
            "../modules/virtual-router/classifiers/config-request-classifier.js"
          ],
          "exports": [
            "ProtocolHandlerConfig",
            "ProtocolHandler",
            "TransparentHeaders",
            "TransparentConfig",
            "MonitorLikeConfig",
            "ProtocolHandler",
            "ProtocolHandler"
          ],
          "classes": [
            "ProtocolHandler",
            "with"
          ],
          "functions": [
            "makeOpenAIChunks",
            "makeAnthropicChunks",
            "tryTransparentOpenAI",
            "tryTransparentAnthropic",
            "joinUrl",
            "fireMonitorAB",
            "tryBridgeResponsesToChat"
          ],
          "content": "import express, { type Request, type Response, type Router } from 'express';\nimport axios from 'axios';\nimport { BaseModule, type ModuleInfo } from 'rcc-basemodule';\n\nimport { RequestHandler } from '../core/request-handler.js';\nimport { ProviderManager } from '../core/provider-manager.js';\nimport { ModuleConfigReader } from '../utils/module-config-reader.js';\nimport {\n  ServiceContainer,\n  initializeDefaultServices,\n  ServiceTokens\n} from './core/service-container.js';\nimport { ChatCompletionsHandler } from './handlers/chat-completions.js';\nimport { CompletionsHandler } from './handlers/completions.js';\nimport { EmbeddingsHandler } from './handlers/embeddings.js';\nimport { ModelsHandler } from './handlers/models.js';\nimport { MessagesHandler } from './handlers/messages.js';\nimport { ResponsesHandler } from './handlers/responses.js';\nimport { OpenAIStreamer } from './streaming/openai-streamer.js';\nimport { AnthropicStreamer } from './streaming/anthropic-streamer.js';\nimport { ResponsesStreamer } from './streaming/responses-streamer.js';\nimport { MonitorConfigUtil } from '../modules/monitoring/monitor-config.js';\nimport { ConfigRequestClassifier, type ConfigClassifierConfig, type ConfigClassificationInput } from '../modules/virtual-router/classifiers/config-request-classifier.js';\n\nexport interface ProtocolHandlerConfig {\n  enableStreaming?: boolean;\n  enableMetrics?: boolean;\n  enableValidation?: boolean;\n  rateLimitEnabled?: boolean;\n  authEnabled?: boolean;\n  targetUrl?: string;\n  timeout?: number;\n  enablePipeline?: boolean;\n  pipelineProvider?: {\n    defaultProvider: string;\n    modelMapping: Record<string, string>;\n  };\n}\n\ntype HandlerMap = {\n  chat: ChatCompletionsHandler;\n  completions: CompletionsHandler;\n  embeddings: EmbeddingsHandler;\n  models: ModelsHandler;\n  messages: MessagesHandler;\n  responses: ResponsesHandler;\n};\n\nconst DEFAULT_CONFIG: Required<Pick<ProtocolHandlerConfig,\n  'enableStreaming' | 'enableMetrics' | 'enableValidation' | 'rateLimitEnabled' | 'authEnabled' | 'timeout' | 'enablePipeline'>> = {\n  enableStreaming: true,\n  enableMetrics: true,\n  enableValidation: true,\n  rateLimitEnabled: false,\n  authEnabled: false,\n  timeout: 30000,\n  enablePipeline: false\n};\n\nexport class ProtocolHandler extends BaseModule {\n  private readonly router: Router;\n  private readonly serviceContainer: ServiceContainer;\n  private readonly handlers: HandlerMap;\n  private readonly config: ProtocolHandlerConfig;\n\n  private initialized = false;\n  private pipelineManager: unknown = null;\n  private routePools: Record<string, string[]> | null = null;\n  private routeMeta: Record<string, { providerId: string; modelId: string; keyId?: string }> | null = null;\n  private classifierConfig: Record<string, unknown> | null = null;\n  private authMappings: Record<string, string> | null = null;\n  private classifier: ConfigRequestClassifier | null = null;\n  private classifierAdapter: { classify: (payload: unknown) => Promise<unknown> } | null = null;\n\n  constructor(\n    _requestHandler: RequestHandler,\n    _providerManager: ProviderManager,\n    _moduleConfigReader: ModuleConfigReader,\n    config: ProtocolHandlerConfig = {}\n  ) {\n    const moduleInfo: ModuleInfo = {\n      id: 'protocol-handler',\n      name: 'ProtocolHandler',\n      version: '1.0.0',\n      description: 'Modular protocol entrypoint',\n      type: 'server'\n    };\n\n    super(moduleInfo);\n\n    this.router = express.Router();\n    this.config = { ...DEFAULT_CONFIG, ...config };\n    this.serviceContainer = ServiceContainer.getInstance();\n    initializeDefaultServices(this.serviceContainer);\n\n    // Enable pipeline specifically for Chat endpoint to route via llmswitch → OpenAI\n    this.handlers = {\n      chat: new ChatCompletionsHandler({ ...this.config, enablePipeline: true }),\n      completions: new CompletionsHandler(this.config),\n      embeddings: new EmbeddingsHandler(this.config),\n      models: new ModelsHandler(this.config),\n      messages: new MessagesHandler(this.config),\n      responses: new ResponsesHandler(this.config)\n    };\n\n    this.registerRoutes();\n  }\n\n  public async initialize(): Promise<void> {\n    this.initialized = true;\n  }\n\n  public getRouter(): Router {\n    return this.router;\n  }\n\n  public async stop(): Promise<void> {\n    this.initialized = false;\n  }\n\n  public attachPipelineManager(pipelineManager: unknown): void {\n    this.pipelineManager = pipelineManager;\n    Object.values(this.handlers).forEach(handler => handler.attachPipelineManager(pipelineManager));\n    try {\n      this.serviceContainer.registerInstance(ServiceTokens.PIPELINE_MANAGER, pipelineManager);\n    } catch { /* ignore duplicate registrations */ }\n  }\n\n  public attachRoutePools(routePools: Record<string, string[]>): void {\n    this.routePools = routePools;\n    Object.values(this.handlers).forEach(handler => handler.attachRoutePools(routePools));\n    try {\n      this.serviceContainer.registerInstance(ServiceTokens.ROUTE_POOLS, routePools);\n    } catch { /* ignore duplicate registrations */ }\n  }\n\n  public attachRouteMeta(routeMeta: Record<string, { providerId: string; modelId: string; keyId: string }>): void {\n    this.routeMeta = routeMeta;\n    Object.values(this.handlers).forEach(handler => handler.attachRouteMeta(routeMeta));\n    try {\n      this.serviceContainer.registerInstance(ServiceTokens.ROUTE_META, routeMeta);\n    } catch { /* ignore duplicate registrations */ }\n  }\n\n  public attachRoutingClassifierConfig(classifierConfig: Record<string, unknown>): void {\n    this.classifierConfig = classifierConfig;\n    try {\n      const typedConfig = classifierConfig as unknown as ConfigClassifierConfig;\n      this.classifier = new ConfigRequestClassifier(typedConfig);\n      this.classifierAdapter = {\n        classify: async (payload: unknown) => {\n          return this.classifier!.classify(payload as ConfigClassificationInput);\n        }\n      };\n    } catch {\n      this.classifier = null;\n      this.classifierAdapter = null;\n    }\n\n    Object.values(this.handlers).forEach(handler => {\n      handler.attachRoutingClassifierConfig(classifierConfig);\n      if (this.classifierAdapter) {\n        handler.attachRoutingClassifier(this.classifierAdapter);\n      }\n    });\n\n    if (this.classifierAdapter) {\n      try {\n        this.serviceContainer.registerInstance(ServiceTokens.ROUTING_CLASSIFIER, this.classifierAdapter);\n      } catch { /* ignore duplicate registrations */ }\n    }\n  }\n\n  public attachAuthMappings(authMappings: Record<string, string>): void {\n    this.authMappings = authMappings;\n  }\n\n  public async streamFromPipeline(\n    response: unknown,\n    requestId: string,\n    res: Response,\n    model?: string,\n    protocol: 'openai' | 'anthropic' | 'responses' = 'openai'\n  ): Promise<void> {\n    const options = { requestId, model: model ?? 'unknown', chunkDelay: 25 };\n\n    // Build synthetic chunk list for non-stream responses\n    const makeOpenAIChunks = (resp: any) => {\n      const chunks: any[] = [];\n      try {\n        const msg = resp?.choices?.[0]?.message || {};\n        const content = typeof msg?.content === 'string' ? msg.content : '';\n        const toolCalls = Array.isArray(msg?.tool_calls) ? msg.tool_calls : [];\n\n        // 1) Emit content as-is to preserve newlines\n        if (content.length > 0) {\n          chunks.push({ metadata: { model: options.model }, content, done: false });\n        }\n\n        // 2) Emit tool_calls as OpenAI-style delta.tool_calls\n        if (toolCalls.length > 0) {\n          for (let i = 0; i < toolCalls.length; i++) {\n            const tc = toolCalls[i] || {};\n            const fn = tc.function || {};\n            const id = typeof tc.id === 'string' ? tc.id : undefined;\n            const name = typeof fn.name === 'string' ? fn.name : undefined;\n            const args = typeof fn.arguments === 'string' ? fn.arguments : (fn.arguments != null ? JSON.stringify(fn.arguments) : undefined);\n\n            // Emit name delta (optional, but helps some clients)\n            if (name) {\n              chunks.push({\n                metadata: { model: options.model },\n                content: '',\n                tool_calls: [{ index: i, id, type: 'function', function: { name } }],\n                done: false,\n              });\n            }\n            // Emit arguments delta as a single chunk (clients accumulate string)\n            if (args) {\n              chunks.push({\n                metadata: { model: options.model },\n                content: '',\n                tool_calls: [{ index: i, id, type: 'function', function: { arguments: args } }],\n                done: false,\n              });\n            }\n          }\n        }\n      } catch { /* ignore */ }\n\n      // 3) Final chunk with usage and finish_reason\n      const finish = (resp?.choices?.[0]?.message?.tool_calls?.length || 0) > 0\n        ? 'tool_calls'\n        : (resp?.choices?.[0]?.finish_reason || resp?.finish_reason || 'stop');\n      chunks.push({ metadata: { model: options.model, usage: resp?.usage, finish_reason: finish }, content: '', done: true });\n      return chunks;\n    };\n\n    const makeAnthropicChunks = (resp: any) => {\n      const chunks: any[] = [];\n      try {\n        const blocks = Array.isArray(resp?.content) ? resp.content : [];\n        for (const b of blocks) {\n          if (b && b.type === 'text' && typeof b.text === 'string') {\n            const words = b.text.split(/\\s+/g);\n            for (const w of words) {\n              chunks.push({ metadata: { model: options.model }, content: `${w  } `, done: false });\n            }\n          }\n        }\n      } catch { /* ignore */ }\n      chunks.push({ metadata: { model: options.model, usage: resp?.usage }, content: '', done: true });\n      return chunks;\n    };\n\n    if (protocol === 'anthropic') {\n      const streamer = new AnthropicStreamer(this.config);\n      const resp = (response && typeof response === 'object' && 'data' in (response as any)) ? (response as any).data : response;\n      const chunks = Array.isArray((resp as any)) || (resp && typeof resp === 'object' && Array.isArray((resp as any).data))\n        ? resp\n        : makeAnthropicChunks(resp);\n      await streamer.streamResponse(chunks, options, res);\n      return;\n    }\n\n    if (protocol === 'responses') {\n      const streamer = new ResponsesStreamer(this.config);\n      const resp = (response && typeof response === 'object' && 'data' in (response as any)) ? (response as any).data : response;\n      const chunks = Array.isArray((resp as any)) || (resp && typeof resp === 'object' && Array.isArray((resp as any).data))\n        ? resp\n        : makeOpenAIChunks(resp);\n      await streamer.streamResponse(chunks, options, res);\n      return;\n    }\n\n    const streamer = new OpenAIStreamer(this.config);\n    const resp = (response && typeof response === 'object' && 'data' in (response as any)) ? (response as any).data : response;\n    const chunks = Array.isArray((resp as any)) || (resp && typeof resp === 'object' && Array.isArray((resp as any).data))\n      ? resp\n      : makeOpenAIChunks(resp);\n    await streamer.streamResponse(chunks, options, res);\n  }\n\n  private registerRoutes(): void {\n    this.router.post('/chat/completions', async (req: Request, res: Response) => {\n      // Fire side-by-side upstream capture (non-blocking)\n      void this.fireMonitorAB(req, 'chat');\n      if (await this.tryTransparentOpenAI(req, res, 'chat')) { return; }\n      void this.handlers.chat.handleRequest(req, res);\n    });\n\n    this.router.post('/completions', (req: Request, res: Response) => {\n      void this.handlers.completions.handleRequest(req, res);\n    });\n\n    this.router.post('/embeddings', (req: Request, res: Response) => {\n      void this.handlers.embeddings.handleRequest(req, res);\n    });\n\n    this.router.get('/models', (req: Request, res: Response) => {\n      void this.handlers.models.handleRequest(req, res);\n    });\n\n    this.router.get('/models/:model', (req: Request, res: Response) => {\n      void this.handlers.models.handleRequest(req, res);\n    });\n\n    this.router.post('/messages', (req: Request, res: Response) => {\n      (async () => {\n        // Try transparent passthrough for Anthropic /v1/messages when enabled via monitor config\n        if (await (this as unknown as ProtocolHandler).tryTransparentAnthropic?.(req, res)) { return; }\n        void this.handlers.messages.handleRequest(req, res);\n      })().catch(() => { void this.handlers.messages.handleRequest(req, res); });\n    });\n\n    this.router.post('/responses', async (req: Request, res: Response) => {\n      // Fire side-by-side upstream capture (non-blocking)\n      void this.fireMonitorAB(req, 'responses');\n      // Bridge mode: convert to Chat → upstream → back to Responses (JSON) when requested\n      if (await this.tryBridgeResponsesToChat(req, res)) { return; }\n      // Transparent passthrough only when explicitly enabled\n      if (await this.tryTransparentOpenAI(req, res, 'responses')) { return; }\n      void this.handlers.responses.handleRequest(req, res);\n    });\n  }\n}\n\n// Transparent passthrough helpers (monitor mode)\nexport interface TransparentHeaders { [key: string]: string }\n\nexport interface TransparentConfig {\n  enabled?: boolean;\n  defaultUpstream?: 'openai' | 'anthropic';\n  endpoints?: { openai?: string; anthropic?: string };\n  auth?: { openai?: string; anthropic?: string };\n  authorization?: string;\n  headerAllowlist?: string[];\n  timeoutMs?: number;\n  preferClientHeaders?: boolean;\n  modelMapping?: Record<string, string>;\n  wireApi?: 'chat' | 'responses';\n  extraHeaders?: Record<string, string>;\n}\n\nexport interface MonitorLikeConfig { mode?: string; transparent?: TransparentConfig }\n\n// Extend class with methods via declaration merging pattern\nexport interface ProtocolHandler {\n  tryTransparentOpenAI(req: Request, res: Response, wire: 'chat' | 'responses'): Promise<boolean>;\n  tryBridgeResponsesToChat(req: Request, res: Response): Promise<boolean>;\n  tryTransparentAnthropic(req: Request, res: Response): Promise<boolean>;\n}\n\nProtocolHandler.prototype.tryTransparentOpenAI = async function tryTransparentOpenAI(this: ProtocolHandler, req: Request, res: Response, wire: 'chat' | 'responses'): Promise<boolean> {\n  try {\n    // Transparent passthrough is active only if explicitly requested via env flags (normal mode ignores monitor.json mode)\n    const envTransparent = (\n      process.env.ROUTECODEX_MONITOR_TRANSPARENT === '1' ||\n      process.env.ROUTECODEX_TRANSPARENT_ROUTING === '1' ||\n      process.env.RCC_MONITOR_TRANSPARENT === '1' ||\n      process.env.RCC_TRANSPARENT_ROUTING === '1'\n    );\n    if (!envTransparent) {return false;}\n\n    const monitorCfg = await MonitorConfigUtil.load();\n    const tcfg = MonitorConfigUtil.getTransparent(monitorCfg as MonitorLikeConfig as any) as TransparentConfig | null;\n    if (!tcfg) {return false;}\n\n    // Resolve upstream base\n    const hdrUp = (req.headers['x-rc-upstream-url'] as string | undefined) || (req.headers['x-rcc-upstream-url'] as string | undefined);\n    const upstreamBase = hdrUp || tcfg.endpoints?.openai || '';\n    if (!upstreamBase) {return false;}\n\n    // Decide wire API\n    const preferWire: 'chat' | 'responses' = wire || tcfg.wireApi || 'responses';\n    const targetPath = preferWire === 'chat' ? '/chat/completions' : '/responses';\n    const url = joinUrl(upstreamBase, targetPath);\n\n    // Compose headers\n    const overrideAuth = (req.headers['x-rcc-upstream-authorization'] as string | undefined) || (req.headers['x-rc-upstream-authorization'] as string | undefined);\n    const incomingAuth = (req.headers['authorization'] as string | undefined) || (req.headers['Authorization'] as unknown as string | undefined);\n    const envAuth = tcfg.auth?.openai || undefined;\n    const allowlist = (tcfg.headerAllowlist || ['accept','content-type','x-*']).map(h => h.toLowerCase());\n    const passHeader = (key: string): boolean => {\n      const k = key.toLowerCase();\n      if (allowlist.includes(k)) {return true;}\n      if (allowlist.some(x => x.endsWith('*') && k.startsWith(x.slice(0, -1)))) {return true;}\n      return false;\n    };\n    const headers: TransparentHeaders = {};\n    for (const [k, v] of Object.entries(req.headers)) {\n      if (v === undefined) {continue;}\n      if (!passHeader(k)) {continue;}\n      headers[k] = Array.isArray(v) ? v.join(', ') : String(v);\n    }\n    const normalizeBearerAuth = (val: string): string => {\n      const s = String(val || '').trim();\n      if (!s) {return s;}\n      return /^bearer\\s+/i.test(s) ? s : `Bearer ${s}`;\n    };\n    const finalAuth = overrideAuth || envAuth || incomingAuth || tcfg.authorization || undefined;\n    if (finalAuth) {\n      const bearer = normalizeBearerAuth(finalAuth);\n      headers['authorization'] = bearer;\n      headers['Authorization'] = bearer;\n    }\n    // Attach extra headers (do not override Authorization)\n    if (tcfg.extraHeaders) {\n      for (const [ek, ev] of Object.entries(tcfg.extraHeaders)) {\n        if (!ek) {continue;} if (ek.toLowerCase() === 'authorization') {continue;}\n        headers[ek] = ev;\n      }\n    }\n    // Ensure Beta header for Responses wire\n    if (preferWire === 'responses') {\n      const lower = Object.fromEntries(Object.entries(headers).map(([k, v]) => [k.toLowerCase(), v]));\n      if (!('openai-beta' in lower)) {headers['OpenAI-Beta'] = 'responses-2024-12-17';}\n    }\n\n    // Timeout\n    const timeout = typeof tcfg.timeoutMs === 'number' ? tcfg.timeoutMs! : 30000;\n\n    // Prepare body and model mapping\n    const rawData: any = req.body || {};\n    const working = JSON.parse(JSON.stringify(rawData));\n    try {\n      const mm = tcfg.modelMapping || {};\n      const normalizeModel = (model: string): string => {\n        const trimmed = String(model || '').trim();\n        if (!trimmed) {return trimmed;}\n        if (/gpt-5(?:-codex)?$/i.test(trimmed)) {return 'gpt-5';}\n        if (/-codex$/i.test(trimmed)) {return trimmed.replace(/-codex$/i, '');}\n        return trimmed;\n      };\n      const inputModel = normalizeModel(working.model ?? '');\n      let mapped = inputModel && mm[inputModel];\n      if (!mapped && mm['*']) {mapped = mm['*'];}\n      if (!mapped && mm['default']) {mapped = mm['default'];}\n      if (!mapped && preferWire === 'responses') {\n        mapped = 'gpt-5';\n      }\n      if (mapped) {working.model = normalizeModel(mapped);}\n      else {working.model = inputModel || working.model;}\n    } catch { /* ignore mapping errors */ }\n\n    const isStream = !!working?.stream;\n    if (isStream) {\n      let upstream = await axios.post(url, working, { headers: { ...headers, accept: 'text/event-stream' }, timeout, responseType: 'stream', validateStatus: () => true });\n      // If chat preferred but not supported, fallback to /responses once\n      if (upstream.status >= 400 && preferWire === 'chat') {\n        try {\n          const altUrl = joinUrl(upstreamBase, '/responses');\n          const retry = await axios.post(altUrl, working, { headers: { ...headers, accept: 'text/event-stream', 'OpenAI-Beta': headers['OpenAI-Beta'] || 'responses-2024-12-17' }, timeout, responseType: 'stream', validateStatus: () => true });\n          if (retry.status < 400) { upstream = retry; }\n        } catch { /* ignore */ }\n      }\n      try {\n        res.status(upstream.status);\n        res.setHeader('Content-Type', upstream.headers['content-type'] || 'text/event-stream');\n        res.setHeader('Cache-Control', 'no-cache');\n        res.setHeader('Connection', 'keep-alive');\n        res.setHeader('Transfer-Encoding', 'chunked');\n      } catch { /* ignore */ }\n      // Capture SSE to file while piping to client\n      try {\n        const fs = await import('fs');\n        const fsp = await import('fs/promises');\n        const home = process.env.HOME || process.env.USERPROFILE || '';\n        const dirSSE = `${home}/.routecodex/codex-samples/upstream-sse`;\n        const dirReq = `${home}/.routecodex/codex-samples/upstream-requests`;\n        try { await fsp.mkdir(dirSSE, { recursive: true }); } catch { /* ignore */ }\n        try { await fsp.mkdir(dirReq, { recursive: true }); } catch { /* ignore */ }\n        const rid = `up_${Date.now()}_${Math.random().toString(36).slice(2,8)}`;\n        res.setHeader('x-request-id', rid);\n        // Write request body for later diff\n        try { await fsp.writeFile(`${dirReq}/req-${rid}.json`, JSON.stringify(working, null, 2), 'utf-8'); } catch { /* ignore */ }\n        const w = fs.createWriteStream(`${dirSSE}/sse-${rid}.log`, { encoding: 'utf-8' });\n        upstream.data.on('data', (chunk: any) => { try { w.write(chunk); } catch { /* ignore */ } });\n        upstream.data.on('end', () => { try { w.end(); } catch { /* ignore */ } });\n      } catch { /* ignore */ }\n      upstream.data.on('error', () => { try { res.end(); } catch { /* ignore */ } });\n      upstream.data.pipe(res);\n      return true;\n    } else {\n      let upstream = await axios.post(url, working, { headers: { ...headers, accept: 'application/json' }, timeout, validateStatus: () => true });\n      if (upstream.status >= 400 && preferWire === 'chat') {\n        try {\n          const altUrl = joinUrl(upstreamBase, '/responses');\n          const retry = await axios.post(altUrl, working, { headers: { ...headers, accept: 'application/json', 'OpenAI-Beta': headers['OpenAI-Beta'] || 'responses-2024-12-17' }, timeout, validateStatus: () => true });\n          if (retry.status < 400) { upstream = retry; }\n        } catch { /* ignore */ }\n      }\n      try {\n        res.status(upstream.status);\n        for (const [k, v] of Object.entries(upstream.headers || {})) {\n          if (k.toLowerCase() === 'content-length') {continue;}\n          try { res.setHeader(k, String(v)); } catch { /* ignore */ }\n        }\n        res.type('application/json');\n        // Capture JSON\n        try {\n          const { writeFile, mkdir } = await import('fs/promises');\n          const home = process.env.HOME || process.env.USERPROFILE || '';\n          const dir = `${home}/.routecodex/codex-samples/upstream-json`;\n          await mkdir(dir, { recursive: true });\n          const rid = `up_${Date.now()}_${Math.random().toString(36).slice(2,8)}`;\n          res.setHeader('x-request-id', rid);\n          await writeFile(`${dir}/json-${rid}.json`, typeof upstream.data === 'string' ? upstream.data : JSON.stringify(upstream.data, null, 2), 'utf-8');\n        } catch { /* ignore */ }\n        res.send(upstream.data);\n      } catch {\n        try { res.status(502).json({ error: { message: 'Upstream passthrough failed', type: 'bad_gateway' } }); } catch { /* ignore */ }\n      }\n      return true;\n    }\n  } catch {\n    return false;\n  }\n};\n\n// Transparent passthrough for Anthropic /v1/messages using monitor.json\nProtocolHandler.prototype.tryTransparentAnthropic = async function tryTransparentAnthropic(this: ProtocolHandler, req: Request, res: Response): Promise<boolean> {\n  try {\n    // Only active when transparent routing is explicitly enabled (env or monitor.json)\n    const envTransparent = (\n      process.env.ROUTECODEX_MONITOR_TRANSPARENT === '1' ||\n      process.env.ROUTECODEX_TRANSPARENT_ROUTING === '1' ||\n      process.env.RCC_MONITOR_TRANSPARENT === '1' ||\n      process.env.RCC_TRANSPARENT_ROUTING === '1'\n    );\n    const monitorCfg = await MonitorConfigUtil.load();\n    const enabled = envTransparent || MonitorConfigUtil.isTransparentEnabled(monitorCfg);\n    if (!enabled) {return false;}\n\n    const tcfg = MonitorConfigUtil.getTransparent(monitorCfg as any) as TransparentConfig | null;\n    if (!tcfg || !tcfg.endpoints?.anthropic) {return false;}\n\n    const upstreamBase = tcfg.endpoints.anthropic;\n    const url = joinUrl(upstreamBase, '/messages');\n\n    // Compose headers\n    const headers: TransparentHeaders = {};\n    // Prefer incoming Authorization, then configured authorization/auth.anthropic\n    const overrideAuth = (req.headers['x-rc-upstream-authorization'] as string | undefined) || (req.headers['x-rcc-upstream-authorization'] as string | undefined);\n    const incomingAuth = (req.headers['authorization'] as string | undefined) || (req.headers['Authorization'] as unknown as string | undefined);\n    const envAuth = tcfg.auth?.anthropic || undefined;\n    const finalAuth = overrideAuth || envAuth || incomingAuth || tcfg.authorization || undefined;\n    if (finalAuth) {\n      const s = String(finalAuth).trim();\n      const bearer = /^bearer\\s+/i.test(s) ? s : `Bearer ${s}`;\n      headers['authorization'] = bearer;\n      headers['Authorization'] = bearer;\n    }\n    // Anthropic-Version default\n    const lower = Object.fromEntries(Object.entries(req.headers).map(([k,v]) => [k.toLowerCase(), v]));\n    const av = (lower['anthropic-version'] as string | undefined) || '2023-06-01';\n    headers['Anthropic-Version'] = av;\n    // Pass through common headers (content-type/accept)\n    headers['content-type'] = 'application/json';\n\n    const working = JSON.parse(JSON.stringify(req.body || {}));\n    const isStream = !!working?.stream;\n    const timeout = typeof tcfg.timeoutMs === 'number' ? tcfg.timeoutMs! : 30000;\n\n    if (isStream) {\n      const upstream = await axios.post(url, working, { headers: { ...headers, Accept: 'text/event-stream' }, timeout, responseType: 'stream', validateStatus: () => true });\n      try {\n        res.status(upstream.status);\n        res.setHeader('Content-Type', upstream.headers['content-type'] || 'text/event-stream');\n        res.setHeader('Cache-Control', 'no-cache');\n        res.setHeader('Connection', 'keep-alive');\n        res.setHeader('Transfer-Encoding', 'chunked');\n      } catch { /* ignore */ }\n      // Capture SSE to file while piping to client\n      try {\n        const fs = await import('fs');\n        const fsp = await import('fs/promises');\n        const home = process.env.HOME || process.env.USERPROFILE || '';\n        const dirSSE = `${home}/.routecodex/codex-samples/upstream-anth-sse`;\n        const dirReq = `${home}/.routecodex/codex-samples/upstream-anth-requests`;\n        try { await fsp.mkdir(dirSSE, { recursive: true }); } catch {}\n        try { await fsp.mkdir(dirReq, { recursive: true }); } catch {}\n        const rid = `anth_${Date.now()}_${Math.random().toString(36).slice(2,8)}`;\n        try { await fsp.writeFile(`${dirReq}/req-${rid}.json`, JSON.stringify(working, null, 2), 'utf-8'); } catch {}\n        const w = fs.createWriteStream(`${dirSSE}/sse-${rid}.log`, { encoding: 'utf-8' });\n        upstream.data.on('data', (chunk: any) => { try { w.write(chunk); } catch {} });\n        upstream.data.on('end', () => { try { w.end(); } catch {} });\n      } catch { /* ignore */ }\n      upstream.data.on('error', () => { try { res.end(); } catch {} });\n      upstream.data.pipe(res);\n      return true;\n    } else {\n      const upstream = await axios.post(url, working, { headers: { ...headers, Accept: 'application/json' }, timeout, validateStatus: () => true });\n      try {\n        res.status(upstream.status);\n        for (const [k, v] of Object.entries(upstream.headers || {})) {\n          if (k.toLowerCase() === 'content-length') {continue;}\n          try { res.setHeader(k, String(v)); } catch {}\n        }\n        res.type('application/json');\n        // Capture\n        try {\n          const { writeFile, mkdir } = await import('fs/promises');\n          const home = process.env.HOME || process.env.USERPROFILE || '';\n          const dir = `${home}/.routecodex/codex-samples/upstream-anth-json`;\n          await mkdir(dir, { recursive: true });\n          const rid = `anth_${Date.now()}_${Math.random().toString(36).slice(2,8)}`;\n          await writeFile(`${dir}/json-${rid}.json`, typeof upstream.data === 'string' ? upstream.data : JSON.stringify(upstream.data, null, 2), 'utf-8');\n        } catch { /* ignore */ }\n        res.send(upstream.data);\n      } catch {\n        try { res.status(502).json({ error: { message: 'Upstream passthrough failed', type: 'bad_gateway' } }); } catch {}\n      }\n      return true;\n    }\n  } catch {\n    return false;\n  }\n};\n\nfunction joinUrl(base: string, path: string): string {\n  try {\n    return `${base.replace(/\\/$/, '')}/${path.replace(/^\\//, '')}`;\n  } catch { return `${base}${path}`; }\n}\n\n// Fire side-by-side upstream request for A/B monitoring without impacting the live response\nexport interface ProtocolHandler {\n  fireMonitorAB(req: Request, wire: 'chat' | 'responses'): Promise<void>;\n}\n\nProtocolHandler.prototype.fireMonitorAB = async function fireMonitorAB(this: ProtocolHandler, req: Request, wire: 'chat' | 'responses'): Promise<void> {\n  const home = (process.env.HOME || process.env.USERPROFILE || '') as string;\n  const baseDir = `${home}/.routecodex/codex-samples/monitor-ab`;\n  let captureDir: string | null = null;\n  let captureRid: string | null = null;\n  try {\n    const monitorCfg = await MonitorConfigUtil.load();\n    const mode = (monitorCfg as any)?.mode;\n    const envAB = process.env.ROUTECODEX_MONITOR_AB === '1' || process.env.RCC_MONITOR_AB === '1';\n    const enabledAB = envAB || mode === 'passive';\n    if (!enabledAB) {return;}\n\n    const tcfg = MonitorConfigUtil.getTransparent(monitorCfg as any) as TransparentConfig | null;\n    if (!tcfg || !tcfg.endpoints?.openai) {return;}\n\n    const upstreamBase = tcfg.endpoints.openai;\n    const pathPart = wire === 'chat' ? '/chat/completions' : '/responses';\n    const url = joinUrl(upstreamBase, pathPart);\n\n    const headers: Record<string,string> = {};\n    // Authorization: prefer explicit embedded token, otherwise reuse client header\n    const incomingAuthHeader = (req.headers['authorization'] as string | undefined) || (req.headers['Authorization'] as unknown as string | undefined);\n    const auth = tcfg.authorization || tcfg.auth?.openai || incomingAuthHeader || '';\n    if (auth) { headers['Authorization'] = /^Bearer\\s+/i.test(String(auth)) ? String(auth) : `Bearer ${String(auth)}`; }\n    // Responses Beta header if needed\n    if (wire === 'responses') { headers['OpenAI-Beta'] = 'responses-2024-12-17'; }\n    // Extra headers\n    if (tcfg.extraHeaders) { for (const [k,v] of Object.entries(tcfg.extraHeaders)) { if (k.toLowerCase()!=='authorization') {headers[k]=v;} } }\n\n    // Clone original body before any handler mutation (prefer raw capture)\n    const sourceBody: any = (req as any).__rawBody !== undefined ? (req as any).__rawBody : (req as any).body;\n    const rawBody: any = sourceBody ? JSON.parse(JSON.stringify(sourceBody)) : {};\n    const mappedBody: any = JSON.parse(JSON.stringify(rawBody));\n\n\n    // Determine capture dir and files\n    const startedAt = new Date();\n    const fsPromises = await import('fs/promises');\n    const { mkdir, writeFile } = fsPromises;\n    await mkdir(baseDir, { recursive: true });\n    captureRid = `ab_${Date.now()}_${Math.random().toString(36).slice(2,8)}`;\n    captureDir = `${baseDir}/${captureRid}`;\n    await mkdir(captureDir, { recursive: true });\n    const requestPayload = {\n      url,\n      wire,\n      requested_at: startedAt.toISOString(),\n      headers,\n      raw_body: rawBody,\n      mapped_body: mappedBody\n    };\n    await writeFile(`${captureDir}/request.json`, JSON.stringify(requestPayload, null, 2), 'utf-8');\n\n    const normalizeHeaders = (h: Record<string, unknown>): Record<string, string> => {\n      const out: Record<string, string> = {};\n      for (const [hk, hv] of Object.entries(h || {})) {\n        if (hv === undefined || hv === null) {continue;}\n        out[hk] = Array.isArray(hv) ? hv.map(v => String(v)).join(', ') : String(hv);\n      }\n      return out;\n    };\n\n    const isStream = !!mappedBody?.stream;\n    if (isStream) {\n      const resp = await axios.post(url, mappedBody, { headers: { ...headers, Accept: 'text/event-stream', 'Content-Type': 'application/json' }, responseType: 'stream', validateStatus: () => true, timeout: tcfg.timeoutMs || 30000 });\n      const responseMeta = {\n        status: resp.status,\n        statusText: resp.statusText,\n        headers: normalizeHeaders(resp.headers || {}),\n        received_at: new Date().toISOString()\n      };\n      await writeFile(`${captureDir}/response-meta.json`, JSON.stringify(responseMeta, null, 2), 'utf-8');\n      await new Promise<void>((resolve) => {\n        const fs = require('fs');\n        const w = fs.createWriteStream(`${captureDir}/upstream.sse`, { encoding: 'utf-8' });\n        resp.data.on('error', (error: unknown) => {\n          try { w.end(); } catch { /* ignore */ }\n          const errPayload = { message: 'stream error', error: error instanceof Error ? error.message : error };\n          if (captureDir) {\n            writeFile(`${captureDir}/response-error.json`, JSON.stringify(errPayload, null, 2), 'utf-8').catch(() => { /* ignore */ });\n          }\n          resolve();\n        });\n        resp.data.on('end', () => { try { w.end(); } catch { /* ignore */ } resolve(); });\n        resp.data.pipe(w);\n      });\n    } else {\n      const resp = await axios.post(url, mappedBody, { headers: { ...headers, Accept: 'application/json', 'Content-Type': 'application/json' }, validateStatus: () => true, timeout: tcfg.timeoutMs || 30000 });\n      const responseMeta = {\n        status: resp.status,\n        statusText: resp.statusText,\n        headers: normalizeHeaders(resp.headers || {}),\n        received_at: new Date().toISOString()\n      };\n      await writeFile(`${captureDir}/response-meta.json`, JSON.stringify(responseMeta, null, 2), 'utf-8');\n      await writeFile(`${captureDir}/upstream.json`, typeof resp.data === 'string' ? resp.data : JSON.stringify(resp.data, null, 2), 'utf-8');\n    }\n  } catch (err) {\n    try {\n      const fsPromises = await import('fs/promises');\n      const { mkdir, writeFile } = fsPromises;\n      await mkdir(baseDir, { recursive: true });\n      let dir = captureDir;\n      if (!dir) {\n        captureRid = `ab_err_${Date.now()}_${Math.random().toString(36).slice(2,8)}`;\n        dir = `${baseDir}/${captureRid}`;\n        await mkdir(dir, { recursive: true });\n      }\n      const errorPayload = {\n        message: 'monitor AB capture failed',\n        rid: captureRid,\n        error: err instanceof Error ? { name: err.name, message: err.message, stack: err.stack } : err\n      };\n      await writeFile(`${dir}/error.json`, JSON.stringify(errorPayload, null, 2), 'utf-8');\n    } catch { /* ignore secondary errors */ }\n    // swallow errors (AB capture must be non-intrusive)\n  }\n};\n\n// Bridge: convert Responses→Chat, call upstream Chat, then convert back to Responses and return to client (JSON only).\nProtocolHandler.prototype.tryBridgeResponsesToChat = async function tryBridgeResponsesToChat(this: ProtocolHandler, req: Request, res: Response): Promise<boolean> {\n  try {\n    const bridgeEnabled = (req.headers['x-rc-bridge-chat'] === '1') || (process.env.ROUTECODEX_BRIDGE_CHAT === '1');\n    if (!bridgeEnabled) {return false;}\n\n    const monitorCfg = await MonitorConfigUtil.load();\n    const tcfg = MonitorConfigUtil.getTransparent(monitorCfg as any) as TransparentConfig | null;\n    if (!tcfg || !tcfg.endpoints?.openai) {return false;}\n    const upstreamBase = tcfg.endpoints.openai;\n    const url = joinUrl(upstreamBase, '/chat/completions');\n    const headers: Record<string,string> = {};\n    const auth = (tcfg as any)?.auth?.openai || (tcfg as any)?.authorization || '';\n    if (auth) { headers['Authorization'] = /^Bearer\\s+/i.test(String(auth)) ? String(auth) : `Bearer ${String(auth)}`; }\n    if (tcfg.extraHeaders) { for (const [k,v] of Object.entries(tcfg.extraHeaders)) { if (k.toLowerCase()!=='authorization') {headers[k]=v;} } }\n\n    // Convert incoming Responses payload to Chat\n    // eslint-disable-next-line @typescript-eslint/no-var-requires\n    const { ResponsesToChatLLMSwitch } = await import('rcc-llmswitch-core/llmswitch/llmswitch-response-chat');\n    // eslint-disable-next-line @typescript-eslint/no-var-requires\n    const { PipelineDebugLogger } = require('../utils/debug-logger.js');\n    const logger = new PipelineDebugLogger(null, { enableConsoleLogging: false, enableDebugCenter: false });\n    const deps = { errorHandlingCenter: {}, debugCenter: {}, logger } as any;\n    const conv = new ResponsesToChatLLMSwitch({ type: 'llmswitch-response-chat', config: {} }, deps);\n    if (typeof conv.initialize === 'function') { await conv.initialize(); }\n    const chatReq: any = await conv.transformRequest(req.body);\n\n    // Apply model mapping only to upstream\n    try {\n      const mm = tcfg.modelMapping || {};\n      const normalizeModel = (model: string): string => {\n        const val = String(model || '').trim();\n        if (!val) {return val;}\n        if (/gpt-5(?:-codex)?$/i.test(val)) {return 'gpt-5-high';}\n        if (/-codex$/i.test(val)) {return val.replace(/-codex$/i, '');}\n        return val;\n      };\n      const src = normalizeModel(chatReq?.model ?? '');\n      let mapped = src && mm[src];\n      if (!mapped && mm['*']) {mapped = mm['*'];}\n      if (!mapped && mm['default']) {mapped = mm['default'];}\n      if (!mapped) {mapped = src;}\n      chatReq.model = normalizeModel(mapped);\n    } catch { /* ignore */ }\n\n    // Force non-stream upstream for simpler conversion back\n    chatReq.stream = false;\n    const upstream = await axios.post(url, chatReq, { headers: { ...headers, Accept: 'application/json', 'Content-Type': 'application/json' }, validateStatus: () => true, timeout: tcfg.timeoutMs || 30000 });\n\n    // Convert Chat JSON back to Responses JSON\n    const responsesJson = await conv.transformResponse(upstream.data);\n    res.status(upstream.status);\n    try { res.setHeader('Content-Type', 'application/json'); } catch { /* ignore */ }\n    res.send(responsesJson);\n    return true;\n  } catch {\n    return false;\n  }\n};\n"
        },
        "src/server/responses-sse-simulator.ts": {
          "path": "src/server/responses-sse-simulator.ts",
          "size": 5033,
          "lines": 89,
          "imports": [],
          "exports": [
            "ResponsesSSESimulator"
          ],
          "classes": [
            "ResponsesSSESimulator"
          ],
          "functions": [
            "push"
          ],
          "content": "/**\n * Responses SSE Simulator\n *\n * Converts a complete Responses payload into SSE events by simulating\n * incremental delivery when the pipeline returns non-stream data.\n */\n\nexport type ResponsesEvent = { event: string; data: Record<string, unknown> };\n\nexport class ResponsesSSESimulator {\n  constructor(private chunkSize: number = Math.max(32, Math.min(1024, Number(process.env.ROUTECODEX_RESPONSES_TOOLCALL_DELTA_CHUNK || 256)))) {}\n\n  public buildEvents(payload: any): ResponsesEvent[] {\n    const events: ResponsesEvent[] = [];\n    if (!payload || typeof payload !== 'object') {\n      return events;\n    }\n\n    const responseId = typeof payload.id === 'string' ? payload.id : `resp_${Date.now().toString(36)}${Math.random().toString(36).slice(2, 8)}`;\n    const created = typeof payload.created === 'number' ? payload.created : Math.floor(Date.now() / 1000);\n    const model = typeof payload.model === 'string' ? payload.model : 'unknown';\n\n    let seq = 1;\n    const push = (event: string, data: Record<string, unknown>) => {\n      events.push({ event, data: { ...data, sequence_number: seq++ } });\n    };\n\n    push('response.created', { type: 'response.created', response: { id: responseId, object: 'response', created, model } });\n    push('response.in_progress', { type: 'response.in_progress', response: { id: responseId } });\n\n    const output: any[] = Array.isArray(payload.output) ? payload.output : [];\n    let outputIndex = 0;\n    for (const item of output) {\n      const type = item?.type;\n      if (type === 'reasoning') {\n        const reasonId = typeof item.id === 'string' ? item.id : `rs_${Math.random().toString(36).slice(2, 8)}`;\n        push('response.output_item.added', { type: 'response.output_item.added', output_index: outputIndex, item: { id: reasonId, type: 'reasoning', summary: Array.isArray(item?.summary) ? item.summary : [], status: 'completed' } });\n        push('response.output_item.done', { type: 'response.output_item.done', output_index: outputIndex, item: { id: reasonId, type: 'reasoning' } });\n        outputIndex += 1;\n        continue;\n      }\n      if (type === 'message') {\n        const message = item?.message || {};\n        const messageId = typeof message.id === 'string' ? message.id : `msg_${Math.random().toString(36).slice(2, 8)}`;\n        push('response.output_item.added', { type: 'response.output_item.added', output_index: outputIndex, item: { id: messageId, type: 'message', status: 'in_progress', content: [], role: message.role || 'assistant' } });\n        const content = Array.isArray(message.content) ? message.content : [];\n        let contentIndex = 0;\n        for (const part of content) {\n          const text = typeof part?.text === 'string' ? part.text : '';\n          push('response.content_part.added', { type: 'response.content_part.added', item_id: messageId, output_index: outputIndex, content_index: contentIndex, part: { type: 'output_text', annotations: [], logprobs: [], text: '' } });\n          if (text) {\n            push('response.output_text.delta', { type: 'response.output_text.delta', item_id: messageId, output_index: outputIndex, content_index: contentIndex, delta: text, logprobs: [] });\n          }\n          contentIndex += 1;\n        }\n        push('response.output_item.done', { type: 'response.output_item.done', output_index: outputIndex, item: { id: messageId, type: 'message', status: 'completed' } });\n        outputIndex += 1;\n        continue;\n      }\n      if (type === 'tool_call') {\n        const id = typeof item?.tool_call?.id === 'string' ? item.tool_call.id : (typeof item?.id === 'string' ? item.id : `call_${Math.random().toString(36).slice(2, 8)}`);\n        const name = item?.name || item?.tool_call?.name || 'tool';\n        push('response.output_item.added', { type: 'response.output_item.added', output_index: outputIndex, item: { id, type: 'tool_call', name } });\n        const args = typeof item?.arguments === 'string' ? item.arguments : (typeof item?.tool_call?.function?.arguments === 'string' ? item.tool_call.function.arguments : '');\n        if (args) {\n          for (const part of this.chunkString(args, this.chunkSize)) {\n            push('response.tool_call.delta', { type: 'response.tool_call.delta', item_id: id, output_index: outputIndex, delta: { arguments: part } });\n          }\n        }\n        push('response.output_item.done', { type: 'response.output_item.done', output_index: outputIndex, item: { id, type: 'tool_call' } });\n        outputIndex += 1;\n        continue;\n      }\n    }\n\n    push('response.completed', { type: 'response.completed', response: { id: responseId, status: payload.status || 'completed', stop_reason: payload.stop_reason ?? null }, usage: payload.usage || undefined });\n    push('response.done', { type: 'response.done' });\n    return events;\n  }\n\n  private chunkString(source: string, size: number): string[] {\n    const out: string[] = [];\n    for (let i = 0; i < source.length; i += size) {\n      out.push(source.slice(i, i + size));\n    }\n    return out;\n  }\n}\n"
        },
        "src/server/responses-sse-transformer.ts": {
          "path": "src/server/responses-sse-transformer.ts",
          "size": 7691,
          "lines": 159,
          "imports": [],
          "exports": [
            "ResponsesSSETransformer"
          ],
          "classes": [
            "ResponsesSSETransformer"
          ],
          "functions": [],
          "content": "/**\n * Responses SSE Transformer\n *\n * Converts OpenAI ChatCompletion streaming chunks into OpenAI Responses\n * SSE events (response.*) so clients receive incremental updates.\n */\n\nexport type ResponsesEvent = { event: string; data: Record<string, unknown> };\n\ninterface ToolAccumulator {\n  id: string;\n  name: string;\n  buffer: string;\n  started: boolean;\n}\n\nexport class ResponsesSSETransformer {\n  private responseId = '';\n  private model = '';\n  private created = 0;\n  private sequence = 0;\n  private textItemId: string | null = null;\n  private finishReason: string | null = null;\n  private usage: { prompt_tokens?: number; completion_tokens?: number; total_tokens?: number } | null = null;\n  private toolCalls: Map<number, ToolAccumulator> = new Map();\n  private requiredActionMode = String(process.env.ROUTECODEX_RESP_SSE_REQUIRED_ACTION || process.env.RCC_RESP_SSE_REQUIRED_ACTION || '0').toLowerCase() === '1' || String(process.env.ROUTECODEX_RESP_SSE_REQUIRED_ACTION || process.env.RCC_RESP_SSE_REQUIRED_ACTION || '0') === 'true';\n  private requiredActionEmitted = false;\n\n  constructor(private chunkSize: number = Math.max(32, Math.min(1024, Number(process.env.ROUTECODEX_RESPONSES_TOOLCALL_DELTA_CHUNK || 256)))) {}\n\n  private nextSeq(): number {\n    this.sequence += 1;\n    return this.sequence;\n  }\n\n  private ensureMeta(chunk: any): void {\n    if (!this.responseId) {\n      const id = typeof chunk?.id === 'string' ? chunk.id : (typeof chunk?.request_id === 'string' ? chunk.request_id : null);\n      this.responseId = id || `resp_${Date.now().toString(36)}${Math.random().toString(36).slice(2, 8)}`;\n    }\n    if (!this.model) {\n      this.model = typeof chunk?.model === 'string' ? chunk.model : 'unknown';\n    }\n    if (!this.created) {\n      const ts = Number(chunk?.created);\n      this.created = Number.isFinite(ts) ? ts : Math.floor(Date.now() / 1000);\n    }\n  }\n\n  public processOpenAIChunk(raw: any): ResponsesEvent[] {\n    const events: ResponsesEvent[] = [];\n    if (!raw) {return events;}\n\n    this.ensureMeta(raw);\n\n    if (this.sequence === 0) {\n      events.push({ event: 'response.created', data: { type: 'response.created', response: { id: this.responseId, object: 'response', created: this.created, model: this.model }, sequence_number: this.nextSeq() } });\n      events.push({ event: 'response.in_progress', data: { type: 'response.in_progress', response: { id: this.responseId }, sequence_number: this.nextSeq() } });\n    }\n\n    const choice = Array.isArray(raw?.choices) ? raw.choices[0] : undefined;\n    const delta = choice?.delta || {};\n\n    const textPieces = Array.isArray(delta?.content) ? delta.content.filter((c: any) => c && c.type === 'text' && typeof c.text === 'string') : [];\n    for (const piece of textPieces) {\n      if (!this.textItemId) {\n        this.textItemId = `msg_${this.responseId}`;\n        events.push({ event: 'response.output_item.added', data: { type: 'response.output_item.added', output_index: 0, item: { id: this.textItemId, type: 'message', status: 'in_progress', content: [], role: 'assistant' }, sequence_number: this.nextSeq() } });\n        events.push({ event: 'response.content_part.added', data: { type: 'response.content_part.added', item_id: this.textItemId, output_index: 0, content_index: 0, part: { type: 'output_text', annotations: [], logprobs: [], text: '' }, sequence_number: this.nextSeq() } });\n      }\n      events.push({ event: 'response.output_text.delta', data: { type: 'response.output_text.delta', item_id: this.textItemId, output_index: 0, content_index: 0, delta: piece.text, logprobs: [], sequence_number: this.nextSeq() } });\n    }\n\n    const toolDeltas = Array.isArray(delta?.tool_calls) ? delta.tool_calls : [];\n    for (const tc of toolDeltas) {\n      const index = Number(tc?.index ?? 0);\n      let acc = this.toolCalls.get(index);\n      if (!acc) {\n        const id = typeof tc?.id === 'string' ? tc.id : `call_${Math.random().toString(36).slice(2, 8)}`;\n        const name = typeof tc?.function?.name === 'string' ? tc.function.name : 'tool';\n        acc = { id, name, buffer: '', started: false };\n        this.toolCalls.set(index, acc);\n        // Optional CCR-style required_action envelope\n        if (this.requiredActionMode && !this.requiredActionEmitted) {\n          this.requiredActionEmitted = true;\n          events.push({\n            event: 'response.required_action',\n            data: {\n              type: 'response.required_action',\n              response: { id: this.responseId },\n              required_action: {\n                type: 'tool_calls',\n                tool_calls: [ { id, type: 'function', name } ]\n              },\n              sequence_number: this.nextSeq()\n            }\n          });\n        }\n        events.push({ event: 'response.output_item.added', data: { type: 'response.output_item.added', output_index: index + 1, item: { id, type: 'tool_call', name }, sequence_number: this.nextSeq() } });\n      }\n      const args = typeof tc?.function?.arguments === 'string' ? tc.function.arguments : '';\n      if (args) {\n        acc.buffer += args;\n        for (const part of this.chunkString(args, this.chunkSize)) {\n          events.push({ event: 'response.tool_call.delta', data: { type: 'response.tool_call.delta', item_id: acc.id, output_index: index + 1, delta: { arguments: part }, sequence_number: this.nextSeq() } });\n          if (this.requiredActionMode) {\n            events.push({ event: 'response.function_call_arguments.delta', data: { type: 'response.function_call_arguments.delta', id: acc.id, delta: part, sequence_number: this.nextSeq() } });\n          }\n        }\n      }\n    }\n\n    const finish = choice?.finish_reason;\n    if (typeof finish === 'string') {\n      this.finishReason = finish;\n    }\n\n    const usage = raw?.usage;\n    if (usage && typeof usage === 'object') {\n      this.usage = {\n        prompt_tokens: Number(usage?.prompt_tokens ?? usage?.input_tokens) || 0,\n        completion_tokens: Number(usage?.completion_tokens ?? usage?.output_tokens) || 0,\n        total_tokens: Number(usage?.total_tokens) || undefined,\n      };\n    }\n\n    return events;\n  }\n\n  public finalize(): ResponsesEvent[] {\n    const events: ResponsesEvent[] = [];\n\n    for (const [index, acc] of this.toolCalls.entries()) {\n      // Close each tool_call item using its correct output_index (tool indexes start at 1)\n      const outIdx = Number(index) + 1;\n      events.push({ event: 'response.output_item.done', data: { type: 'response.output_item.done', output_index: outIdx, item: { id: acc.id, type: 'tool_call' }, sequence_number: this.nextSeq() } });\n      if (this.requiredActionMode) {\n        events.push({ event: 'response.function_call_arguments.done', data: { type: 'response.function_call_arguments.done', id: acc.id, sequence_number: this.nextSeq() } });\n      }\n    }\n\n    if (this.textItemId) {\n      events.push({ event: 'response.output_item.done', data: { type: 'response.output_item.done', output_index: 0, item: { id: this.textItemId, type: 'message', status: 'completed' }, sequence_number: this.nextSeq() } });\n    }\n\n    events.push({ event: 'response.completed', data: { type: 'response.completed', response: { id: this.responseId, status: 'completed', stop_reason: this.finishReason ?? null }, usage: this.usage || undefined, sequence_number: this.nextSeq() } });\n    events.push({ event: 'response.done', data: { type: 'response.done', sequence_number: this.nextSeq() } });\n    return events;\n  }\n\n  private chunkString(input: string, size: number): string[] {\n    const chunks: string[] = [];\n    for (let i = 0; i < input.length; i += size) {\n      chunks.push(input.slice(i, i + size));\n    }\n    return chunks;\n  }\n}\n"
        },
        "src/server/streaming/anthropic-streamer.ts": {
          "path": "src/server/streaming/anthropic-streamer.ts",
          "size": 1773,
          "lines": 54,
          "imports": [
            "express"
          ],
          "exports": [
            "AnthropicStreamer"
          ],
          "classes": [
            "without",
            "AnthropicStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\n\nimport {\n  BaseStreamer,\n  type ProtocolHandlerConfig,\n  type StreamChunk,\n  type StreamingOptions\n} from './base-streamer.js';\n\n/**\n * Minimal Anthropic-compatible streamer that emits SSE payloads using the\n * generic formatting helpers from {@link BaseStreamer}. The implementation is\n * intentionally lightweight; when richer streaming support lands we can extend\n * this class without breaking consumers of the current refactor.\n */\nexport class AnthropicStreamer extends BaseStreamer {\n  constructor(config?: ProtocolHandlerConfig) {\n    super(config);\n  }\n\n  async streamResponse(response: unknown, options: StreamingOptions, res: Response): Promise<void> {\n    try {\n      this.validateOptions(options);\n      if (!this.isStreamingEnabled()) {\n        throw new Error('Streaming is disabled for this handler');\n      }\n\n      this.setupStreamingHeaders(res, options.requestId);\n      this.startStreaming();\n\n      const chunks: StreamChunk[] = Array.isArray(response)\n        ? (response as StreamChunk[])\n        : typeof response === 'object' && response !== null && 'data' in (response as Record<string, unknown>) && Array.isArray((response as any).data)\n          ? ((response as any).data as StreamChunk[])\n          : [];\n\n      for (const chunk of chunks) {\n        const payload = this.formatAnthropicChunk(chunk, options.requestId);\n        const wrote = this.sendSSEData(res, payload);\n        if (!wrote) {\n          break;\n        }\n        this.incrementChunkCount();\n        await this.applyChunkDelay(options.chunkDelay);\n      }\n\n      this.endStreaming(res);\n      this.logStreamingMetrics(options);\n    } catch (error) {\n      this.handleStreamingError(res, error as Error, options.requestId);\n    }\n  }\n}\n"
        },
        "src/server/streaming/base-streamer.ts": {
          "path": "src/server/streaming/base-streamer.ts",
          "size": 7549,
          "lines": 313,
          "imports": [
            "express"
          ],
          "exports": [
            "StreamChunk",
            "StreamingOptions",
            "ProtocolHandlerConfig"
          ],
          "classes": [
            "BaseStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\n\n/**\n * Stream chunk interface\n */\nexport interface StreamChunk {\n  content?: string;\n  done?: boolean;\n  // For OpenAI Chat SSE: include tool_calls delta when present\n  tool_calls?: Array<{\n    index: number;\n    id?: string;\n    type?: 'function';\n    function?: { name?: string; arguments?: string };\n  }>;\n  metadata?: {\n    model?: string;\n    usage?: {\n      prompt_tokens?: number;\n      completion_tokens?: number;\n      total_tokens?: number;\n    };\n    finish_reason?: string;\n  };\n}\n\n/**\n * Streaming options interface\n */\nexport interface StreamingOptions {\n  requestId: string;\n  model: string;\n  enableMetrics?: boolean;\n  chunkDelay?: number;\n  maxTokens?: number;\n}\n\n/**\n * Protocol handler configuration\n */\nexport interface ProtocolHandlerConfig {\n  enableStreaming?: boolean;\n  enableMetrics?: boolean;\n  targetUrl?: string;\n  timeout?: number;\n}\n\n/**\n * Base Streamer Abstract Class\n * Provides common streaming functionality for different protocols\n */\nexport abstract class BaseStreamer {\n  protected config: ProtocolHandlerConfig;\n  protected isStreaming = false;\n  protected startTime = 0;\n  protected chunkCount = 0;\n\n  constructor(config: ProtocolHandlerConfig = {}) {\n    this.config = {\n      enableStreaming: true,\n      enableMetrics: true,\n      timeout: 30000,\n      ...config,\n    };\n  }\n\n  /**\n   * Stream response - must be implemented by subclasses\n   */\n  abstract streamResponse(\n    response: any,\n    options: StreamingOptions,\n    res: Response\n  ): Promise<void>;\n\n  /**\n   * Send SSE (Server-Sent Events) data\n   */\n  protected sendSSEData(res: Response, data: string): boolean {\n    try {\n      if (res.writableEnded) {\n        return false;\n      }\n\n      res.write(`data: ${data}\\n\\n`);\n      return true;\n    } catch (error) {\n      console.error('Failed to send SSE data:', error);\n      return false;\n    }\n  }\n\n  /**\n   * Send SSE event with custom event name\n   */\n  protected sendSSEEvent(res: Response, event: string, data: string): boolean {\n    try {\n      if (res.writableEnded) {\n        return false;\n      }\n\n      res.write(`event: ${event}\\n`);\n      res.write(`data: ${data}\\n\\n`);\n      return true;\n    } catch (error) {\n      console.error('Failed to send SSE event:', error);\n      return false;\n    }\n  }\n\n  /**\n   * Set up streaming response headers\n   */\n  protected setupStreamingHeaders(res: Response, requestId: string): void {\n    // Avoid \"ERR_HTTP_HEADERS_SENT\" if a pre-heartbeat or another layer already set headers\n    if (res.headersSent) {\n      return;\n    }\n    try { res.setHeader('Content-Type', 'text/event-stream'); } catch { /* ignore */ }\n    try { res.setHeader('Cache-Control', 'no-cache'); } catch { /* ignore */ }\n    try { res.setHeader('Connection', 'keep-alive'); } catch { /* ignore */ }\n    try { res.setHeader('Access-Control-Allow-Origin', '*'); } catch { /* ignore */ }\n    try { res.setHeader('Access-Control-Allow-Headers', 'Cache-Control'); } catch { /* ignore */ }\n    try { res.setHeader('x-request-id', requestId); } catch { /* ignore */ }\n  }\n\n  /**\n   * Start streaming session\n   */\n  protected startStreaming(): void {\n    this.isStreaming = true;\n    this.startTime = Date.now();\n    this.chunkCount = 0;\n  }\n\n  /**\n   * End streaming session\n   */\n  protected endStreaming(res: Response): void {\n    this.isStreaming = false;\n\n    if (!res.writableEnded) {\n      try {\n        res.write('data: [DONE]\\n\\n');\n        res.end();\n      } catch (error) {\n        console.error('Error ending stream:', error);\n      }\n    }\n  }\n\n  /**\n   * Log streaming metrics\n   */\n  protected logStreamingMetrics(options: StreamingOptions): void {\n    if (!this.config.enableMetrics) {\n      return;\n    }\n\n    const duration = Date.now() - this.startTime;\n    const metrics = {\n      requestId: options.requestId,\n      model: options.model,\n      duration,\n      chunkCount: this.chunkCount,\n      chunksPerSecond: this.chunkCount > 0 ? (this.chunkCount / (duration / 1000)).toFixed(2) : 0,\n    };\n\n    console.log('Streaming metrics:', metrics);\n  }\n\n  /**\n   * Validate streaming options\n   */\n  protected validateOptions(options: StreamingOptions): void {\n    if (!options.requestId) {\n      throw new Error('Request ID is required for streaming');\n    }\n\n    if (!options.model) {\n      throw new Error('Model is required for streaming');\n    }\n  }\n\n  /**\n   * Handle streaming error\n   */\n  protected handleStreamingError(res: Response, error: Error, requestId: string): void {\n    console.error('Streaming error:', error);\n\n    if (!res.writableEnded) {\n      const errorData = {\n        error: {\n          message: error.message,\n          type: 'streaming_error',\n          code: 'STREAM_FAILED',\n        },\n        requestId,\n      };\n\n      this.sendSSEEvent(res, 'error', JSON.stringify(errorData));\n      res.end();\n    }\n  }\n\n  /**\n   * Check if streaming is enabled\n   */\n  protected isStreamingEnabled(): boolean {\n    return this.config.enableStreaming === true;\n  }\n\n  /**\n   * Get streaming statistics\n   */\n  protected getStreamingStats(): {\n    isStreaming: boolean;\n    duration: number;\n    chunkCount: number;\n  } {\n    return {\n      isStreaming: this.isStreaming,\n      duration: this.startTime > 0 ? Date.now() - this.startTime : 0,\n      chunkCount: this.chunkCount,\n    };\n  }\n\n  /**\n   * Increment chunk counter\n   */\n  protected incrementChunkCount(): void {\n    this.chunkCount++;\n  }\n\n  /**\n   * Apply chunk delay if configured\n   */\n  protected async applyChunkDelay(delayMs?: number): Promise<void> {\n    const ms = typeof delayMs === 'number' ? delayMs : 0;\n    if (ms > 0) {\n      await new Promise(resolve => setTimeout(resolve, ms));\n    }\n  }\n\n  /**\n   * Format OpenAI-style chunk\n   */\n  protected formatOpenAIChunk(chunk: StreamChunk, requestId: string): string {\n    const openAIChunk: any = {\n      id: `chatcmpl-${requestId}-${this.chunkCount}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model: chunk.metadata?.model || 'unknown',\n      choices: [{\n        index: 0,\n        delta: {\n          content: chunk.content ?? '',\n          // Attach tool_calls delta if present\n          ...(Array.isArray(chunk.tool_calls) && chunk.tool_calls.length\n            ? { tool_calls: chunk.tool_calls.map((tc) => ({\n                index: tc.index,\n                id: tc.id,\n                type: tc.type || 'function',\n                function: tc.function || {}\n              })) }\n            : {})\n        },\n        finish_reason: chunk.done ? chunk.metadata?.finish_reason || 'stop' : null,\n      }],\n    };\n\n    if (chunk.done && chunk.metadata?.usage) {\n      openAIChunk.usage = chunk.metadata.usage;\n    }\n\n    return JSON.stringify(openAIChunk);\n  }\n\n  /**\n   * Format Anthropic-style chunk\n   */\n  protected formatAnthropicChunk(chunk: StreamChunk, requestId: string): string {\n    if (chunk.done) {\n      return JSON.stringify({\n        type: 'message_stop',\n      });\n    }\n\n    return JSON.stringify({\n      type: 'content_block_delta',\n      index: 0,\n      delta: {\n        type: 'text_delta',\n        text: chunk.content ?? ''\n      }\n    });\n  }\n\n  /**\n   * Format Response-style chunk (generic)\n   */\n  protected formatResponseChunk(chunk: StreamChunk, requestId: string): string {\n    return JSON.stringify({\n      id: requestId,\n      object: 'response.chunk',\n      created: Math.floor(Date.now() / 1000),\n      content: chunk.content,\n      done: chunk.done,\n      metadata: chunk.metadata,\n    });\n  }\n}\n"
        },
        "src/server/streaming/openai-streamer.ts": {
          "path": "src/server/streaming/openai-streamer.ts",
          "size": 1450,
          "lines": 42,
          "imports": [
            "express",
            "./base-streamer.js"
          ],
          "exports": [
            "OpenAIStreamer"
          ],
          "classes": [
            "OpenAIStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\nimport { BaseStreamer, type StreamChunk, type StreamingOptions, type ProtocolHandlerConfig } from './base-streamer.js';\n\nexport class OpenAIStreamer extends BaseStreamer {\n  constructor(config?: ProtocolHandlerConfig) {\n    super(config);\n  }\n\n  async streamResponse(response: unknown, options: StreamingOptions, res: Response): Promise<void> {\n    try {\n      this.validateOptions(options);\n      if (!this.isStreamingEnabled()) {\n        throw new Error('Streaming is disabled for this handler');\n      }\n\n      this.setupStreamingHeaders(res, options.requestId);\n      this.startStreaming();\n\n      const chunks: StreamChunk[] = Array.isArray(response)\n        ? (response as StreamChunk[])\n        : typeof response === 'object' && response !== null && 'data' in (response as Record<string, unknown>) && Array.isArray((response as any).data)\n          ? ((response as any).data as StreamChunk[])\n          : [];\n\n      for (const chunk of chunks) {\n        const payload = this.formatOpenAIChunk(chunk, options.requestId);\n        const wrote = this.sendSSEData(res, payload);\n        if (!wrote) {\n          break;\n        }\n        this.incrementChunkCount();\n        await this.applyChunkDelay(options.chunkDelay);\n      }\n\n      this.endStreaming(res);\n      this.logStreamingMetrics(options);\n    } catch (error) {\n      this.handleStreamingError(res, error as Error, options.requestId);\n    }\n  }\n}\n"
        },
        "src/server/streaming/responses-streamer.ts": {
          "path": "src/server/streaming/responses-streamer.ts",
          "size": 2074,
          "lines": 65,
          "imports": [
            "express"
          ],
          "exports": [
            "ResponsesStreamer"
          ],
          "classes": [
            "ResponsesStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\n\nimport {\n  BaseStreamer,\n  type ProtocolHandlerConfig,\n  type StreamChunk,\n  type StreamingOptions\n} from './base-streamer.js';\n\n/**\n * Protocol-agnostic streamer that falls back to the generic response chunk\n * format. This keeps the new streaming layer compilable while the richer\n * implementation is still being designed.\n */\nexport class ResponsesStreamer extends BaseStreamer {\n  constructor(config?: ProtocolHandlerConfig) {\n    super(config);\n  }\n\n  async streamResponse(response: unknown, options: StreamingOptions, res: Response): Promise<void> {\n    try {\n      this.validateOptions(options);\n      if (!this.isStreamingEnabled()) {\n        throw new Error('Streaming is disabled for this handler');\n      }\n\n      this.setupStreamingHeaders(res, options.requestId);\n      this.startStreaming();\n\n      const chunks: StreamChunk[] = Array.isArray(response)\n        ? (response as StreamChunk[])\n        : typeof response === 'object' && response !== null && 'data' in (response as Record<string, unknown>) && Array.isArray((response as any).data)\n          ? ((response as any).data as StreamChunk[])\n          : [];\n\n      for (const chunk of chunks) {\n        const payload = this.formatResponseChunk(chunk, options.requestId);\n        const wrote = this.sendSSEData(res, payload);\n        if (!wrote) {\n          break;\n        }\n        this.incrementChunkCount();\n        await this.applyChunkDelay(options.chunkDelay);\n      }\n\n      // Emit Anthropic Responses-style completion signal before closing\n      try {\n        const completion = {\n          type: 'response.completed',\n          response: {\n            id: `resp_${options.requestId}`,\n            model: options.model,\n          },\n        };\n        this.sendSSEEvent(res, 'response.completed', JSON.stringify(completion));\n      } catch { /* ignore */ }\n\n      try { res.end(); } catch { /* ignore */ }\n      this.logStreamingMetrics(options);\n    } catch (error) {\n      this.handleStreamingError(res, error as Error, options.requestId);\n    }\n  }\n}\n"
        },
        "src/server/types/handler-types.ts": {
          "path": "src/server/types/handler-types.ts",
          "size": 1676,
          "lines": 89,
          "imports": [
            "express"
          ],
          "exports": [
            "ValidationResult",
            "ProtocolHandlerConfig",
            "ErrorResponse",
            "RequestHandler",
            "StreamHandler",
            "ErrorResponder",
            "HandlerMetadata",
            "HandlerLifecycle"
          ],
          "classes": [],
          "functions": [],
          "content": "/**\n * Handler Types Module\n * Central type definitions for protocol handlers\n */\n\nimport type { Request, Response } from 'express';\n\n/**\n * Request validation result interface\n */\nexport interface ValidationResult {\n  isValid: boolean;\n  errors: string[];\n}\n\n/**\n * Protocol handler configuration interface\n */\nexport interface ProtocolHandlerConfig {\n  enableStreaming?: boolean;\n  enableMetrics?: boolean;\n  enableValidation?: boolean;\n  rateLimitEnabled?: boolean;\n  authEnabled?: boolean;\n  targetUrl?: string;\n  timeout?: number;\n  enablePipeline?: boolean;\n  pipelineProvider?: {\n    defaultProvider: string;\n    modelMapping: Record<string, string>;\n  };\n}\n\n/**\n * Error response interface\n */\nexport interface ErrorResponse {\n  status: number;\n  body: {\n    error: {\n      message: string;\n      type: string;\n      code: string;\n      param?: string | null;\n      details?: Record<string, unknown>;\n    };\n  };\n}\n\n/**\n * Request handler interface\n */\nexport interface RequestHandler {\n  handleRequest(req: Request, res: Response): Promise<void>;\n}\n\n/**\n * Stream handler interface\n */\nexport interface StreamHandler {\n  handleStream(req: Request, res: Response): Promise<void>;\n}\n\n/**\n * Error responder interface\n */\nexport interface ErrorResponder {\n  handleError(error: unknown, res: Response, requestId: string): Promise<void>;\n}\n\n/**\n * Handler metadata interface\n */\nexport interface HandlerMetadata {\n  name: string;\n  version: string;\n  protocol: string;\n  capabilities: string[];\n}\n\n/**\n * Handler lifecycle interface\n */\nexport interface HandlerLifecycle {\n  initialize?(): Promise<void>;\n  cleanup?(): Promise<void>;\n  healthCheck?(): Promise<boolean>;\n}\n"
        },
        "src/server/types.ts": {
          "path": "src/server/types.ts",
          "size": 7569,
          "lines": 364,
          "imports": [
            "rcc-basemodule",
            "../types/common-types.js"
          ],
          "exports": [
            "OpenAIChatMessage",
            "OpenAIChatToolCall",
            "OpenAIChatCompletionRequest",
            "OpenAICompletionRequest",
            "OpenAICompletionResponseChoice",
            "OpenAICompletionResponse",
            "OpenAIModel",
            "ServerConfig",
            "ProviderConfig",
            "ModelConfig",
            "RequestContext",
            "ResponseContext",
            "ServerError",
            "RouteCodexError",
            "HealthStatus",
            "ProviderHealth",
            "ProviderStats",
            "ServerEvent",
            "ServerModuleInfo",
            "IHttpServer",
            "IRouter",
            "MiddlewareConfig",
            "RateLimitInfo",
            "RateLimitConfig",
            "StreamOptions",
            "StreamResponse"
          ],
          "classes": [
            "RouteCodexError"
          ],
          "functions": [],
          "content": "/**\n * Server type definitions\n * Type definitions for HTTP server and OpenAI router components\n */\n\nimport { type ModuleInfo } from 'rcc-basemodule';\nimport type { UnknownObject } from '../types/common-types.js';\n\n/**\n * OpenAI API request types\n */\nexport interface OpenAIChatMessage {\n  role: 'system' | 'user' | 'assistant' | 'tool';\n  content: string;\n  name?: string;\n  tool_call_id?: string;\n}\n\nexport interface OpenAIChatToolCall {\n  id: string;\n  type: 'function';\n  function: {\n    name: string;\n    arguments: string;\n  };\n}\n\nexport interface OpenAIChatCompletionRequest {\n  model: string;\n  messages: OpenAIChatMessage[];\n  temperature?: number;\n  max_tokens?: number;\n  top_p?: number;\n  frequency_penalty?: number;\n  presence_penalty?: number;\n  stop?: string | string[];\n  stream?: boolean;\n  tools?: Array<{\n    type: 'function';\n    function: {\n      name: string;\n      description?: string;\n      parameters: Record<string, UnknownObject>;\n    };\n  }>;\n  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };\n  user?: string;\n}\n\nexport interface OpenAICompletionRequest {\n  model: string;\n  prompt: string | string[];\n  suffix?: string;\n  max_tokens?: number;\n  temperature?: number;\n  top_p?: number;\n  n?: number;\n  stream?: boolean;\n  logprobs?: number;\n  echo?: boolean;\n  stop?: string | string[];\n  presence_penalty?: number;\n  frequency_penalty?: number;\n  best_of?: number;\n  logit_bias?: Record<string, number>;\n  user?: string;\n}\n\nexport interface OpenAICompletionResponseChoice {\n  index: number;\n  message: {\n    role: 'assistant';\n    content: string | null;\n    tool_calls?: OpenAIChatToolCall[];\n  };\n  finish_reason?: 'stop' | 'length' | 'tool_calls' | 'content_filter';\n}\n\nexport interface OpenAICompletionResponse {\n  id: string;\n  object: 'chat.completion';\n  created: number;\n  model: string;\n  choices: OpenAICompletionResponseChoice[];\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\nexport interface OpenAIModel {\n  id: string;\n  object: 'model';\n  created: number;\n  owned_by: string;\n}\n\n/**\n * Server configuration types\n */\nexport interface ServerConfig {\n  server: {\n    port: number;\n    host: string;\n    cors?: {\n      origin: string | string[];\n      credentials?: boolean;\n    };\n    timeout?: number;\n    bodyLimit?: string;\n  };\n  logging: {\n    level: 'debug' | 'info' | 'warn' | 'error';\n    enableConsole?: boolean;\n    enableFile?: boolean;\n    filePath?: string;\n    categories?: string[];\n    categoryPath?: string;\n  };\n  providers: Record<string, UnknownObject>;\n  routing?: {\n    strategy: 'round-robin' | 'weighted' | 'least-loaded';\n    timeout?: number;\n    retryAttempts?: number;\n  };\n}\n\n/**\n * Provider configuration types\n */\nexport interface ProviderConfig {\n  id: string;\n  type: 'openai' | 'anthropic' | 'custom' | 'pass-through';\n  enabled: boolean;\n  baseUrl?: string;\n  apiKey?: string;\n  targetUrl?: string;\n  models: Record<string, ModelConfig>;\n  rateLimit?: {\n    requestsPerMinute: number;\n    tokensPerMinute: number;\n  };\n  timeout?: number;\n  retryAttempts?: number;\n  weight?: number;\n  headers?: Record<string, string>;\n}\n\nexport interface ModelConfig {\n  id: string;\n  maxTokens: number;\n  temperature?: number;\n  topP?: number;\n  enabled: boolean;\n  costPer1kTokens?: {\n    input: number;\n    output: number;\n  };\n  supportsStreaming?: boolean;\n  supportsTools?: boolean;\n  supportsVision?: boolean;\n  contextWindow?: number;\n}\n\n/**\n * Request context types\n */\nexport interface RequestContext {\n  id: string;\n  timestamp: number;\n  method: string;\n  url: string;\n  headers: Record<string, string>;\n  body: UnknownObject;\n  userAgent?: string;\n  ip?: string;\n  userId?: string;\n}\n\nexport interface ResponseContext {\n  id: string;\n  requestId: string;\n  timestamp: number;\n  status: number;\n  headers: Record<string, string>;\n  body: UnknownObject;\n  duration: number;\n  providerId?: string;\n  modelId?: string;\n  usage?: {\n    promptTokens: number;\n    completionTokens: number;\n    totalTokens: number;\n  };\n}\n\n/**\n * Error types\n */\nexport interface ServerError extends Error {\n  code: string;\n  status: number;\n  context?: Record<string, UnknownObject>;\n  providerError?: UnknownObject;\n}\n\nexport class RouteCodexError extends Error implements ServerError {\n  public code: string;\n  public status: number;\n  public context?: Record<string, UnknownObject>;\n  public providerError?: UnknownObject;\n\n  constructor(\n    message: string,\n    code: string,\n    status: number = 500,\n    context?: Record<string, UnknownObject>,\n    providerError?: UnknownObject\n  ) {\n    super(message);\n    this.name = 'RouteCodexError';\n    this.code = code;\n    this.status = status;\n    this.context = context;\n    this.providerError = providerError;\n  }\n}\n\n/**\n * Health check types\n */\nexport interface HealthStatus {\n  status: 'healthy' | 'unhealthy' | 'degraded';\n  timestamp: string;\n  uptime: number;\n  memory: NodeJS.MemoryUsage;\n  providers: Record<string, ProviderHealth>;\n}\n\nexport interface ProviderHealth {\n  status: 'healthy' | 'unhealthy' | 'unknown';\n  responseTime?: number;\n  lastCheck?: string;\n  error?: string;\n  consecutiveFailures: number;\n  lastSuccess?: string;\n}\n\nexport interface ProviderStats {\n  totalRequests: number;\n  successfulRequests: number;\n  failedRequests: number;\n  averageResponseTime: number;\n  totalTokensUsed: number;\n  lastRequest?: string;\n  lastError?: string;\n  uptime?: number;\n}\n\n/**\n * Event types\n */\nexport interface ServerEvent {\n  id: string;\n  type: 'request_start' | 'request_end' | 'error' | 'provider_change' | 'config_update';\n  timestamp: number;\n  moduleId: string;\n  data: UnknownObject;\n}\n\n/**\n * Module info extension\n */\nexport interface ServerModuleInfo extends ModuleInfo {\n  type: 'server';\n  capabilities: string[];\n  dependencies?: string[];\n}\n\n/**\n * HTTP server interface\n */\nexport interface IHttpServer {\n  start(): Promise<void>;\n  stop(): Promise<void>;\n  getStatus(): HealthStatus;\n  getModuleInfo(): ServerModuleInfo;\n}\n\n/**\n * Router interface\n */\nexport interface IRouter {\n  handleRequest(request: RequestContext): Promise<ResponseContext>;\n  getRoutes(): Array<{\n    method: string;\n    path: string;\n    handler: string;\n  }>;\n}\n\n/**\n * Middleware types\n */\nexport type MiddlewareFunction = (req: UnknownObject, res: UnknownObject, next: (err?: UnknownObject) => void) => void;\n\nexport interface MiddlewareConfig {\n  name: string;\n  priority: number;\n  middleware: MiddlewareFunction;\n  enabled: boolean;\n  paths?: string[];\n  methods?: string[];\n}\n\n/**\n * Rate limiting types\n */\nexport interface RateLimitInfo {\n  requests: number;\n  windowStart: number;\n  windowMs: number;\n  limit: number;\n  remaining: number;\n  reset: number;\n}\n\nexport interface RateLimitConfig {\n  windowMs: number;\n  max: number;\n  skip?: (req: UnknownObject) => boolean;\n  keyGenerator?: (req: UnknownObject) => string;\n  handler?: (req: UnknownObject, res: UnknownObject) => void;\n}\n\n/**\n * Streaming types\n */\nexport interface StreamOptions {\n  enabled: boolean;\n  chunkSize?: number;\n  timeout?: number;\n  onChunk?: (chunk: string) => void;\n  onError?: (error: Error) => void;\n  onComplete?: () => void;\n}\n\nexport interface StreamResponse {\n  id: string;\n  object: 'chat.completion.chunk';\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    delta: {\n      role?: string;\n      content?: string;\n      tool_calls?: OpenAIChatToolCall[];\n    };\n    finish_reason?: 'stop' | 'length' | 'tool_calls' | 'content_filter';\n  }>;\n}\n"
        },
        "src/server/utils/error-builder.ts": {
          "path": "src/server/utils/error-builder.ts",
          "size": 9186,
          "lines": 260,
          "imports": [
            "../types.js"
          ],
          "exports": [
            "ErrorDetails",
            "ErrorResponse",
            "ErrorBuilder"
          ],
          "classes": [
            "ErrorBuilder"
          ],
          "functions": [],
          "content": "/**\n * Error Builder Utility\n * Centralized error response construction for protocol handlers\n */\n\nimport { RouteCodexError } from '../types.js';\n\nexport interface ErrorDetails {\n  code?: string;\n  type?: string;\n  message?: string;\n  status?: number;\n  details?: Record<string, unknown>;\n  cause?: Error;\n}\n\nexport interface ErrorResponse {\n  status: number;\n  body: {\n    error: {\n      message: string;\n      type: string;\n      code: string;\n      param?: string | null;\n      details?: Record<string, unknown>;\n    };\n  };\n}\n\n/**\n * Centralized error response builder\n */\nexport class ErrorBuilder {\n  /**\n   * Build standardized error response\n   */\n  static buildError(error: unknown, requestId: string): ErrorResponse {\n    const e = error as Record<string, unknown>;\n\n    // Determine status code with precedence\n    const statusFromObj = this.extractStatus(e);\n    const routeCodexStatus = error instanceof RouteCodexError ? error.status : undefined;\n    const status = statusFromObj ?? routeCodexStatus ?? 500;\n\n    // Extract best-effort message from common shapes\n    const message = this.extractMessage(error, e);\n\n    // Derive type and code from error information\n    const { type, code } = this.deriveTypeAndCode(e, status, error);\n\n    // Build details object\n    const details = this.buildErrorDetails(error, e, requestId);\n\n    return {\n      status,\n      body: {\n        error: {\n          message,\n          type,\n          code,\n          param: null,\n          details: Object.keys(details).length ? details : undefined,\n        },\n      },\n    };\n  }\n\n  /**\n   * Extract status code from error object\n   */\n  private static extractStatus(e: Record<string, unknown>): number | undefined {\n    if (typeof e?.status === 'number') {\n      return e.status as number;\n    }\n    if (typeof e?.statusCode === 'number') {\n      return e.statusCode as number;\n    }\n    if (e?.response && typeof e.response === 'object' && e.response !== null && 'status' in e.response) {\n      const status = (e.response as Record<string, unknown>).status;\n      if (typeof status === 'number') {\n        return status;\n      }\n    }\n    return undefined;\n  }\n\n  /**\n   * Extract meaningful error message\n   */\n  private static extractMessage(error: unknown, e: Record<string, unknown>): string {\n    const response = e?.response as any;\n    const data = e?.data as any;\n    const upstreamMsg = response?.data?.error?.message\n      || response?.data?.message\n      || data?.error?.message\n      || data?.message\n      || (typeof e?.message === 'string' ? e.message : undefined);\n\n    let message = upstreamMsg ? String(upstreamMsg) : (error instanceof Error ? error.message : String(error));\n\n    // Guard against unhelpful stringification of objects\n    if (message && /^\\[object\\s+Object\\]$/.test(message)) {\n      const serializable = e?.response && typeof e.response === 'object' && e.response !== null && 'data' in e.response && (e.response as Record<string, unknown>).data && typeof (e.response as Record<string, unknown>).data === 'object' && (e.response as Record<string, unknown>).data !== null ? (e.response as Record<string, unknown>).data\n        : e?.error ? e.error\n        : e?.data ? e.data\n        : e?.details ? e.details\n        : e;\n      try {\n        message = JSON.stringify(serializable);\n      } catch {\n        message = 'Unknown error';\n      }\n    }\n\n    return message;\n  }\n\n  /**\n   * Derive error type and code from various sources\n   */\n  private static deriveTypeAndCode(e: Record<string, unknown>, status: number, originalError: unknown): { type: string; code: string } {\n    const providerKind = typeof e?.type === 'string' ? e.type : undefined;\n    const cause = (e && typeof e === 'object' && e !== null && 'cause' in e) ? (e as any).cause : undefined;\n    const causeCode: string | undefined = cause && typeof cause === 'object' && cause !== null && 'code' in cause && typeof (cause as any).code === 'string' ? (cause as any).code : undefined;\n\n    // Adjust status for known network cause codes\n    let finalStatus = status;\n    if (!this.extractStatus(e) && causeCode) {\n      const cc = causeCode.toUpperCase();\n      if (cc === 'ETIMEDOUT' || cc === 'UND_ERR_CONNECT_TIMEOUT') {\n        finalStatus = 504;\n      } else if (cc === 'ENOTFOUND' || cc === 'EAI_AGAIN') {\n        finalStatus = 502;\n      } else if (cc === 'ECONNREFUSED' || cc === 'ECONNRESET') {\n        finalStatus = 502;\n      } else if (cc.startsWith('CERT_') || cc.includes('TLS')) {\n        finalStatus = 502;\n      }\n    }\n\n    const rcxCode = originalError instanceof RouteCodexError ? originalError.code : undefined;\n    const upstreamCode = (e?.response as any)?.data?.error?.code || (typeof e?.code === 'string' ? e.code : undefined);\n\n    const mapStatusToType = (s: number): string => {\n      if (s === 400) {return 'bad_request';}\n      if (s === 401) {return 'unauthorized';}\n      if (s === 403) {return 'forbidden';}\n      if (s === 404) {return 'not_found';}\n      if (s === 408) {return 'request_timeout';}\n      if (s === 409) {return 'conflict';}\n      if (s === 422) {return 'unprocessable_entity';}\n      if (s === 429) {return 'rate_limit_exceeded';}\n      if (s >= 500) {return 'server_error';}\n      return 'internal_error';\n    };\n\n    const mapKindToType = (k?: string): string | undefined => {\n      if (!k) {return undefined;}\n      const m: Record<string, string> = {\n        network: 'network_error',\n        server: 'server_error',\n        timeout: 'request_timeout',\n        rate_limit: 'rate_limit_exceeded',\n      };\n      return m[k] || undefined;\n    };\n\n    const type = rcxCode || mapKindToType(providerKind) || (causeCode ? 'network_error' : mapStatusToType(finalStatus));\n    const code = (causeCode || upstreamCode || type);\n\n    return { type, code };\n  }\n\n  /**\n   * Build detailed error information for debugging\n   */\n  private static buildErrorDetails(error: unknown, e: Record<string, unknown>, requestId: string): Record<string, unknown> {\n    const details: Record<string, unknown> = {};\n\n    // Add retryable flag if present\n    if (typeof e?.retryable === 'boolean') {\n      details.retryable = e.retryable;\n    }\n\n    // Add upstream status if available\n    const statusFromObj = this.extractStatus(e);\n    if (typeof statusFromObj === 'number') {\n      details.upstreamStatus = statusFromObj;\n    }\n\n    // Add provider/upstream details\n    if (e?.details && typeof e.details === 'object' && e.details !== null) {\n      const d = e.details as Record<string, unknown>;\n      if ('provider' in d) {details.provider = d.provider;}\n      if ('upstream' in d) {details.upstream = d.upstream;}\n    } else if (e?.response && typeof e.response === 'object' && e.response !== null && 'data' in e.response) {\n      details.upstream = (e.response as Record<string, unknown>).data;\n    }\n\n    // Add network information if available\n    const rawCause = (e && typeof e === 'object' && e !== null && 'cause' in e) ? (e as any).cause : undefined;\n    const normalizedCauseCode: string | undefined = rawCause && typeof rawCause === 'object' && rawCause !== null && 'code' in rawCause && typeof rawCause.code === 'string' ? rawCause.code : undefined;\n\n    if (normalizedCauseCode || rawCause) {\n      details.network = {\n        code: normalizedCauseCode,\n        message: rawCause && typeof rawCause === 'object' && rawCause !== null && 'message' in rawCause ? (rawCause as Record<string, unknown>).message : undefined,\n        errno: rawCause && typeof rawCause === 'object' && rawCause !== null && 'errno' in rawCause ? (rawCause as Record<string, unknown>).errno : undefined,\n        syscall: rawCause && typeof rawCause === 'object' && rawCause !== null && 'syscall' in rawCause ? (rawCause as Record<string, unknown>).syscall : undefined,\n        hostname: rawCause && typeof rawCause === 'object' && rawCause !== null && 'hostname' in rawCause ? (rawCause as Record<string, unknown>).hostname : undefined,\n      };\n    }\n\n    // Add request ID for tracing\n    details.requestId = requestId;\n\n    return details;\n  }\n\n  /**\n   * Check for sandbox permission errors and normalize them\n   */\n  static checkSandboxPermissionError(error: unknown): ErrorResponse | null {\n    try {\n      // Import here to avoid circular dependencies\n      const { ErrorHandlingUtils } = require('../../utils/error-handling-utils.js');\n      const det = ErrorHandlingUtils.detectSandboxPermissionError(error);\n\n      if (det.isSandbox) {\n        const requestId = 'unknown';\n        const details: Record<string, unknown> = {\n          category: 'sandbox',\n          retryable: false,\n        };\n\n        if (det.reason) {\n          details.sandbox = { reason: det.reason };\n        }\n\n        return {\n          status: 500,\n          body: {\n            error: {\n              message: typeof (error as any)?.message === 'string' ? (error as any).message : 'Operation denied by sandbox or permission policy',\n              type: 'server_error',\n              code: 'sandbox_denied',\n              param: null,\n              details,\n            },\n          },\n        };\n      }\n    } catch {\n      // Ignore detection errors\n    }\n\n    return null;\n  }\n}\n"
        },
        "src/server/utils/error-context.ts": {
          "path": "src/server/utils/error-context.ts",
          "size": 7772,
          "lines": 274,
          "imports": [
            "os",
            "../types.js",
            "../../types/common-types.js"
          ],
          "exports": [
            "ErrorContext",
            "SystemEnvironment",
            "DetailedError",
            "ErrorContextBuilder",
            "EnhancedRouteCodexError"
          ],
          "classes": [
            "ErrorContextBuilder",
            "EnhancedRouteCodexError"
          ],
          "functions": [
            "arguments",
            "arguments",
            "arguments"
          ],
          "content": "/**\n * Detailed Error Context Builder\n *\n * Implements RouteCodex 9大架构原则中的快速死亡和暴露问题原则\n * 提供完整的错误上下文信息，包括模块、文件、函数、行号等详细信息\n */\n\nimport os from 'os';\nimport { RouteCodexError } from '../types.js';\nimport { type UnknownObject } from '../../types/common-types.js';\n\nexport interface ErrorContext extends UnknownObject {\n  module: string;        // 模块名称\n  file?: string;         // 文件路径\n  function?: string;     // 函数名称\n  line?: number;         // 行号\n  requestId?: string;    // 请求ID\n  additional?: any;      // 额外上下文\n}\n\nexport interface SystemEnvironment {\n  nodeVersion: string;\n  platform: string;\n  arch: string;\n  memory: NodeJS.MemoryUsage;\n  uptime: number;\n  loadAverage: number[];\n}\n\nexport interface DetailedError {\n  error: {\n    code: string;\n    message: string;\n    type: string;\n    context: {\n      module: string;\n      file: string;\n      function: string;\n      line: number;\n      requestId: string;\n      timestamp: string;\n      environment: SystemEnvironment;\n      stack?: string;\n      additional?: any;\n    };\n  };\n  debug: string[];\n  source: string;\n  remediation: {\n    immediate: string;\n    documentation: string;\n    support: string;\n  };\n}\n\n/**\n * 错误上下文构建器\n * 符合原则4(快速死亡)和原则5(暴露问题)\n */\nexport class ErrorContextBuilder {\n\n  /**\n   * 构建详细的错误上下文\n   */\n  static buildErrorContext(error: Error, context: ErrorContext): DetailedError {\n\n    return {\n      error: {\n        code: this.getErrorCode(error),\n        message: error.message,\n        type: error.constructor.name,\n\n        // 完整的上下文信息 - 符合暴露问题原则\n        context: {\n          module: context.module,\n          file: context.file || 'unknown',\n          function: context.function || 'unknown',\n          line: context.line || 0,\n          requestId: context.requestId || 'unknown',\n          timestamp: new Date().toISOString(),\n\n          // 系统环境信息\n          environment: this.getSystemEnvironment(),\n\n          // 错误堆栈追踪 - 快速死亡原则要求完整信息\n          stack: error.stack,\n\n          // 额外调试信息\n          additional: context.additional\n        }\n      },\n\n      // 调试建议\n      debug: this.generateDebugSuggestions(error, context),\n\n      // 明确错误来源\n      source: 'RouteCodex-Enhanced',\n\n      // 修复建议\n      remediation: {\n        immediate: this.getImmediateAction(error),\n        documentation: '参考CLAUDE.md中的9大架构原则和错误处理章节',\n        support: '如需进一步帮助，请提供完整的错误信息和requestId'\n      }\n    };\n  }\n\n  /**\n   * 获取系统环境信息\n   */\n  private static getSystemEnvironment(): SystemEnvironment {\n    return {\n      nodeVersion: process.version,\n      platform: os.platform(),\n      arch: os.arch(),\n      memory: process.memoryUsage(),\n      uptime: os.uptime(),\n      loadAverage: os.loadavg()\n    };\n  }\n\n  /**\n   * 获取标准化错误代码\n   */\n  private static getErrorCode(error: Error): string {\n    const message = error.message.toLowerCase();\n\n    if (message.includes('parse function arguments')) {\n      return 'TOOL_ARGUMENT_PARSE_ERROR';\n    }\n    if (message.includes('500') || message.includes('internal server error')) {\n      return 'INTERNAL_SERVER_ERROR';\n    }\n    if (message.includes('timeout')) {\n      return 'TIMEOUT_ERROR';\n    }\n    if (message.includes('connection')) {\n      return 'CONNECTION_ERROR';\n    }\n    if (message.includes('validation')) {\n      return 'VALIDATION_ERROR';\n    }\n\n    return 'UNKNOWN_ERROR';\n  }\n\n  /**\n   * 生成调试建议\n   * 基于错误类型和上下文提供具体建议\n   */\n  private static generateDebugSuggestions(error: Error, context: ErrorContext): string[] {\n    const suggestions = [];\n    const message = error.message.toLowerCase();\n\n    // 工具调用参数解析错误\n    if (message.includes('parse function arguments') || message.includes('expected a sequence')) {\n      suggestions.push('检查工具调用参数格式是否符合OpenAI规范');\n      suggestions.push('确认llmswitch-core参数处理逻辑');\n      suggestions.push('验证shell工具的command参数是否为数组格式');\n      suggestions.push('检查是否存在参数重复编码问题');\n    }\n\n    // 500错误\n    if (message.includes('500') || message.includes('operation failed')) {\n      suggestions.push('检查Provider连接状态和配置');\n      suggestions.push('查看Provider错误日志');\n      suggestions.push('验证网络连接和API密钥');\n      suggestions.push('检查请求频率是否超过限制');\n    }\n\n    // 连接错误\n    if (message.includes('connection') || message.includes('connect')) {\n      suggestions.push('检查目标服务是否运行');\n      suggestions.push('验证端口是否正确');\n      suggestions.push('检查防火墙设置');\n      suggestions.push('确认网络可达性');\n    }\n\n    // 验证错误\n    if (message.includes('validation')) {\n      suggestions.push('检查请求参数格式');\n      suggestions.push('验证必需字段是否缺失');\n      suggestions.push('确认参数类型是否正确');\n    }\n\n    // 超时错误\n    if (message.includes('timeout')) {\n      suggestions.push('增加超时时间配置');\n      suggestions.push('检查网络延迟');\n      suggestions.push('优化处理逻辑减少执行时间');\n    }\n\n    // 模块特定建议\n    if (context.module === 'GLMCompatibility') {\n      suggestions.push('检查GLM特定的字段标准化逻辑');\n      suggestions.push('确认reasoning_content处理');\n      suggestions.push('验证schema字段转换');\n    }\n\n    if (context.module === 'ChatCompletionsHandler') {\n      suggestions.push('检查OpenAI请求格式验证');\n      suggestions.push('确认流式响应处理逻辑');\n      suggestions.push('验证工具调用参数序列化');\n    }\n\n    return suggestions;\n  }\n\n  /**\n   * 获取立即行动建议\n   */\n  private static getImmediateAction(error: Error): string {\n    const message = error.message.toLowerCase();\n\n    if (message.includes('parse function arguments')) {\n      return '检查工具调用参数格式，确保command为数组类型';\n    }\n\n    if (message.includes('500')) {\n      return '检查Provider服务状态和网络连接';\n    }\n\n    if (message.includes('connection')) {\n      return '检查目标服务是否运行在正确端口';\n    }\n\n    return '查看详细错误上下文和调试建议';\n  }\n\n  /**\n   * 从错误堆栈中提取行号\n   */\n  static extractLineNumber(error: Error): number {\n    if (!error.stack) {return 0;}\n\n    const stackLines = error.stack.split('\\n');\n    for (const line of stackLines) {\n      const match = line.match(/:(\\d+):\\d+\\)?$/);\n      if (match) {\n        return parseInt(match[1], 10);\n      }\n    }\n\n    return 0;\n  }\n\n  /**\n   * 安全地序列化对象（限制大小）\n   */\n  static safeStringify(obj: any, maxSize: number = 1000): string {\n    try {\n      const str = JSON.stringify(obj, null, 2);\n      return str.length > maxSize ? `${str.substring(0, maxSize)  }...` : str;\n    } catch {\n      return '[无法序列化的对象]';\n    }\n  }\n}\n\n/**\n * 增强的RouteCodex错误类\n */\nexport class EnhancedRouteCodexError extends RouteCodexError {\n  public readonly detailedError: DetailedError;\n\n  constructor(error: Error, context: ErrorContext) {\n    super(error.message, 'ENHANCED_ERROR', 500, context as Record<string, UnknownObject>);\n    this.detailedError = ErrorContextBuilder.buildErrorContext(error, context);\n  }\n\n  toJSON(): DetailedError {\n    return this.detailedError;\n  }\n}"
        },
        "src/server/utils/request-validator.ts": {
          "path": "src/server/utils/request-validator.ts",
          "size": 17722,
          "lines": 574,
          "imports": [
            "../types.js",
            "../types.js"
          ],
          "exports": [
            "ValidationResult",
            "ValidationOptions",
            "RequestValidator"
          ],
          "classes": [
            "RequestValidator"
          ],
          "functions": [
            "as",
            "at",
            "arguments",
            "as",
            "at",
            "description",
            "parameters",
            "must",
            "object",
            "as",
            "must"
          ],
          "content": "/**\n * Request Validator Utility\n * Provides centralized request validation for all protocol handlers\n */\n\nimport { type OpenAIChatCompletionRequest } from '../types.js';\nimport { RouteCodexError } from '../types.js';\n\n/**\n * Validation result interface\n */\nexport interface ValidationResult {\n  isValid: boolean;\n  errors: string[];\n}\n\n/**\n * Request validation options\n */\nexport interface ValidationOptions {\n  strictMode?: boolean;\n  allowUnknownFields?: boolean;\n  maxMessageCount?: number;\n  maxTokens?: number;\n}\n\n/**\n * Default validation options\n */\nconst DEFAULT_OPTIONS: ValidationOptions = {\n  strictMode: true,\n  allowUnknownFields: false,\n  // Do not enforce a hard cap in the proxy; upstream provider should decide.\n  maxMessageCount: undefined,\n  maxTokens: 32768,\n};\n\n/**\n * Request Validator Class\n */\nexport class RequestValidator {\n  private options: ValidationOptions;\n\n  constructor(options: ValidationOptions = {}) {\n    this.options = { ...DEFAULT_OPTIONS, ...options };\n  }\n\n  /**\n   * Validate chat completion request\n   */\n  validateChatCompletion(request: unknown): ValidationResult {\n    const errors: string[] = [];\n\n    // Basic type validation\n    if (!request || typeof request !== 'object') {\n      errors.push('Request must be a valid object');\n      return { isValid: false, errors };\n    }\n\n    const req = request as Record<string, unknown>;\n\n    // Required fields validation\n    if (!req.model || typeof req.model !== 'string') {\n      errors.push('model field is required and must be a string');\n    }\n\n    if (!req.messages || !Array.isArray(req.messages)) {\n      errors.push('messages field is required and must be an array');\n    } else {\n      const messageErrors = this.validateMessagesArray(req.messages);\n      errors.push(...messageErrors);\n    }\n\n    // Optional fields validation\n    if (req.max_tokens !== undefined) {\n      if (typeof req.max_tokens !== 'number' || req.max_tokens <= 0) {\n        errors.push('max_tokens must be a positive number');\n      } else if (req.max_tokens > this.options.maxTokens!) {\n        errors.push(`max_tokens cannot exceed ${this.options.maxTokens}`);\n      }\n    }\n\n    if (req.temperature !== undefined) {\n      if (typeof req.temperature !== 'number' || req.temperature < 0 || req.temperature > 2) {\n        errors.push('temperature must be a number between 0 and 2');\n      }\n    }\n\n    if (req.top_p !== undefined) {\n      if (typeof req.top_p !== 'number' || req.top_p <= 0 || req.top_p > 1) {\n        errors.push('top_p must be a number between 0 and 1');\n      }\n    }\n\n    if (req.stream !== undefined && typeof req.stream !== 'boolean') {\n      errors.push('stream must be a boolean');\n    }\n\n    if (req.tools !== undefined) {\n      if (!Array.isArray(req.tools)) {\n        errors.push('tools must be an array');\n      } else {\n        const toolErrors = this.validateTools(req.tools);\n        errors.push(...toolErrors);\n      }\n    }\n\n    if (req.tool_choice !== undefined) {\n      const toolChoiceErrors = this.validateToolChoice(req.tool_choice);\n      errors.push(...toolChoiceErrors);\n    }\n\n    return {\n      isValid: errors.length === 0,\n      errors,\n    };\n  }\n\n  /**\n   * Validate messages array\n   */\n  private validateMessagesArray(messages: unknown[]): string[] {\n    const errors: string[] = [];\n\n    if (messages.length === 0) {\n      errors.push('messages array cannot be empty');\n    }\n\n    if (typeof this.options.maxMessageCount === 'number' && messages.length > this.options.maxMessageCount) {\n      errors.push(`messages array cannot exceed ${this.options.maxMessageCount} items`);\n    }\n\n    for (let i = 0; i < messages.length; i++) {\n      const message = messages[i];\n      const messageErrors = this.validateMessage(message, i);\n      errors.push(...messageErrors);\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate individual message\n   */\n  private validateMessage(message: unknown, index: number): string[] {\n    const errors: string[] = [];\n\n    if (!message || typeof message !== 'object') {\n      errors.push(`Message at index ${index} must be a valid object`);\n      return errors;\n    }\n\n    const msg = message as Record<string, unknown>;\n\n    // Role validation\n    if (!msg.role || typeof msg.role !== 'string') {\n      errors.push(`Message at index ${index} must have a role field`);\n    } else if (!['system', 'user', 'assistant', 'tool'].includes(msg.role)) {\n      errors.push(`Message at index ${index} has invalid role: ${msg.role}`);\n    }\n\n    // Content validation\n    if (msg.content !== undefined && msg.content !== null) {\n      if (typeof msg.content !== 'string') {\n        errors.push(`Message at index ${index} content must be a string or null`);\n      }\n    }\n\n    // Tool call validation for assistant messages\n    if (msg.role === 'assistant' && msg.tool_calls) {\n      if (!Array.isArray(msg.tool_calls)) {\n        errors.push(`Message at index ${index} tool_calls must be an array`);\n      } else {\n        msg.tool_calls.forEach((toolCall: unknown, toolIndex: number) => {\n          const toolCallErrors = this.validateToolCall(toolCall, index, toolIndex);\n          errors.push(...toolCallErrors);\n        });\n      }\n    }\n\n    // Tool call ID validation for tool messages\n    if (msg.role === 'tool') {\n      if (!msg.tool_call_id || typeof msg.tool_call_id !== 'string') {\n        errors.push(`Tool message at index ${index} must have tool_call_id`);\n      }\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate tool call\n   */\n  private validateToolCall(toolCall: unknown, messageIndex: number, toolIndex: number): string[] {\n    const errors: string[] = [];\n\n    if (!toolCall || typeof toolCall !== 'object') {\n      errors.push(`Tool call at message ${messageIndex}, tool ${toolIndex} must be a valid object`);\n      return errors;\n    }\n\n    const tc = toolCall as Record<string, unknown>;\n\n    if (!tc.id || typeof tc.id !== 'string') {\n      errors.push(`Tool call at message ${messageIndex}, tool ${toolIndex} must have an id`);\n    }\n\n    if (!tc.type || typeof tc.type !== 'string') {\n      errors.push(`Tool call at message ${messageIndex}, tool ${toolIndex} must have a type`);\n    } else if (tc.type !== 'function') {\n      errors.push(`Tool call at message ${messageIndex}, tool ${toolIndex} type must be 'function'`);\n    }\n\n    if (!tc.function || typeof tc.function !== 'object') {\n      errors.push(`Tool call at message ${messageIndex}, tool ${toolIndex} must have a function`);\n    } else {\n      const func = tc.function as Record<string, unknown>;\n\n      if (!func.name || typeof func.name !== 'string') {\n        errors.push(`Tool function at message ${messageIndex}, tool ${toolIndex} must have a name`);\n      }\n\n      if (func.arguments !== undefined && typeof func.arguments !== 'string') {\n        errors.push(`Tool function arguments at message ${messageIndex}, tool ${toolIndex} must be a string`);\n      }\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate tools array\n   */\n  private validateTools(tools: unknown[]): string[] {\n    const errors: string[] = [];\n\n    for (let i = 0; i < tools.length; i++) {\n      const tool = tools[i];\n      const toolErrors = this.validateTool(tool, i);\n      errors.push(...toolErrors);\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate individual tool\n   */\n  private validateTool(tool: unknown, index: number): string[] {\n    const errors: string[] = [];\n\n    if (!tool || typeof tool !== 'object') {\n      errors.push(`Tool at index ${index} must be a valid object`);\n      return errors;\n    }\n\n    const t = tool as Record<string, unknown>;\n\n    if (!t.type || typeof t.type !== 'string') {\n      errors.push(`Tool at index ${index} must have a type`);\n    } else if (t.type !== 'function') {\n      errors.push(`Tool at index ${index} type must be 'function'`);\n    }\n\n    if (!t.function || typeof t.function !== 'object') {\n      errors.push(`Tool at index ${index} must have a function`);\n    } else {\n      const func = t.function as Record<string, unknown>;\n\n      if (!func.name || typeof func.name !== 'string') {\n        errors.push(`Tool function at index ${index} must have a name`);\n      }\n\n      if (func.description && typeof func.description !== 'string') {\n        errors.push(`Tool function description at index ${index} must be a string`);\n      }\n\n      if (func.parameters) {\n        if (typeof func.parameters !== 'object') {\n          errors.push(`Tool function parameters at index ${index} must be an object`);\n        } else {\n          const schemaErrors = this.validateJSONSchema(func.parameters, `tool.${index}.function.parameters`);\n          errors.push(...schemaErrors);\n        }\n      }\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate tool choice\n   */\n  private validateToolChoice(toolChoice: unknown): string[] {\n    const errors: string[] = [];\n\n    if (typeof toolChoice === 'string') {\n      if (!['none', 'auto', 'required'].includes(toolChoice)) {\n        errors.push(`tool_choice string must be one of: none, auto, required`);\n      }\n    } else if (typeof toolChoice === 'object' && toolChoice !== null) {\n      const tc = toolChoice as Record<string, unknown>;\n\n      if (!tc.type || typeof tc.type !== 'string') {\n        errors.push(`tool_choice object must have a type field`);\n      } else if (tc.type !== 'function') {\n        errors.push(`tool_choice object type must be 'function'`);\n      }\n\n      if (tc.type === 'function') {\n        if (!tc.function || typeof tc.function !== 'object') {\n          errors.push(`tool_choice function must have a function object`);\n        } else {\n          const func = tc.function as Record<string, unknown>;\n\n          if (!func.name || typeof func.name !== 'string') {\n            errors.push(`tool_choice function must have a name`);\n          }\n        }\n      }\n    } else {\n      errors.push(`tool_choice must be a string or object`);\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate JSON Schema\n   */\n  private validateJSONSchema(schema: unknown, context: string): string[] {\n    const errors: string[] = [];\n\n    if (!schema || typeof schema !== 'object') {\n      errors.push(`${context} must be a valid object`);\n      return errors;\n    }\n\n    const s = schema as Record<string, unknown>;\n\n    if (s.type !== undefined && typeof s.type !== 'string') {\n      errors.push(`${context}.type must be a string`);\n    }\n\n    if (s.properties !== undefined) {\n      if (typeof s.properties !== 'object') {\n        errors.push(`${context}.properties must be an object`);\n      }\n    }\n\n    if (s.required !== undefined) {\n      if (!Array.isArray(s.required) || s.required.some(item => typeof item !== 'string')) {\n        errors.push(`${context}.required must be an array of strings`);\n      }\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate completion request (non-chat)\n   */\n  validateCompletion(request: unknown): ValidationResult {\n    const errors: string[] = [];\n\n    if (!request || typeof request !== 'object') {\n      errors.push('Request must be a valid object');\n      return { isValid: false, errors };\n    }\n\n    const req = request as Record<string, unknown>;\n\n    if (!req.model || typeof req.model !== 'string') {\n      errors.push('model field is required and must be a string');\n    }\n\n    if (!req.prompt || typeof req.prompt !== 'string') {\n      errors.push('prompt field is required and must be a string');\n    }\n\n    if (req.max_tokens !== undefined) {\n      if (typeof req.max_tokens !== 'number' || req.max_tokens <= 0) {\n        errors.push('max_tokens must be a positive number');\n      }\n    }\n\n    if (req.temperature !== undefined) {\n      if (typeof req.temperature !== 'number' || req.temperature < 0 || req.temperature > 2) {\n        errors.push('temperature must be a number between 0 and 2');\n      }\n    }\n\n    return {\n      isValid: errors.length === 0,\n      errors,\n    };\n  }\n\n  /**\n   * Validate embedding request\n   */\n  validateEmbedding(request: unknown): ValidationResult {\n    const errors: string[] = [];\n\n    if (!request || typeof request !== 'object') {\n      errors.push('Request must be a valid object');\n      return { isValid: false, errors };\n    }\n\n    const req = request as Record<string, unknown>;\n\n    if (!req.model || typeof req.model !== 'string') {\n      errors.push('model field is required and must be a string');\n    }\n\n    if (!req.input) {\n      errors.push('input field is required');\n    } else if (typeof req.input !== 'string' && !Array.isArray(req.input)) {\n      errors.push('input must be a string or array of strings');\n    } else if (Array.isArray(req.input) && req.input.some(item => typeof item !== 'string')) {\n      errors.push('all items in input array must be strings');\n    }\n\n    return {\n      isValid: errors.length === 0,\n      errors,\n    };\n  }\n\n  /**\n   * Validate Anthropic messages request\n   */\n  validateMessages(request: unknown): ValidationResult {\n    const errors: string[] = [];\n\n    if (!request || typeof request !== 'object') {\n      errors.push('Request must be a valid object');\n      return { isValid: false, errors };\n    }\n\n    const req = request as Record<string, unknown>;\n\n    // Required fields\n    if (!req.model || typeof req.model !== 'string') {\n      errors.push('model field is required and must be a string');\n    }\n\n    if (!req.messages || !Array.isArray(req.messages)) {\n      errors.push('messages field is required and must be an array');\n    } else {\n      if (req.messages.length === 0) {\n        errors.push('messages array cannot be empty');\n      }\n\n      if (typeof this.options.maxMessageCount === 'number' && req.messages.length > this.options.maxMessageCount) {\n        errors.push(`messages array cannot exceed ${this.options.maxMessageCount} items`);\n      }\n\n      for (let i = 0; i < req.messages.length; i++) {\n        const message = req.messages[i];\n        const messageErrors = this.validateAnthropicMessage(message, i);\n        errors.push(...messageErrors);\n      }\n    }\n\n    // Optional fields\n    if (req.max_tokens !== undefined) {\n      if (typeof req.max_tokens !== 'number' || req.max_tokens <= 0) {\n        errors.push('max_tokens must be a positive number');\n      }\n    }\n\n    if (req.temperature !== undefined) {\n      if (typeof req.temperature !== 'number' || req.temperature < 0 || req.temperature > 1) {\n        errors.push('temperature must be a number between 0 and 1');\n      }\n    }\n\n    if (req.top_p !== undefined) {\n      if (typeof req.top_p !== 'number' || req.top_p <= 0 || req.top_p > 1) {\n        errors.push('top_p must be a number between 0 and 1');\n      }\n    }\n\n    if (req.stream !== undefined && typeof req.stream !== 'boolean') {\n      errors.push('stream must be a boolean');\n    }\n\n    return {\n      isValid: errors.length === 0,\n      errors,\n    };\n  }\n\n  /**\n   * Validate Anthropic response request\n   */\n  validateAnthropicResponse(request: unknown): ValidationResult {\n    // Anthropic responses use the same format as messages\n    return this.validateMessages(request);\n  }\n\n  /**\n   * Validate individual Anthropic message\n   */\n  private validateAnthropicMessage(message: unknown, index: number): string[] {\n    const errors: string[] = [];\n\n    if (!message || typeof message !== 'object') {\n      errors.push(`Message at index ${index} must be a valid object`);\n      return errors;\n    }\n\n    const msg = message as Record<string, unknown>;\n\n    // Role validation\n    if (!msg.role || typeof msg.role !== 'string') {\n      errors.push(`Message at index ${index} must have a role field`);\n    } else if (!['user', 'assistant', 'system'].includes(msg.role)) {\n      errors.push(`Message at index ${index} has invalid role: ${msg.role}`);\n    }\n\n    // Content validation\n    if (!msg.content) {\n      errors.push(`Message at index ${index} must have content`);\n    } else if (typeof msg.content !== 'string' && !Array.isArray(msg.content)) {\n      errors.push(`Message at index ${index} content must be a string or array`);\n    } else if (Array.isArray(msg.content)) {\n      for (let i = 0; i < msg.content.length; i++) {\n        const content = msg.content[i];\n        const contentErrors = this.validateAnthropicContent(content, index, i);\n        errors.push(...contentErrors);\n      }\n    }\n\n    return errors;\n  }\n\n  /**\n   * Validate Anthropic content block\n   */\n  private validateAnthropicContent(content: unknown, messageIndex: number, contentIndex: number): string[] {\n    const errors: string[] = [];\n\n    if (!content || typeof content !== 'object') {\n      errors.push(`Content at message ${messageIndex}, block ${contentIndex} must be a valid object`);\n      return errors;\n    }\n\n    const c = content as Record<string, unknown>;\n\n    if (!c.type || typeof c.type !== 'string') {\n      errors.push(`Content at message ${messageIndex}, block ${contentIndex} must have a type`);\n    } else if (!['text', 'image', 'tool_use', 'tool_result', 'message', 'reasoning', 'function_call', 'function_call_output'].includes(c.type)) {\n      // For unknown/extended types, do not fail hard; allow pipeline/adapters to filter.\n      // errors.push(`Content at message ${messageIndex}, block ${contentIndex} has invalid type: ${c.type}`);\n    }\n\n    if (c.type === 'text' && !c.text) {\n      errors.push(`Text content at message ${messageIndex}, block ${contentIndex} must have text`);\n    }\n\n    if (c.type === 'image' && (!c.source || typeof c.source !== 'object')) {\n      errors.push(`Image content at message ${messageIndex}, block ${contentIndex} must have a source`);\n    }\n\n    if (c.type === 'tool_use' && (!c.id || typeof c.id !== 'string')) {\n      errors.push(`Tool use content at message ${messageIndex}, block ${contentIndex} must have an id`);\n    }\n\n    return errors;\n  }\n}\n"
        },
        "src/server/utils/response-normalizer.ts": {
          "path": "src/server/utils/response-normalizer.ts",
          "size": 8823,
          "lines": 309,
          "imports": [
            "../../modules/pipeline/utils/debug-logger.js"
          ],
          "exports": [
            "NormalizedResponse",
            "ResponseNormalizer"
          ],
          "classes": [
            "ResponseNormalizer"
          ],
          "functions": [],
          "content": "/**\n * Response Normalizer Utility\n * Normalizes responses from different AI providers to standard formats\n */\n\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\n\n/**\n * Normalized response interface\n */\nexport interface NormalizedResponse {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: any[];\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n  error?: {\n    message: string;\n    type: string;\n    code: string;\n  };\n}\n\n/**\n * Response Normalizer Class\n */\nexport class ResponseNormalizer {\n  private logger: PipelineDebugLogger;\n\n  constructor(logger?: PipelineDebugLogger) {\n    this.logger = logger || new PipelineDebugLogger(null, {\n      enableConsoleLogging: true,\n      enableDebugCenter: false,\n    });\n  }\n\n  /**\n   * Normalize OpenAI chat completion response\n   */\n  normalizeOpenAIResponse(response: any, type: 'chat' | 'completion' = 'chat'): NormalizedResponse {\n    try {\n      this.logger.logModule('ResponseNormalizer', 'normalize_openai_start', {\n        type,\n        responseId: response.id,\n        model: response.model,\n      });\n\n      const normalized: NormalizedResponse = {\n        id: response.id || `resp_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,\n        object: response.object || (type === 'chat' ? 'chat.completion' : 'text_completion'),\n        created: response.created || Math.floor(Date.now() / 1000),\n        model: response.model || 'unknown',\n        choices: this.normalizeChoices(response.choices || []),\n      };\n\n      // Normalize usage information (fallback to input/output tokens for compat providers)\n      if (response.usage) {\n        const u = response.usage as Record<string, unknown>;\n        const prompt = (typeof u.prompt_tokens === 'number')\n          ? (u.prompt_tokens as number)\n          : (typeof u.input_tokens === 'number' ? (u.input_tokens as number) : 0);\n        const completion = (typeof u.completion_tokens === 'number')\n          ? (u.completion_tokens as number)\n          : (typeof u.output_tokens === 'number' ? (u.output_tokens as number) : 0);\n        const total = (typeof u.total_tokens === 'number')\n          ? (u.total_tokens as number)\n          : (prompt + completion);\n        normalized.usage = {\n          prompt_tokens: prompt,\n          completion_tokens: completion,\n          total_tokens: total,\n        };\n      }\n\n      this.logger.logModule('ResponseNormalizer', 'normalize_openai_complete', {\n        responseId: normalized.id,\n        choiceCount: normalized.choices.length,\n      });\n\n      return normalized;\n    } catch (error) {\n      this.logger.logModule('ResponseNormalizer', 'normalize_openai_error', {\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      return this.createErrorResponse(error instanceof Error ? error.message : String(error));\n    }\n  }\n\n  /**\n   * Normalize Anthropic messages response to OpenAI format\n   */\n  normalizeAnthropicResponse(response: any): NormalizedResponse {\n    try {\n      this.logger.logModule('ResponseNormalizer', 'normalize_anthropic_start', {\n        responseId: response.id,\n      });\n\n      const normalized: NormalizedResponse = {\n        id: response.id || `resp_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,\n        object: 'chat.completion',\n        created: Math.floor(Date.now() / 1000),\n        model: response.model || 'unknown',\n        choices: this.normalizeAnthropicChoices(response),\n      };\n\n      // Extract usage information from Anthropic response\n      if (response.usage) {\n        normalized.usage = {\n          prompt_tokens: response.usage.input_tokens || 0,\n          completion_tokens: response.usage.output_tokens || 0,\n          total_tokens: (response.usage.input_tokens || 0) + (response.usage.output_tokens || 0),\n        };\n      }\n\n      this.logger.logModule('ResponseNormalizer', 'normalize_anthropic_complete', {\n        responseId: normalized.id,\n      });\n\n      return normalized;\n    } catch (error) {\n      this.logger.logModule('ResponseNormalizer', 'normalize_anthropic_error', {\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      return this.createErrorResponse(error instanceof Error ? error.message : String(error));\n    }\n  }\n\n  /**\n   * Normalize response choices array\n   */\n  private normalizeChoices(choices: any[]): any[] {\n    return choices.map((choice, index) => ({\n      index: choice.index || index,\n      message: choice.message || {\n        role: 'assistant',\n        content: choice.text || '',\n      },\n      finish_reason: choice.finish_reason || 'stop',\n      delta: choice.delta || null,\n    }));\n  }\n\n  /**\n   * Normalize Anthropic response to OpenAI choices format\n   */\n  private normalizeAnthropicChoices(response: any): any[] {\n    const choices: any[] = [];\n\n    if (response.content) {\n      choices.push({\n        index: 0,\n        message: {\n          role: 'assistant',\n          content: typeof response.content === 'string'\n            ? response.content\n            : response.content\n                .filter((item: any) => item.type === 'text')\n                .map((item: any) => item.text)\n                .join('\\n'),\n        },\n        finish_reason: response.stop_reason ? this.mapAnthropicStopReason(response.stop_reason) : 'stop',\n      });\n    }\n\n    // Handle tool calls if present\n    if (response.content && response.content.some((item: any) => item.type === 'tool_use')) {\n      const toolCalls = response.content\n        .filter((item: any) => item.type === 'tool_use')\n        .map((tool: any) => ({\n          id: tool.id,\n          type: 'function',\n          function: {\n            name: tool.name,\n            arguments: JSON.stringify(tool.input || {}),\n          },\n        }));\n\n      if (choices.length > 0) {\n        choices[0].message.tool_calls = toolCalls;\n      }\n    }\n\n    return choices;\n  }\n\n  /**\n   * Map Anthropic stop reasons to OpenAI finish reasons\n   */\n  private mapAnthropicStopReason(reason: string): string {\n    const mapping: Record<string, string> = {\n      'end_turn': 'stop',\n      'max_tokens': 'length',\n      'stop_sequence': 'stop',\n      'tool_use': 'tool_calls',\n    };\n\n    return mapping[reason] || 'stop';\n  }\n\n  /**\n   * Create error response\n   */\n  private createErrorResponse(message: string): NormalizedResponse {\n    return {\n      id: `error_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,\n      object: 'chat.completion',\n      created: Math.floor(Date.now() / 1000),\n      model: 'error',\n      choices: [],\n      error: {\n        message,\n        type: 'normalization_error',\n        code: 'RESPONSE_NORMALIZATION_FAILED',\n      },\n    };\n  }\n\n  /**\n   * Validate normalized response\n   */\n  validateResponse(response: NormalizedResponse): boolean {\n    try {\n      // Check required fields\n      if (!response.id || !response.object || !response.model) {\n        return false;\n      }\n\n      // Check choices array\n      if (!Array.isArray(response.choices) || response.choices.length === 0) {\n        return false;\n      }\n\n      // Check each choice\n      for (const choice of response.choices) {\n        if (typeof choice.index !== 'number' || !choice.message) {\n          return false;\n        }\n      }\n\n      return true;\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Apply response formatting based on configuration\n   */\n  formatResponse(response: NormalizedResponse, config: {\n    includeUsage?: boolean;\n    trimWhitespace?: boolean;\n    sanitizeContent?: boolean;\n  } = {}): NormalizedResponse {\n    const formatted = { ...response };\n\n    // Include/exclude usage information\n    if (config.includeUsage === false && formatted.usage) {\n      delete formatted.usage;\n    }\n\n    // Trim whitespace from content\n    if (config.trimWhitespace) {\n      formatted.choices = formatted.choices.map(choice => ({\n        ...choice,\n        message: {\n          ...choice.message,\n          content: typeof choice.message.content === 'string'\n            ? choice.message.content.trim()\n            : choice.message.content,\n        },\n      }));\n    }\n\n    // Sanitize content if needed\n    if (config.sanitizeContent) {\n      formatted.choices = formatted.choices.map(choice => ({\n        ...choice,\n        message: {\n          ...choice.message,\n          content: this.sanitizeContent(choice.message.content),\n        },\n      }));\n    }\n\n    return formatted;\n  }\n\n  /**\n   * Sanitize content for safety\n   */\n  private sanitizeContent(content: any): any {\n    if (typeof content === 'string') {\n      // Basic sanitization - can be extended\n      return content\n        .replace(/[\\x00-\\x1F\\x7F]/g, '') // Remove control characters\n        .trim();\n    }\n\n    return content;\n  }\n}\n"
        },
        "src/server/utils/streaming-manager.ts": {
          "path": "src/server/utils/streaming-manager.ts",
          "size": 30916,
          "lines": 791,
          "imports": [
            "express",
            "../handlers/base-handler.js",
            "../../modules/pipeline/utils/debug-logger.js",
            "./text-filters.js",
            "rcc-llmswitch-core/conversion/shared/tooling",
            "./error-context.js",
            "node:os",
            "node:path",
            "node:fs"
          ],
          "exports": [
            "StreamingChunk",
            "StreamingManager"
          ],
          "classes": [
            "StreamingManager"
          ],
          "functions": [
            "emitChatRequiredAction",
            "genCallId",
            "synthesizeFromResponse",
            "toolCalls",
            "pushChunk",
            "as",
            "as",
            "toolCalls",
            "filteredToolCalls",
            "cleanse",
            "writeBeat"
          ],
          "content": "/**\n * Streaming Manager Utility\n * Handles streaming responses for different protocols\n */\n\nimport { type Response } from 'express';\nimport type { ProtocolHandlerConfig } from '../handlers/base-handler.js';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\nimport { stripThinkingTags } from './text-filters.js';\nimport { chunkString } from 'rcc-llmswitch-core/conversion/shared/tooling';\nimport { ErrorContextBuilder, EnhancedRouteCodexError, type ErrorContext } from './error-context.js';\nimport os from 'node:os';\nimport path from 'node:path';\nimport fs from 'node:fs';\n\n/**\n * Streaming chunk interface\n */\nexport interface StreamingChunk {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    delta: {\n      content?: string;\n      role?: string;\n      tool_calls?: any[];\n    };\n    finish_reason?: string | null;\n  }>;\n}\n\n/**\n * Streaming Manager Class\n */\nexport class StreamingManager {\n  private config: ProtocolHandlerConfig;\n  private logger: PipelineDebugLogger;\n  private heartbeatTimers: Map<string, NodeJS.Timeout> = new Map();\n  // Track a short window of recent tool_call signatures per request to drop short-range duplicates across chunks\n  private lastToolCallKeys: Map<string, { set: Set<string>; order: string[] }> = new Map();\n  // Per-request SSE event logs\n  private sseLogWriters: Map<string, fs.WriteStream> = new Map();\n\n  constructor(config: ProtocolHandlerConfig) {\n    this.config = config;\n    this.logger = new PipelineDebugLogger(null, {\n      enableConsoleLogging: config.enableMetrics ?? true,\n      enableDebugCenter: false,\n    });\n  }\n\n  /**\n   * Stream response to client\n   */\n  async streamResponse(response: any, requestId: string, res: Response, model: string): Promise<void> {\n    try {\n      // Set appropriate headers for streaming\n      res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n      res.setHeader('Cache-Control', 'no-cache, no-transform');\n      res.setHeader('Connection', 'keep-alive');\n      // Disable proxy buffering for Nginx/Cloudflare style proxies to avoid stuck streams\n      try { res.setHeader('X-Accel-Buffering', 'no'); } catch (error) {\n        // 快速死亡原则 - 暴露SSE响应头设置错误\n        throw new EnhancedRouteCodexError(error as Error, {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber(error as Error),\n          additional: {\n            operation: 'sse-buffering-header',\n            requestId,\n            model\n          }\n        });\n      }\n      res.setHeader('x-request-id', requestId);\n      // Flush headers to start the SSE stream immediately\n      try { (res as any).flushHeaders?.(); } catch (error) {\n        // 快速死亡原则 - 暴露SSE响应头刷新错误\n        throw new EnhancedRouteCodexError(error as Error, {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber(error as Error),\n          additional: {\n            operation: 'sse-header-flush',\n            requestId,\n            model\n          }\n        });\n      }\n\n      // Start streaming\n      if (!this.shouldStreamFromPipeline()) {\n        throw new Error('Streaming pipeline is disabled for this endpoint');\n      }\n\n      // Start SSE heartbeats (pre-heartbeat + periodic)\n      this.startHeartbeat(res, requestId, model);\n\n      // Open SSE event log file for Chat\n      try {\n        const base = path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat');\n        fs.mkdirSync(base, { recursive: true });\n        const file = path.join(base, `${requestId}_sse-events.log`);\n        const ws = fs.createWriteStream(file, { flags: 'a' });\n        this.sseLogWriters.set(requestId, ws);\n        this.writeSSELog(requestId, 'stream.start', { requestId, model });\n      } catch (error) {\n        // 快速死亡原则 - 暴露SSE日志初始化错误\n        throw new EnhancedRouteCodexError(error as Error, {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber(error as Error),\n          additional: {\n            operation: 'sse-log-initialization',\n            requestId,\n            model,\n            logDirectory: path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat')\n          }\n        });\n      }\n\n      await this.streamFromPipeline(response, requestId, res, model);\n\n    } catch (error) {\n      this.logger.logModule('StreamingManager', 'stream_error', {\n        requestId,\n        error: error instanceof Error ? error.message : String(error),\n        model,\n      });\n\n      // Send error chunk and close\n      this.sendErrorChunk(res, error, requestId);\n      this.stopHeartbeat(requestId);\n      res.end();\n      try {\n        this.writeSSELog(requestId, 'stream.error', { message: (error as any)?.message || String(error) });\n      } catch (logError) {\n        // 快速死亡原则 - 暴露SSE错误日志写入失败\n        throw new EnhancedRouteCodexError(logError instanceof Error ? logError : new Error(String(logError)), {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber((logError instanceof Error ? logError : new Error(String(logError))) as Error),\n          additional: {\n            operation: 'sse-error-log-write',\n            requestId,\n            originalError: (error as any)?.message || String(error)\n          }\n        });\n      }\n      try {\n        this.closeSSELog(requestId);\n      } catch (logCloseError) {\n        // 快速死亡原则 - 暴露SSE日志关闭失败\n        throw new EnhancedRouteCodexError(logCloseError instanceof Error ? logCloseError : new Error(String(logCloseError)), {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber((logCloseError instanceof Error ? logCloseError : new Error(String(logCloseError))) as Error),\n          additional: {\n            operation: 'sse-log-close',\n            requestId\n          }\n        });\n      }\n    }\n  }\n\n  /**\n   * Stream Anthropic-compatible responses (delegates to generic streaming)\n   */\n  async streamAnthropicResponse(response: any, requestId: string, res: Response, model: string): Promise<void> {\n    await this.streamResponse(response, requestId, res, model);\n  }\n\n  /**\n   * Check if should stream from pipeline\n   */\n  private shouldStreamFromPipeline(): boolean {\n    return this.config.enablePipeline ?? false;\n  }\n\n  /**\n   * Stream from pipeline\n   */\n  private async streamFromPipeline(\n    response: any,\n    requestId: string,\n    res: Response,\n    model: string\n  ): Promise<void> {\n    // Always synthesize streaming from non-stream JSON\n    if (!response || typeof response !== 'object' || !('data' in response)) {\n      throw new Error('Streaming pipeline response is missing data payload');\n    }\n    const data = (response as Record<string, unknown>).data as any;\n    await this.processStreamingData(data, requestId, res, model);\n  }\n\n  /**\n   * Process streaming data\n   */\n  private async processStreamingData(\n    data: any,\n    requestId: string,\n    res: Response,\n    model: string\n  ): Promise<void> {\n    // Helper: unwrap nested { data: {...} } wrappers until we reach an object with\n    // OpenAI Chat response shape (choices/message) or delta chunks.\n    const unwrapData = (obj: any): any => {\n      try {\n        let cur = obj;\n        const seen = new Set<any>();\n        while (cur && typeof cur === 'object' && !Array.isArray(cur) && !seen.has(cur)) {\n          seen.add(cur);\n          // Stop when the payload clearly looks like an OpenAI Chat response or a delta chunk\n          if (Array.isArray((cur as any).choices) || (cur as any).object === 'chat.completion.chunk') {break;}\n          if ('data' in cur && (cur as any).data && typeof (cur as any).data === 'object') {\n            cur = (cur as any).data; continue;\n          }\n          break;\n        }\n        return cur;\n      } catch { return obj; }\n    };\n    const finishReasons: string[] = [];\n    let sawToolCalls = false;\n    const emitChatRequiredAction = (_msg: any) => { /* removed Responses semantics from Chat SSE */ };\n    // Removed user-visible tool hint delta to avoid confusing client UIs.\n    const genCallId = (index: number) => `call_${requestId}_${index}_${Math.random().toString(36).slice(2, 8)}`;\n\n    // Helper: synthesize OpenAI-style streaming chunks from a non-stream JSON response\n    const synthesizeFromResponse = async (resp: any) => {\n      const chunks: any[] = [];\n      try {\n        const msg = resp?.choices?.[0]?.message || {};\n        // 文本→工具归一化已在 llmswitch-core 的 openai-openai codec 处理，这里不重复\n        const role = typeof msg?.role === 'string' ? msg.role : 'assistant';\n        const content = typeof msg?.content === 'string' ? msg.content : '';\n        const toolCallsFromArray = Array.isArray(msg?.tool_calls) ? msg.tool_calls : [];\n        const fnCall = (msg as any)?.function_call || null;\n        const toolCalls = (() => {\n          if (toolCallsFromArray.length) {return toolCallsFromArray;}\n          if (fnCall && typeof fnCall === 'object') {\n            const name = typeof fnCall.name === 'string' ? fnCall.name : undefined;\n            const args = typeof fnCall.arguments === 'string' ? fnCall.arguments : (fnCall.arguments != null ? JSON.stringify(fnCall.arguments) : undefined);\n            if (name || args) {\n              return [{ id: undefined, type: 'function', function: { ...(name?{name}:{ }), ...(args?{arguments: args}:{ }) } }];\n            }\n          }\n          return [] as any[];\n        })();\n\n        // Emit initial role delta to satisfy some clients' expectations\n        chunks.push({ role });\n\n        // Emit content when it不是工具JSON/补丁块回显\n        const isLikelyToolJson = (s: string): boolean => /\"version\"\\s*:\\s*\"rcc\\.tool\\.v1\"/i.test(s) || /rcc\\.tool\\.v1/i.test(s);\n        const isPatchBlock = (s: string): boolean => /\\*\\*\\*\\s*Begin\\s*Patch/i.test(s);\n        if (content.length > 0 && !isLikelyToolJson(content) && !isPatchBlock(content)) {\n          chunks.push({ content });\n        }\n\n        // Emit tool_calls as delta blocks: name then arguments (single shot)\n        if (toolCalls.length > 0) {\n          sawToolCalls = true;\n          for (let i = 0; i < toolCalls.length; i++) {\n            const tc = toolCalls[i] || {};\n            const fn = tc.function || {};\n            const id = typeof tc.id === 'string' && tc.id.trim() ? tc.id : genCallId(i);\n            const name = typeof fn.name === 'string' ? fn.name : undefined;\n            const args = typeof fn.arguments === 'string' ? fn.arguments : (fn.arguments != null ? JSON.stringify(fn.arguments) : undefined);\n            if (name) {\n              chunks.push({ tool_calls: [{ index: i, id, type: 'function', function: { name } }] });\n            }\n            if (typeof args === 'string' && args.length > 0) {\n              for (const d of chunkString(args)) {\n                chunks.push({ tool_calls: [{ index: i, id, type: 'function', function: { arguments: d } }] });\n              }\n            }\n          }\n        }\n\n        let finish = toolCalls.length > 0\n          ? 'tool_calls'\n          : (resp?.choices?.[0]?.finish_reason || resp?.finish_reason || 'stop');\n        // Guard: if upstream finish_reason=='tool_calls' but we didn't extract any,\n        // do not emit misleading tool_calls final\n        if (finish === 'tool_calls' && toolCalls.length === 0) {finish = 'stop';}\n        return { chunks, finish };\n      } catch {\n        return { chunks, finish: 'stop' };\n      }\n    };\n\n    const pushChunk = async (raw: any) => {\n      const normalized = this.normalizeChunk(raw, model, finishReasons);\n      await this.sendChunk(res, normalized, requestId, model);\n      await this.delay(10);\n    };\n\n    if (Array.isArray(data)) {\n      // If array of chunks, stream as-is. Ensure the first delta carries role=assistant\n      try {\n        const first = data[0];\n        if (first) {\n          const preview = this.normalizeChunk(first, model, []);\n          const hasRole = !!(preview?.choices && preview.choices[0]?.delta && typeof preview.choices[0].delta.role === 'string');\n          if (!hasRole) {\n            await this.sendChunk(res, { role: 'assistant' }, requestId, model);\n            await this.delay(10);\n          }\n        }\n      } catch { /* ignore */ }\n\n      for (const chunk of data) {\n        await pushChunk(chunk);\n      }\n      let finalReason = finishReasons.length ? finishReasons[finishReasons.length - 1] : undefined;\n      if (!finalReason && sawToolCalls) { finalReason = 'tool_calls'; }\n      // Removed hint delta to avoid confusing client UIs\n      this.sendFinalChunk(res, requestId, model, finalReason);\n      return;\n    }\n\n    if (typeof data === 'object' && data !== null) {\n      // Align with validated logic: unwrap nested data so we can reliably synthesize\n      const unwrapped = unwrapData(data);\n      // Non-stream JSON response: synthesize delta stream when choices[].message exists\n      if (Array.isArray((unwrapped as any).choices) && (unwrapped as any).choices.length > 0 && (unwrapped as any).choices[0]?.message) {\n        // Align with core: canonicalize response tools before synthesizing to ensure\n        // tool_calls are present when upstream varies shape\n        let source = unwrapped;\n        try {\n          const core = await import('rcc-llmswitch-core/api');\n          const canon = (core as any).processChatResponseTools?.(unwrapped);\n          if (canon && typeof canon === 'object') {source = canon;}\n        } catch { /* non-blocking */ }\n        const { chunks, finish } = await synthesizeFromResponse(source);\n        for (const c of chunks) {\n          await this.sendChunk(res, c, requestId, model);\n          await this.delay(10);\n        }\n        try { emitChatRequiredAction((data as any).choices[0]?.message); } catch { /* ignore */ }\n        this.sendFinalChunk(res, requestId, model, finish);\n        return;\n      }\n      // Fallback: treat as single chunk\n      await pushChunk(unwrapped);\n      const finalReason = finishReasons.length ? finishReasons[finishReasons.length - 1] : undefined;\n      this.sendFinalChunk(res, requestId, model, finalReason);\n      return;\n    }\n  }\n\n  /**\n   * Normalize chunk into OpenAI streaming shape\n   */\n  private normalizeChunk(chunk: any, model: string, finishReasons: string[]): any {\n    if (!chunk || typeof chunk !== 'object') {\n      return chunk;\n    }\n\n    const hasDelta =\n      chunk.object === 'chat.completion.chunk' ||\n      (Array.isArray(chunk.choices) && chunk.choices.some((choice: any) => choice?.delta));\n\n    if (hasDelta) {\n      // Map delta.function_call -> delta.tool_calls for OpenAI older shape\n      try {\n        const choices = Array.isArray(chunk.choices) ? chunk.choices : [];\n        const isImagePath = (p: any): boolean => {\n          try { const s = String(p || '').toLowerCase(); return /\\.(png|jpg|jpeg|gif|webp|bmp|svg|tiff?|ico|heic|jxl)$/.test(s); } catch { return false; }\n        };\n        const parseArgs = (args: any): any => {\n          if (typeof args === 'string') { try { return JSON.parse(args); } catch { return {}; } }\n          if (args && typeof args === 'object') {return args;}\n          return {};\n        };\n        for (const c of choices) {\n          const d = c?.delta || {};\n          const fc = d?.function_call;\n          if (fc && typeof fc === 'object') {\n            const name = typeof fc.name === 'string' ? fc.name : undefined;\n            const args = typeof fc.arguments === 'string' ? fc.arguments : (fc.arguments != null ? JSON.stringify(fc.arguments) : undefined);\n            const tc: any = { index: 0, type: 'function', function: {} as any };\n            if (name) { (tc.function as any).name = name; }\n            if (typeof args === 'string') { (tc.function as any).arguments = args; }\n            if (!Array.isArray(d.tool_calls)) { d.tool_calls = []; }\n            (d.tool_calls as any[]).push(tc);\n          }\n          // Filter invalid view_image tool_calls where path is not an image\n          if (Array.isArray(d.tool_calls)) {\n            const kept: any[] = [];\n            for (const tc of d.tool_calls as any[]) {\n              try {\n                const fn = tc?.function || {};\n                const nm = typeof fn?.name === 'string' ? fn.name : undefined;\n                if (nm === 'view_image') {\n                  const a = parseArgs(fn?.arguments);\n                  const p = (a && typeof a === 'object') ? (a as any).path : undefined;\n                  if (!isImagePath(p)) {\n                    // Replace with a brief hint in content; drop this tool_call\n                    const hint = typeof p === 'string' && p ? `提示：${p} 不是图片，请改用 shell: {\"command\":[\"cat\",\"${p}\"]}` : '提示：路径不是图片，请改用 shell: {\"command\":[\"cat\",\"<path>\"]}';\n                    if (typeof d.content === 'string' && d.content.length > 0) {\n                      d.content += `\\n${hint}`;\n                    } else {\n                      d.content = hint;\n                    }\n                    continue;\n                  }\n                }\n                kept.push(tc);\n              } catch { kept.push(tc); }\n            }\n            d.tool_calls = kept;\n          }\n        }\n      } catch { /* ignore */ }\n      this.captureFinishReason(chunk, finishReasons);\n      return chunk;\n    }\n\n    if (!Array.isArray(chunk.choices)) {\n      return chunk;\n    }\n\n    const normalizedChoices = chunk.choices.map((choice: any, index: number) => {\n      const message = choice?.message || {};\n      const toolCallsArr = Array.isArray(message.tool_calls) ? message.tool_calls : undefined;\n      const fnCall = (message as any)?.function_call || null;\n      const isImagePath = (p: any): boolean => {\n        try { const s = String(p || '').toLowerCase(); return /\\.(png|jpg|jpeg|gif|webp|bmp|svg|tiff?|ico|heic|jxl)$/.test(s); } catch { return false; }\n      };\n      const parseArgs = (args: any): any => {\n        if (typeof args === 'string') { try { return JSON.parse(args); } catch { return {}; } }\n        if (args && typeof args === 'object') {return args;}\n        return {};\n      };\n      const toolCalls = (() => {\n        if (toolCallsArr && toolCallsArr.length) {return toolCallsArr;}\n        if (fnCall && typeof fnCall === 'object') {\n          const name = typeof fnCall.name === 'string' ? fnCall.name : undefined;\n          const args = typeof fnCall.arguments === 'string' ? fnCall.arguments : (fnCall.arguments != null ? JSON.stringify(fnCall.arguments) : undefined);\n          if (name || args) {\n            return [{ id: undefined, type: 'function', function: { ...(name?{name}:{ }), ...(args?{arguments: args}:{ }) } }];\n          }\n        }\n        return undefined;\n      })();\n      // Filter invalid view_image tool_calls when arguments contain a non-image path\n      const filteredToolCalls = (() => {\n        if (!Array.isArray(toolCalls)) {return toolCalls;}\n        const kept: any[] = [];\n        for (const tc of toolCalls) {\n          try {\n            const fn = (tc as any)?.function || {};\n            const nm = typeof fn?.name === 'string' ? fn.name : undefined;\n            if (nm === 'view_image') {\n              const a = parseArgs((fn as any).arguments);\n              const p = (a && typeof a === 'object') ? (a as any).path : undefined;\n              if (!isImagePath(p)) {\n                const hint = typeof p === 'string' && p ? `提示：${p} 不是图片，请改用 shell: {\"command\":[\"cat\",\"${p}\"]}` : '提示：路径不是图片，请改用 shell: {\"command\":[\"cat\",\"<path>\"]}';\n                if (typeof message.content === 'string' && message.content.length > 0) {\n                  message.content += `\\n${hint}`;\n                } else {\n                  (message as any).content = hint;\n                }\n                continue;\n              }\n            }\n            kept.push(tc);\n          } catch { kept.push(tc); }\n        }\n        return kept;\n      })();\n      const delta: Record<string, unknown> = {};\n      if (message.role && typeof message.role === 'string') {\n        delta.role = message.role;\n      }\n      if (typeof message.content === 'string') {\n        const s = message.content as string;\n        const looksToolJson = /\"version\"\\s*:\\s*\"rcc\\.tool\\.v1\"/i.test(s) || /rcc\\.tool\\.v1/i.test(s);\n        const looksPatch = /\\*\\*\\*\\s*Begin\\s*Patch/i.test(s);\n        if (!looksToolJson && !looksPatch) {\n          delta.content = s;\n        }\n      }\n      if (filteredToolCalls) {\n        delta.tool_calls = filteredToolCalls;\n      }\n      if (typeof choice.content === 'string' && !delta.content) {\n        delta.content = choice.content;\n      }\n\n      const finishReason = choice?.finish_reason ?? (message?.finish_reason as string | undefined);\n      if (finishReason) {\n        finishReasons.push(finishReason);\n      }\n\n      return {\n        index: choice?.index ?? index,\n        delta,\n        finish_reason: null\n      };\n    });\n\n    return {\n      id: chunk.id ?? `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: chunk.created ?? Math.floor(Date.now() / 1000),\n      model: chunk.model ?? model,\n      choices: normalizedChoices\n    };\n  }\n\n  private captureFinishReason(chunk: any, finishReasons: string[]): void {\n    if (!chunk || typeof chunk !== 'object') {\n      return;\n    }\n    const choices = Array.isArray(chunk.choices) ? chunk.choices : [];\n    for (const choice of choices) {\n      const reason = choice?.finish_reason;\n      if (typeof reason === 'string' && reason.length > 0) {\n        finishReasons.push(reason);\n      }\n    }\n  }\n\n  /**\n   * Send chunk to response\n   */\n  private async sendChunk(\n    res: Response,\n    chunk: StreamingChunk | any,\n    requestId: string,\n    model: string\n  ): Promise<void> {\n    // Clean reasoning/thinking tags in content fields (delta.content and message.content)\n    const cleanse = (obj: any) => {\n      try {\n        if (!obj || typeof obj !== 'object') {return obj;}\n        const choices = Array.isArray(obj.choices) ? obj.choices : [];\n        for (const c of choices) {\n          if (c && typeof c === 'object') {\n            const d = c.delta || {};\n            if (typeof d.content === 'string') { d.content = stripThinkingTags(d.content); }\n            if (c.message && typeof c.message === 'object' && typeof c.message.content === 'string') {\n              c.message.content = stripThinkingTags(c.message.content);\n            }\n          }\n        }\n      } catch { /* ignore */ }\n      return obj;\n    };\n\n    // If passing a light delta object (e.g., { role }, { content }, { tool_calls }), clean its content\n    if (chunk && typeof chunk === 'object' && !('id' in chunk)) {\n      if (typeof (chunk as any).content === 'string') {\n        (chunk as any).content = stripThinkingTags((chunk as any).content);\n      }\n    }\n\n    const chunkData = typeof chunk === 'object' && chunk.id ? cleanse(chunk) : {\n      id: `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model,\n      choices: [{\n        index: 0,\n        delta: chunk,\n        finish_reason: null\n      }]\n    };\n\n    // Short-window duplicate tool_call dedupe (per request, across chunks)\n    try {\n      const choices = Array.isArray((chunkData as any).choices) ? (chunkData as any).choices : [];\n      for (const c of choices) {\n        const d = c?.delta || {};\n        if (Array.isArray(d.tool_calls) && d.tool_calls.length > 0) {\n          const kept: any[] = [];\n          const WINDOW = Math.max(1, Math.min(16, Number(process.env.ROUTECODEX_TOOL_CALL_DEDUPE_WINDOW || 4)));\n          const bucket = this.lastToolCallKeys.get(requestId) || { set: new Set<string>(), order: [] };\n          for (const tc of d.tool_calls as any[]) {\n            try {\n              const fn = tc?.function || {};\n              const name = typeof fn?.name === 'string' ? fn.name : '';\n              const args = fn?.arguments;\n              const argsStr = typeof args === 'string' ? args : (args != null ? JSON.stringify(args) : '');\n              const key = `${name}\\n${argsStr}`;\n              const hasBoth = name && argsStr;\n              if (hasBoth && bucket.set.has(key)) { continue; }\n              kept.push(tc);\n              if (hasBoth) {\n                bucket.set.add(key);\n                bucket.order.push(key);\n                if (bucket.order.length > WINDOW) {\n                  const old = bucket.order.shift();\n                  if (old) {bucket.set.delete(old);}\n                }\n              }\n            } catch { kept.push(tc); }\n          }\n          d.tool_calls = kept;\n          this.lastToolCallKeys.set(requestId, bucket);\n        }\n      }\n    } catch { /* ignore dedupe errors */ }\n\n    const sseData = `data: ${JSON.stringify(chunkData)}\\n\\n`;\n    res.write(sseData);\n    try { this.writeSSELog(requestId, 'chunk', chunkData); } catch { /* ignore */ }\n\n    try {\n      const LOG = String(process.env.ROUTECODEX_LOG_STREAM_CHUNKS || process.env.RCC_LOG_STREAM_CHUNKS || '0') === '1';\n      if (LOG) {\n        this.logger.logModule('StreamingManager', 'chunk_sent', {\n          requestId,\n          chunkId: chunkData.id,\n          model,\n        });\n      }\n    } catch { /* no-op */ }\n  }\n\n  /**\n   * Send final chunk\n   */\n  private sendFinalChunk(res: Response, requestId: string, model: string, finishReason?: string): void {\n    const finalChunk = {\n      id: `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model,\n      choices: [{\n        index: 0,\n        delta: {},\n        finish_reason: finishReason ?? 'stop'\n      }]\n    };\n\n    // 可选：发出 Responses 风格的 done 事件（默认关闭，以避免跨协议混用）\n    try {\n      const EMIT_RESP_DONE = String(process.env.ROUTECODEX_CHAT_RESP_DONE || '0') === '1';\n      if (EMIT_RESP_DONE) {\n        const evt = { type: 'response.done' } as Record<string, unknown>;\n        res.write(`event: response.done\\n`);\n        res.write(`data: ${JSON.stringify(evt)}\\n\\n`);\n      }\n    } catch { /* ignore */ }\n    const finalData = `data: ${JSON.stringify(finalChunk)}\\n\\ndata: [DONE]\\n\\n`;\n    res.write(finalData);\n    this.stopHeartbeat(requestId);\n    res.end();\n    // Reset short-window cache for this request\n    try { this.lastToolCallKeys.delete(requestId); } catch { /* ignore */ }\n    try {\n      this.writeSSELog(requestId, 'chunk.final', finalChunk);\n      this.writeSSELog(requestId, 'done', { requestId });\n      this.closeSSELog(requestId);\n    } catch { /* ignore */ }\n\n    try {\n      const LOG = String(process.env.ROUTECODEX_LOG_STREAM_CHUNKS || process.env.RCC_LOG_STREAM_CHUNKS || '0') === '1';\n      if (LOG) {\n        this.logger.logModule('StreamingManager', 'stream_complete', {\n          requestId,\n          model,\n        });\n      }\n    } catch { /* no-op */ }\n  }\n\n  /**\n   * Send error chunk\n   */\n  private sendErrorChunk(res: Response, error: any, requestId: string): void {\n    const errorChunk = {\n      id: `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model: 'unknown',\n      choices: [{\n        index: 0,\n        delta: {\n          content: `Error: ${error instanceof Error ? error.message : String(error)}`\n        },\n        finish_reason: 'error'\n      }]\n    };\n\n    try {\n      const evt = { type: 'response.done' } as Record<string, unknown>;\n      res.write(`event: response.done\\n`);\n      res.write(`data: ${JSON.stringify(evt)}\\n\\n`);\n    } catch { /* ignore */ }\n    const errorData = `data: ${JSON.stringify(errorChunk)}\\n\\ndata: [DONE]\\n\\n`;\n    res.write(errorData);\n    this.stopHeartbeat(requestId);\n    try {\n      this.writeSSELog(requestId, 'error', errorChunk);\n      this.writeSSELog(requestId, 'done', { requestId });\n      this.closeSSELog(requestId);\n    } catch { /* ignore */ }\n  }\n\n  // Write one SSE log record line\n  private writeSSELog(requestId: string, event: string, data: any): void {\n    try {\n      const ws = this.sseLogWriters.get(requestId);\n      if (!ws) {return;}\n      ws.write(`${JSON.stringify({ ts: Date.now(), requestId, event, data })  }\\n`);\n    } catch { /* ignore */ }\n  }\n\n  private closeSSELog(requestId: string): void {\n    try {\n      const ws = this.sseLogWriters.get(requestId);\n      if (ws) {\n        try { ws.end(); } catch { /* ignore */ }\n      }\n    } catch { /* ignore */ }\n    finally { this.sseLogWriters.delete(requestId); }\n  }\n\n  /**\n   * Delay utility\n   */\n  private delay(ms: number): Promise<void> {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n\n  // Heartbeat helpers\n  private startHeartbeat(res: Response, requestId: string, model: string): void {\n    try {\n      const iv = Math.max(1000, Number(process.env.ROUTECODEX_STREAM_HEARTBEAT_MS || process.env.RCC_STREAM_HEARTBEAT_MS || 15000));\n      const pre = String(process.env.ROUTECODEX_STREAM_PRE_HEARTBEAT || process.env.RCC_STREAM_PRE_HEARTBEAT || '1') === '1';\n      const writeBeat = () => {\n        try {\n          const payload = { type: 'heartbeat', ts: Date.now(), model };\n          res.write(`data: ${JSON.stringify(payload)}\\n\\n`);\n        } catch { /* ignore */ }\n      };\n      if (pre) {writeBeat();}\n      const timer = setInterval(writeBeat, iv);\n      this.heartbeatTimers.set(requestId, timer);\n      try { res.on('close', () => this.stopHeartbeat(requestId)); } catch { /* ignore */ }\n    } catch { /* ignore */ }\n  }\n\n  private stopHeartbeat(requestId: string): void {\n    try {\n      const t = this.heartbeatTimers.get(requestId);\n      if (t) { clearInterval(t); this.heartbeatTimers.delete(requestId); }\n    } catch { /* ignore */ }\n  }\n\n  /**\n   * Check if response is streamable\n   */\n  isStreamable(response: any): boolean {\n    return (\n      response?.stream === true ||\n      (typeof response?.pipe === 'function') ||\n      (Array.isArray(response?.data)) ||\n      (this.config.enableStreaming === true)\n    );\n  }\n\n  /**\n   * Get streaming statistics\n   */\n  getStreamingStats(): {\n    enabled: boolean;\n    config: ProtocolHandlerConfig;\n  } {\n    return {\n      enabled: this.config.enableStreaming ?? false,\n      config: this.config,\n    };\n  }\n}\n"
        },
        "src/server/utils/text-filters.ts": {
          "path": "src/server/utils/text-filters.ts",
          "size": 446,
          "lines": 15,
          "imports": [],
          "exports": [
            "stripThinkingTags"
          ],
          "classes": [],
          "functions": [
            "stripThinkingTags"
          ],
          "content": "export function stripThinkingTags(text: string): string {\n  try {\n    if (typeof text !== 'string' || text.length === 0) {return String(text ?? '');}\n    let out = String(text);\n    // Remove paired <think ...> ... </think>\n    out = out.replace(/<think\\b[^>]*>[\\s\\S]*?<\\/think>/gi, '');\n    // Remove stray single tags <think ...> or </think>\n    out = out.replace(/<\\/?think\\b[^>]*>/gi, '');\n    return out;\n  } catch {\n    return text;\n  }\n}\n\n"
        },
        "src/server/utils/tool-executor.ts": {
          "path": "src/server/utils/tool-executor.ts",
          "size": 9618,
          "lines": 236,
          "imports": [
            "node:child_process",
            "node:util",
            "node:path",
            "node:os",
            "node:fs"
          ],
          "exports": [
            "ToolCallSpec",
            "ToolResult"
          ],
          "classes": [],
          "functions": [
            "executeTool",
            "looksJsonArray",
            "test",
            "quoteToken",
            "quoteScript",
            "firstMetaIndex"
          ],
          "content": "import { exec as execCb } from 'node:child_process';\nimport { promisify } from 'node:util';\nimport path from 'node:path';\nimport os from 'node:os';\nimport fs from 'node:fs';\n\nconst exec = promisify(execCb);\n\nexport interface ToolCallSpec {\n  id: string;\n  name: string;\n  args: unknown;\n}\n\nexport interface ToolResult {\n  id: string;\n  name: string;\n  output: string;\n  error?: string;\n}\n\n/**\n * Tool executor (trusts client-defined commands).\n * - Executes { name: 'shell', args: { command: string } } without internal\n *   whitelists or path restrictions.\n * - SECURITY: This mode trusts the caller. To re-enable safeguards in certain\n *   environments, set ROUTECODEX_TOOL_SAFE_MODE=1 (not default).\n */\nexport async function executeTool(spec: ToolCallSpec): Promise<ToolResult> {\n  const name = String(spec.name || '').toLowerCase();\n  if (name !== 'shell') {\n    return { id: spec.id, name, output: '', error: `unsupported tool: ${name}` };\n  }\n\n  let command = '';\n  try {\n    const args = typeof spec.args === 'string' ? JSON.parse(spec.args) : (spec.args as Record<string, unknown>);\n    const rawCmd = (args as any)?.command;\n\n    const isArray = (v: any): v is any[] => Array.isArray(v);\n    const isString = (v: any): v is string => typeof v === 'string';\n    const looksJsonArray = (s: string) => {\n      const t = s.trim();\n      return t.startsWith('[') && t.endsWith(']');\n    };\n    const tryParseJsonArray = (s: string): any[] | null => {\n      try { const v = JSON.parse(s); return Array.isArray(v) ? v : null; } catch { return null; }\n    };\n    const hasMeta = (tokensOrScript: string[] | string): boolean => {\n      const metas = ['>>','<<','|',';','&&','||','>','<'];\n      const test = (s: string) => metas.some(m => s.includes(m));\n      if (typeof tokensOrScript === 'string') {return test(tokensOrScript);}\n      return tokensOrScript.some(t => test(String(t)));\n    };\n    const quoteToken = (s: string) => {\n      if (s === '') {return \"''\";}\n      if (/[^A-Za-z0-9_\\.\\-\\/:]/.test(s)) {return `'${  s.replace(/'/g, \"'\\\\''\")  }'`;}\n      return s;\n    };\n    const quoteScript = (s: string) => `'${  String(s).replace(/'/g, \"'\\\\''\")  }'`;\n\n    const normalizeBashLc = (script: string | string[]): string => {\n      const sc = Array.isArray(script) ? script.map(String).join(' ') : String(script);\n      return ['bash','-lc', quoteScript(sc)].map(quoteToken).join(' ');\n    };\n\n    const normalizeFromArray = (arr: any[]): string => {\n      const tokens = arr.map((x) => String(x));\n      if (tokens.length >= 2 && tokens[0] === 'bash' && tokens[1] === '-lc') {\n        if (tokens.length === 3) {\n          const t2 = tokens[2];\n          // If the script itself is a JSON-array string, unwrap\n          if (looksJsonArray(t2)) {\n            const inner = tryParseJsonArray(t2);\n            if (inner && inner.length) {\n              if (inner[0] === 'bash' && inner[1] === '-lc') {\n                if (inner.length >= 3) {return normalizeBashLc(inner.slice(2).map(String).join(' '));}\n                return normalizeBashLc('');\n              }\n              // Treat as argv → if contains meta, convert to -lc script\n              return hasMeta(inner.map(String))\n                ? normalizeBashLc(inner.map(String).join(' '))\n                : inner.map((t: any) => quoteToken(String(t))).join(' ');\n            }\n          }\n          // Good shape: bash -lc 'script'\n          return ['bash','-lc', quoteScript(t2)].map(quoteToken).join(' ');\n        }\n        // Merge tail tokens into one script\n        return normalizeBashLc(tokens.slice(2));\n      }\n      // Not bash -lc\n      if (tokens.length === 1 && looksJsonArray(tokens[0])) {\n        const inner = tryParseJsonArray(tokens[0]);\n        if (inner) {return normalizeFromArray(inner);}\n      }\n      return hasMeta(tokens)\n        ? normalizeBashLc(tokens)\n        : tokens.map(quoteToken).join(' ');\n    };\n\n    if (isArray(rawCmd)) {\n      command = normalizeFromArray(rawCmd);\n  } else if (isString(rawCmd)) {\n      const s = String(rawCmd).trim();\n      if (!s) {\n        command = '';\n      } else if (looksJsonArray(s)) {\n        const inner = tryParseJsonArray(s);\n        command = inner ? normalizeFromArray(inner) : (hasMeta(s) ? normalizeBashLc(s) : s);\n      } else {\n        // Handle patterns like: [\"find\", \".\", \"-name\", \"README.md\"] | grep \"codex-\"\n        const firstMetaIndex = (() => {\n          const metas = ['|', '&&', '||', ';', '<<', '>>', '>', '<'];\n          let idx = -1;\n          for (const m of metas) {\n            const i = s.indexOf(m);\n            if (i >= 0) {idx = (idx === -1) ? i : Math.min(idx, i);}\n          }\n          return idx;\n        })();\n        if (s.startsWith('[')) {\n          const bound = firstMetaIndex >= 0 ? firstMetaIndex : s.length;\n          const arrPart = s.slice(0, bound).trim();\n          const rest = firstMetaIndex >= 0 ? s.slice(bound).trim() : '';\n          let parsed: any[] | null = null;\n          try { if (arrPart.endsWith(']')) { const v = JSON.parse(arrPart); if (Array.isArray(v)) {parsed = v;} } } catch { parsed = null; }\n          if (parsed && parsed.length) {\n            const script = parsed.map(String).join(' ') + (rest ? (` ${  rest}`) : '');\n            command = normalizeBashLc(script);\n          } else {\n            command = hasMeta(s) ? normalizeBashLc(s) : s;\n          }\n        } else {\n          command = hasMeta(s) ? normalizeBashLc(s) : s;\n        }\n      }\n    } else {\n      command = '';\n    }\n\n    // Optional debug\n    try {\n      const DBG = String(process.env.ROUTECODEX_TOOL_EXEC_DEBUG || process.env.RCC_TOOL_EXEC_DEBUG || '0') === '1';\n      if (DBG) {\n        // eslint-disable-next-line no-console\n        console.log('[TOOL-EXEC][normalize]', { raw: rawCmd, final: command });\n      }\n    } catch { /* ignore */ }\n  } catch {\n    return { id: spec.id, name, output: '', error: 'invalid arguments' };\n  }\n\n  if (!command) {\n    return { id: spec.id, name, output: '', error: 'empty command' };\n  }\n\n  // Optional safe mode (disabled by default). When enabled, block control operators.\n  const SAFE_MODE = String(process.env.ROUTECODEX_TOOL_SAFE_MODE || '').trim() === '1';\n  if (SAFE_MODE && /[;&|]{1,2}/.test(command)) {\n    return { id: spec.id, name, output: '', error: 'blocked by safe mode: control operators not allowed' };\n  }\n\n  try {\n    const unifiedTimeout = Number(process.env.ROUTECODEX_TIMEOUT_MS || process.env.RCC_TIMEOUT_MS || 300000);\n    const { stdout, stderr } = await exec(command, { timeout: unifiedTimeout, maxBuffer: 1024 * 1024 });\n    const out = stdout?.toString()?.trim() || '';\n    const err = stderr?.toString()?.trim() || '';\n    let merged = [out, err].filter(Boolean).join('\\n');\n\n    // If command succeeded but produced no output, synthesize a short success summary\n    if (!merged) {\n      const summarizeWrite = (cmd: string): string | null => {\n        try {\n          // Extract bash -lc 'script'\n          let script = '';\n          const m = cmd.match(/\\bbash\\b\\s+-lc\\s+'([\\s\\S]*)'$/);\n          if (m && m[1]) {\n            script = m[1];\n          }\n          // Detect heredoc write: cat > path << 'EOF'\n          const detectWriteTarget = (s: string): string | null => {\n            const m1 = s.match(/cat\\s*>\\s*([^\\s]+)\\s*<<\\s*['\\\"]?EOF['\\\"]?/i);\n            if (m1 && m1[1]) {return m1[1];}\n            const m2 = s.match(/[>]{1,2}\\s*([^\\s;&|]+)/);\n            if (m2 && m2[1]) {return m2[1];}\n            return null;\n          };\n          const detectSedEdit = (s: string): string | null => {\n            if (!/\\bsed\\b[^\\n]*\\-i\\b/i.test(s)) {return null;}\n            const parts = s.split(/\\s+/).filter(Boolean);\n            for (let i = parts.length - 1; i >= 0; i--) {\n              const t = parts[i];\n              if (!t.startsWith('-')) {return t;}\n            }\n            return null;\n          };\n          const p = script ? (detectWriteTarget(script) || detectSedEdit(script)) : null;\n          if (p) {\n            let filePath = p;\n            // Strip surrounding quotes if any\n            if ((filePath.startsWith(\"'\") && filePath.endsWith(\"'\")) || (filePath.startsWith('\"') && filePath.endsWith('\"'))) {\n              filePath = filePath.slice(1, -1);\n            }\n            // Resolve relative path for stat\n            const resolved = path.resolve(process.cwd(), filePath);\n            try {\n              const stat = fs.statSync(resolved);\n              let lines: number | null = null;\n              if (stat.isFile() && stat.size <= 2_000_000) {\n                try { const buf = fs.readFileSync(resolved, 'utf8'); lines = (buf.match(/\\n/g) || []).length + (buf.endsWith('\\n') ? 0 : 1); } catch { lines = null; }\n              }\n              const sizePart = `大小: ${stat.size} bytes`;\n              const linePart = lines != null ? `，行数: ${lines}` : '';\n              const verb = /sed\\b/i.test(script) ? '修改成功' : '写入成功';\n              return `${verb}: ${filePath}（${sizePart}${linePart}）`;\n            } catch {\n              // File may be temporary or moved; still return generic success\n              const verb = /sed\\b/i.test(script) ? '修改成功' : '写入成功';\n              return `${verb}: ${filePath}`;\n            }\n          }\n        } catch { /* ignore */ }\n        return null;\n      };\n      const summary = summarizeWrite(command);\n      merged = summary || 'Command succeeded (no output).';\n    }\n\n    return { id: spec.id, name, output: merged };\n  } catch (e: any) {\n    const msg = e?.message || String(e);\n    return { id: spec.id, name, output: '', error: msg };\n  }\n}\n"
        }
      },
      "keyFiles": {
        "src/server/RouteCodexServer.ts": {
          "path": "src/server/RouteCodexServer.ts",
          "size": 21616,
          "lines": 769,
          "imports": [
            "express",
            "cors",
            "helmet",
            "rcc-basemodule",
            "rcc-errorhandling",
            "rcc-debugcenter",
            "path",
            "fs",
            "os",
            "../types/common-types.js"
          ],
          "exports": [
            "ServerConfig",
            "RouteCodexServer"
          ],
          "classes": [
            "RouteCodexServer"
          ],
          "functions": [],
          "content": "/**\n * RouteCodex Server - Main server implementation\n * Multi-provider OpenAI proxy server\n */\n\nimport express, { type Application, type Request, type Response, type NextFunction } from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { BaseModule, type ModuleInfo } from 'rcc-basemodule';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport path from 'path';\nimport fs from 'fs';\nimport { homedir } from 'os';\nimport type { UnknownObject } from '../types/common-types.js';\n\ninterface ErrorContext {\n  error: Error | string;\n  source: string;\n  severity: 'low' | 'medium' | 'high' | 'critical';\n  timestamp: number;\n  moduleId?: string;\n  context?: UnknownObject;\n}\n\n/**\n * Server configuration interface\n */\nexport interface ServerConfig {\n  server: {\n    port: number;\n    host: string;\n    cors?: {\n      origin: string | string[];\n      credentials?: boolean;\n    };\n    timeout?: number;\n    bodyLimit?: string;\n  };\n  logging: {\n    level: 'debug' | 'info' | 'warn' | 'error';\n    enableConsole?: boolean;\n    enableFile?: boolean;\n    filePath?: string;\n    categories?: string[];\n    categoryPath?: string;\n  };\n  providers: Record<string, UnknownObject>;\n}\n\n/**\n * RouteCodex Server class\n */\nexport class RouteCodexServer extends BaseModule {\n  private app: Application;\n  private server?: unknown;\n  private config: ServerConfig;\n  private errorHandling: ErrorHandlingCenter;\n  private debugEventBus: DebugEventBus;\n  private logFileStream?: fs.WriteStream;\n  private categoryLogStreams: Map<string, fs.WriteStream> = new Map();\n  private _isInitialized: boolean = false;\n  private _isRunning: boolean = false;\n\n  // Debug enhancement properties\n  private serverMetrics: Map<string, UnknownObject> = new Map();\n  private requestHistory: UnknownObject[] = [];\n  private errorHistory: UnknownObject[] = [];\n  private maxHistorySize = 50;\n\n  constructor(config: ServerConfig) {\n    const moduleInfo: ModuleInfo = {\n      id: 'routecodex-server',\n      name: 'RouteCodexServer',\n      version: '0.50.1',\n      description: 'Multi-provider OpenAI proxy server',\n      type: 'server',\n    };\n\n    super(moduleInfo);\n\n    this.config = config;\n    this.app = express();\n    this.errorHandling = new ErrorHandlingCenter();\n    this.debugEventBus = DebugEventBus.getInstance();\n\n    // Initialize debug enhancements\n    this.initializeDebugEnhancements();\n  }\n\n  /**\n   * Initialize the server\n   */\n  public async initialize(): Promise<void> {\n    try {\n      // Setup logging directory\n      this.setupLoggingDirectory();\n\n      // Load module configurations\n      this.loadModuleConfigurations();\n\n      // Initialize error handling\n      await this.errorHandling.initialize();\n\n      // Setup debug event listener for console logging\n      this.setupDebugLogging();\n\n      // Setup Express middleware\n      this.setupMiddleware();\n\n      // Setup routes\n      this.setupRoutes();\n\n      // Setup error handling\n      this.setupErrorHandling();\n\n      this._isInitialized = true;\n\n      // Log initialization\n      this.logEvent('server', 'initialized', {\n        port: this.config.server.port,\n        host: this.config.server.host,\n      });\n    } catch (error) {\n      await this.handleError(error as Error, 'initialization');\n      throw error;\n    }\n  }\n\n  /**\n   * Start the server\n   */\n  public async start(): Promise<void> {\n    if (!this._isInitialized) {\n      await this.initialize();\n    }\n\n    return new Promise((resolve, reject) => {\n      this.server = this.app.listen(this.config.server.port, this.config.server.host, () => {\n        this._isRunning = true;\n        this.logEvent('server', 'started', {\n          port: this.config.server.port,\n          host: this.config.server.host,\n        });\n        resolve();\n      });\n\n      (this.server as any).on('error', async (error: Error) => {\n        await this.handleError(error, 'server_start');\n        reject(error);\n      });\n    });\n  }\n\n  /**\n   * Stop the server\n   */\n  public async stop(): Promise<void> {\n    if (this.server) {\n      return new Promise(resolve => {\n        (this.server as any)?.close(async () => {\n          this._isRunning = false;\n          this.logEvent('server', 'stopped', {});\n\n          // Cleanup modules\n          await this.errorHandling.destroy();\n\n          // Close log file stream\n          if (this.logFileStream) {\n            this.logFileStream.end();\n            console.log('Log file stream closed');\n          }\n\n          // Close category log file streams\n          this.categoryLogStreams.forEach((stream, category) => {\n            stream.end();\n            console.log(`Category log file stream closed for: ${category}`);\n          });\n          this.categoryLogStreams.clear();\n\n          resolve();\n        });\n      });\n    }\n  }\n\n  /**\n   * Get server status\n   */\n  public getStatus() {\n    return {\n      initialized: this._isInitialized,\n      running: this._isRunning,\n      port: this.config.server.port,\n      host: this.config.server.host,\n      uptime: process.uptime(),\n      memory: process.memoryUsage(),\n    };\n  }\n\n  /**\n   * Override BaseModule methods\n   */\n  public isInitialized(): boolean {\n    return this._isInitialized;\n  }\n\n  public isRunning(): boolean {\n    return this._isRunning;\n  }\n\n  /**\n   * Setup logging directory and file stream\n   */\n  private setupLoggingDirectory(): void {\n    if (this.config.logging.enableFile && this.config.logging.filePath) {\n      // Resolve ~ to home directory\n      let logPath = this.config.logging.filePath;\n      if (logPath.startsWith('~')) {\n        logPath = path.join(homedir(), logPath.substring(1));\n      }\n\n      // Create main log directory if it doesn't exist\n      const logDir = path.dirname(logPath);\n      if (!fs.existsSync(logDir)) {\n        fs.mkdirSync(logDir, { recursive: true });\n        console.log(`Created logging directory: ${logDir}`);\n      }\n\n      // Create category subdirectories\n      const categories = this.config.logging.categories || [\n        'server',\n        'api',\n        'request',\n        'config',\n        'error',\n        'message',\n      ];\n      const categoryPath = this.config.logging.categoryPath || logDir;\n\n      categories.forEach(category => {\n        const categoryDir = path.join(categoryPath, category);\n        if (!fs.existsSync(categoryDir)) {\n          fs.mkdirSync(categoryDir, { recursive: true });\n          console.log(`Created category logging directory: ${categoryDir}`);\n        }\n\n        // Create category-specific log file stream\n        const categoryLogPath = path.join(categoryDir, `${category}.log`);\n        const categoryStream = fs.createWriteStream(categoryLogPath, { flags: 'a' });\n        categoryStream.on('error', error => {\n          console.error(`Category log file stream error for ${category}:`, error);\n        });\n        this.categoryLogStreams.set(category, categoryStream);\n      });\n\n      // Create main log file write stream\n      this.logFileStream = fs.createWriteStream(logPath, { flags: 'a' });\n      this.logFileStream.on('error', error => {\n        console.error('Main log file stream error:', error);\n      });\n\n      console.log(`Logging configured for file: ${logPath}`);\n      console.log(`Category logging enabled for: ${categories.join(', ')}`);\n      console.log(`Category log directory: ${categoryPath}`);\n    }\n  }\n\n  /**\n   * Load module configurations\n   */\n  private loadModuleConfigurations(): void {\n    // Use import.meta.url to get the current file's directory\n    const currentDir = path.dirname(new URL(import.meta.url).pathname);\n    const moduleConfigPath = path.join(currentDir, '..', '..', 'config', 'modules.json');\n\n    if (fs.existsSync(moduleConfigPath)) {\n      try {\n        const moduleConfig = JSON.parse(fs.readFileSync(moduleConfigPath, 'utf8'));\n        console.log('Module configurations loaded successfully');\n\n        // Log each module's configuration status\n        if (moduleConfig.modules) {\n          Object.entries(moduleConfig.modules).forEach(([moduleName, config]: [string, unknown]) => {\n            const configObj = config as UnknownObject;\n            console.log(`[CONFIG] ${moduleName}: ${configObj.enabled ? 'enabled' : 'disabled'}`);\n            if (configObj.enabled && configObj.config) {\n              console.log(`[CONFIG] ${moduleName} settings:`, Object.keys(configObj.config));\n            }\n          });\n        }\n\n        this.logEvent('config', 'modules_loaded', {\n          modules: Object.keys(moduleConfig.modules || {}),\n          configPath: moduleConfigPath,\n        });\n      } catch (error) {\n        console.error('Failed to load module configurations:', error);\n        this.logEvent('config', 'modules_load_failed', {\n          error: error instanceof Error ? error.message : String(error),\n          configPath: moduleConfigPath,\n        });\n      }\n    } else {\n      console.log('Module configuration file not found, using defaults');\n      this.logEvent('config', 'modules_not_found', {\n        configPath: moduleConfigPath,\n      });\n    }\n  }\n\n  /**\n   * Write to category-specific log file\n   */\n  private writeToCategoryLog(category: string, message: string): void {\n    const categoryStream = this.categoryLogStreams.get(category);\n    if (categoryStream) {\n      categoryStream.write(message);\n    }\n  }\n\n  /**\n   * Setup debug logging\n   */\n  private setupDebugLogging(): void {\n    // Subscribe to all debug events\n    this.debugEventBus.subscribe('*', (event: UnknownObject) => {\n      if (this.config.logging.level === 'debug') {\n        const timestamp = new Date(event.timestamp as any).toISOString();\n        const eventData = event.data as Record<string, unknown>;\n        const category = (eventData?.category as string) || 'general';\n        const action = (eventData?.action as string) || 'unknown';\n\n        const logMessage = `[${timestamp}] [DEBUG] ${event.operationId}: ${JSON.stringify(\n          {\n            category,\n            action,\n            moduleId: event.moduleId,\n            data: event.data,\n          },\n          null,\n          2\n        )}\\n`;\n\n        // Log to console\n        console.log(`[DEBUG] ${event.operationId}:`, {\n          category,\n          action,\n          timestamp,\n          moduleId: event.moduleId,\n          data: event.data,\n        });\n\n        // Log to main file\n        if (this.logFileStream) {\n          this.logFileStream.write(logMessage);\n        }\n\n        // Log to category-specific file\n        this.writeToCategoryLog(category, logMessage);\n      }\n    });\n\n    // Log debug center setup\n    console.log('Debug logging enabled for event:', '*');\n  }\n\n  /**\n   * Setup Express middleware\n   */\n  private setupMiddleware(): void {\n    // Security middleware\n    this.app.use(helmet());\n\n    // CORS middleware\n    if (this.config.server.cors) {\n      this.app.use(cors(this.config.server.cors));\n    } else {\n      this.app.use(\n        cors({\n          origin: '*',\n          credentials: true,\n        })\n      );\n    }\n\n    // Body parsing middleware\n    this.app.use(express.json({ limit: this.config.server.bodyLimit || '10mb' }));\n    this.app.use(\n      express.urlencoded({ extended: true, limit: this.config.server.bodyLimit || '10mb' })\n    );\n\n    // Request logging middleware\n    this.app.use((req: Request, res: Response, next: NextFunction) => {\n      const start = Date.now();\n\n      res.on('finish', () => {\n        const duration = Date.now() - start;\n        this.logEvent('request', 'completed', {\n          method: req.method,\n          url: req.url,\n          status: res.statusCode,\n          duration: duration,\n          userAgent: req.get('user-agent'),\n          ip: req.ip,\n        });\n      });\n\n      next();\n    });\n  }\n\n  /**\n   * Setup routes\n   */\n  private setupRoutes(): void {\n    // Health check endpoint\n    this.app.get('/health', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        status: status.running ? 'healthy' : 'unhealthy',\n        timestamp: new Date().toISOString(),\n        uptime: status.uptime,\n        memory: status.memory,\n        version: this.getModuleInfo().version,\n      });\n    });\n\n    // OpenAI-compatible endpoints\n    this.app.post('/v1/chat/completions', this.handleChatCompletions.bind(this));\n    this.app.post('/v1/completions', this.handleCompletions.bind(this));\n    this.app.get('/v1/models', this.handleModels.bind(this));\n\n    // Status endpoint\n    this.app.get('/status', (req: Request, res: Response) => {\n      res.json(this.getStatus());\n    });\n\n    // Test error endpoint\n    this.app.get('/test-error', async (req: Request, res: Response) => {\n      try {\n        // Simulate an error that should be handled by error handling center\n        const testError = new Error('This is a test error for error handling validation');\n        await this.handleError(testError, 'test_error_endpoint');\n\n        res.json({\n          message: 'Test error processed successfully',\n          error: testError.message,\n        });\n      } catch (error) {\n        res.status(500).json({\n          error: {\n            message: 'Failed to process test error',\n            type: 'test_error_failed',\n          },\n        });\n      }\n    });\n\n    // 404 handler\n    this.app.use('*', (req: Request, res: Response) => {\n      res.status(404).json({\n        error: {\n          message: 'Not Found',\n          type: 'not_found_error',\n          code: 'not_found',\n        },\n      });\n    });\n  }\n\n  /**\n   * Setup error handling\n   */\n  private setupErrorHandling(): void {\n    this.app.use((error: UnknownObject, req: Request, res: Response, _next: NextFunction) => {\n      this.handleError(error as unknown as Error, 'request_handler').catch(() => {\n        // Ignore handler errors, just send response\n      });\n\n      const errorStatus = (error as any).status || 500;\n      res.status(errorStatus).json({\n        error: {\n          message: (error as any).message || 'Internal Server Error',\n          type: (error as any).type || 'internal_error',\n          code: error.code || 'internal_error',\n        },\n      });\n    });\n  }\n\n  /**\n   * Handle chat completions\n   */\n  private async handleChatCompletions(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api', 'chat_completions_request', {\n        model: req.body.model,\n        messages: req.body.messages?.length || 0,\n      });\n\n      // Provider routing not yet implemented - return proper error\n      res.status(501).json({\n        error: {\n          message: 'Provider routing not yet implemented',\n          type: 'not_implemented',\n          code: 'provider_routing_not_implemented',\n        },\n      });\n      return;\n    } catch (error) {\n      await this.handleError(error as Error, 'chat_completions_handler');\n      throw error;\n    }\n  }\n\n  /**\n   * Handle completions\n   */\n  private async handleCompletions(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api', 'completions_request', {\n        model: req.body.model,\n        prompt: req.body.prompt?.length || 0,\n      });\n\n      // Provider routing not yet implemented - return proper error\n      res.status(501).json({\n        error: {\n          message: 'Provider routing not yet implemented',\n          type: 'not_implemented',\n          code: 'provider_routing_not_implemented',\n        },\n      });\n      return;\n    } catch (error) {\n      await this.handleError(error as Error, 'completions_handler');\n      throw error;\n    }\n  }\n\n  /**\n   * Handle models list\n   */\n  private async handleModels(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api', 'models_request', {});\n\n      // Return available models from configured providers\n      const models = [];\n\n      for (const [providerId, providerConfig] of Object.entries(this.config.providers)) {\n        if (providerConfig.enabled && providerConfig.models) {\n          for (const [modelId, _modelConfig] of Object.entries(providerConfig.models)) { // eslint-disable-line @typescript-eslint/no-unused-vars\n            models.push({\n              id: modelId,\n              object: 'model',\n              created: Math.floor(Date.now() / 1000),\n              owned_by: providerId,\n            });\n          }\n        }\n      }\n\n      res.json({\n        object: 'list',\n        data: models,\n      });\n    } catch (error) {\n      await this.handleError(error as Error, 'models_handler');\n      throw error;\n    }\n  }\n\n  /**\n   * Log event to debug center\n   */\n  private logEvent(category: string, action: string, data: UnknownObject): void {\n    try {\n      // Also log to category-specific file directly\n      if (this.config.logging.enableFile) {\n        const timestamp = new Date().toISOString();\n        const logMessage = `[${timestamp}] [EVENT] ${category}_${action}: ${JSON.stringify(\n          {\n            category,\n            action,\n            ...data,\n          },\n          null,\n          2\n        )}\\n`;\n\n        this.writeToCategoryLog(category, logMessage);\n      }\n\n      this.debugEventBus.publish({\n        sessionId: `session_${Date.now()}`,\n        moduleId: this.getModuleInfo().id,\n        operationId: `${category}_${action}`,\n        timestamp: Date.now(),\n        type: 'start',\n        position: 'middle',\n        data: {\n          category,\n          action,\n          ...data,\n        },\n      });\n    } catch (error) {\n      // Don't let logging errors break the server\n      console.error('Failed to log event:', error);\n    }\n  }\n\n  /**\n   * Handle error with error handling center\n   */\n  private async handleError(error: Error, context: string): Promise<void> {\n    try {\n      const errorContext: ErrorContext = {\n        error: error.message,\n        source: `${this.getModuleInfo().id}.${context}`,\n        severity: 'medium',\n        timestamp: Date.now(),\n        moduleId: this.getModuleInfo().id,\n        context: {\n          stack: error.stack,\n          name: error.name,\n        },\n      };\n\n      await this.errorHandling.handleError(errorContext);\n    } catch (handlerError) {\n      console.error('Failed to handle error:', handlerError);\n      console.error('Original error:', error);\n    }\n  }\n\n  /**\n   * Get module info\n   */\n  public getModuleInfo(): ModuleInfo {\n    return {\n      id: 'routecodex-server',\n      name: 'RouteCodexServer',\n      version: '0.0.1',\n      description: 'Multi-provider OpenAI proxy server',\n      type: 'server',\n    };\n  }\n\n  /**\n   * Initialize debug enhancements\n   */\n  private initializeDebugEnhancements(): void {\n    try {\n      console.log('RouteCodexServer debug enhancements initialized');\n    } catch (error) {\n      console.warn('Failed to initialize RouteCodexServer debug enhancements:', error);\n    }\n  }\n\n  /**\n   * Record server metric\n   */\n  private recordServerMetric(operation: string, data: UnknownObject): void {\n    if (!this.serverMetrics.has(operation)) {\n      this.serverMetrics.set(operation, {\n        values: [],\n        lastUpdated: Date.now(),\n      });\n    }\n\n    const metric = this.serverMetrics.get(operation)! as any;\n    metric.values.push(data);\n    metric.lastUpdated = Date.now();\n\n    // Keep only last 50 measurements\n    if (metric.values.length > 50) {\n      metric.values.shift();\n    }\n  }\n\n  /**\n   * Add to request history\n   */\n  private addToRequestHistory(operation: UnknownObject): void {\n    this.requestHistory.push(operation);\n\n    // Keep only recent history\n    if (this.requestHistory.length > this.maxHistorySize) {\n      this.requestHistory.shift();\n    }\n  }\n\n  /**\n   * Add to error history\n   */\n  private addToErrorHistory(operation: UnknownObject): void {\n    this.errorHistory.push(operation);\n\n    // Keep only recent history\n    if (this.errorHistory.length > this.maxHistorySize) {\n      this.errorHistory.shift();\n    }\n  }\n\n  /**\n   * Get debug status with enhanced information\n   */\n  getDebugStatus(): UnknownObject {\n    const baseStatus = {\n      serverId: this.getModuleInfo().id,\n      name: this.getModuleInfo().name,\n      version: this.getModuleInfo().version,\n      isInitialized: this._isInitialized,\n      isRunning: this._isRunning,\n      config: this.config.server,\n      isEnhanced: true,\n    };\n\n    return {\n      ...baseStatus,\n      debugInfo: this.getDebugInfo(),\n      serverMetrics: this.getServerMetrics(),\n      requestHistory: [...this.requestHistory.slice(-10)],\n      errorHistory: [...this.errorHistory.slice(-10)],\n    };\n  }\n\n  /**\n   * Get detailed debug information\n   */\n  private getDebugInfo(): UnknownObject {\n    return {\n      serverId: this.getModuleInfo().id,\n      name: this.getModuleInfo().name,\n      version: this.getModuleInfo().version,\n      enhanced: true,\n      uptime: this._isRunning ? process.uptime() : 0,\n      memory: process.memoryUsage(),\n      requestHistorySize: this.requestHistory.length,\n      errorHistorySize: this.errorHistory.length,\n      serverMetricsSize: this.serverMetrics.size,\n      maxHistorySize: this.maxHistorySize,\n      categoryLogStreams: this.categoryLogStreams.size,\n      hasLogFile: !!this.logFileStream,\n    };\n  }\n\n  /**\n   * Get server metrics\n   */\n  private getServerMetrics(): UnknownObject {\n    const metrics: UnknownObject = {};\n\n    for (const [operation, metric] of this.serverMetrics.entries()) {\n      const metricObj = metric as any;\n      metrics[operation] = {\n        count: metricObj.values.length,\n        lastUpdated: metricObj.lastUpdated,\n        recentValues: metricObj.values.slice(-5),\n      };\n    }\n\n    return metrics;\n  }\n}\n"
        },
        "src/server/handlers/chat-completions.ts": {
          "path": "src/server/handlers/chat-completions.ts",
          "size": 17359,
          "lines": 399,
          "imports": [
            "express",
            "./base-handler.js",
            "../types.js",
            "../types.js",
            "../utils/request-validator.js",
            "../utils/response-normalizer.js",
            "../utils/streaming-manager.js",
            "../protocol/protocol-detector.js",
            "../protocol/openai-adapter.js",
            "../../modules/pipeline/utils/debug-logger.js",
            "../utils/error-context.js",
            "os",
            "path"
          ],
          "exports": [
            "ChatCompletionsHandler"
          ],
          "classes": [
            "ChatCompletionsHandler"
          ],
          "functions": [
            "currentSys",
            "snapshotDir",
            "writeSnapshot",
            "ctSummary"
          ],
          "content": "/**\n * Chat Completions Handler Implementation\n * Handles OpenAI-compatible chat completion requests\n */\n\nimport { type Request, type Response } from 'express';\nimport { BaseHandler, type ProtocolHandlerConfig } from './base-handler.js';\nimport { type RequestContext, type OpenAIChatCompletionRequest } from '../types.js';\nimport { RouteCodexError } from '../types.js';\nimport { RequestValidator } from '../utils/request-validator.js';\nimport { ResponseNormalizer } from '../utils/response-normalizer.js';\nimport { StreamingManager } from '../utils/streaming-manager.js';\nimport { ProtocolDetector } from '../protocol/protocol-detector.js';\nimport { OpenAIAdapter } from '../protocol/openai-adapter.js';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\nimport { ErrorContextBuilder, EnhancedRouteCodexError, type ErrorContext } from '../utils/error-context.js';\nimport os from 'os';\nimport path from 'path';\n\n/**\n * Chat Completions Handler\n * Handles /v1/chat/completions endpoint\n */\nexport class ChatCompletionsHandler extends BaseHandler {\n  private requestValidator: RequestValidator;\n  private responseNormalizer: ResponseNormalizer;\n  private streamingManager: StreamingManager;\n\n  constructor(config: ProtocolHandlerConfig) {\n    super(config);\n    this.requestValidator = new RequestValidator();\n    this.responseNormalizer = new ResponseNormalizer();\n    this.streamingManager = new StreamingManager(config);\n  }\n\n  /**\n   * Handle chat completions request\n  */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = this.generateRequestId();\n\n    // 记录开始时间用于错误上下文\n    (req as any).__startTime = startTime;\n    // Capture raw request for debugging\n    try {\n      const rawBody = JSON.parse(JSON.stringify((req as any).body || {}));\n      (req as any).__rawBody = rawBody;\n      const fs = await import('fs/promises');\n      const dir = path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat');\n      await (fs as any).mkdir(dir, { recursive: true });\n      const rawFile = path.join(dir, `${requestId}_raw-request.json`);\n      const payload = {\n        requestId,\n        method: req.method,\n        url: req.originalUrl || req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        body: rawBody,\n      };\n      await (fs as any).writeFile(rawFile, JSON.stringify(payload, null, 2), 'utf-8');\n    } catch (error) {\n      // 快速死亡原则 - 暴露捕获失败的原因\n      throw new EnhancedRouteCodexError(error as Error, {\n        module: 'ChatCompletionsHandler',\n        file: 'src/server/handlers/chat-completions.ts',\n        function: 'handleRequest',\n        line: 58,\n        requestId,\n        additional: {\n          operation: 'raw-request-capture',\n          payload: ErrorContextBuilder.safeStringify({\n            requestId,\n            method: req.method,\n            url: req.originalUrl || req.url\n          }, 500)\n        }\n      });\n    }\n\n    this.logModule('ChatCompletionsHandler', 'request_start', {\n      requestId,\n      model: req.body.model,\n      messageCount: req.body.messages?.length || 0,\n      streaming: req.body.stream || false,\n      tools: !!req.body.tools,\n      timestamp: startTime,\n    });\n\n    try {\n      // 可选：受控的“静态”系统提示注入（默认关闭）。\n      // 禁止样本抓取式注入；仅当明确配置 ROUTECODEX_ENABLE_SERVER_SYSTEM_PROMPT=1 且提供\n      // ROUTECODEX_SERVER_SYSTEM_PROMPT=</path/to/prompt.txt> 时，才会替换/插入一条 system。\n      try {\n        const enable = String(process.env.ROUTECODEX_ENABLE_SERVER_SYSTEM_PROMPT || '0') === '1';\n        const filePath = String(process.env.ROUTECODEX_SERVER_SYSTEM_PROMPT || '').trim();\n        if (enable && filePath && Array.isArray((req.body as any)?.messages)) {\n          const fs = await import('fs/promises');\n          const sysText = await fs.readFile(filePath, 'utf-8');\n          const messages = (req.body as any).messages as any[];\n          const currentSys = (() => {\n            try {\n              const m = messages[0];\n              return (m && typeof m === 'object' && m.role === 'system' && typeof m.content === 'string') ? String(m.content) : '';\n            } catch { return ''; }\n          })();\n          const hasMdMarkers = /\\bCLAUDE\\.md\\b|\\bAGENT(?:S)?\\.md\\b/i.test(currentSys);\n          const replaceSystemInOpenAIMessagesSimple = (msgs: any[], sys: string): any[] => {\n            if (!Array.isArray(msgs)) {return msgs;}\n            const out = msgs.slice();\n            if (out.length > 0 && out[0] && out[0].role === 'system') {\n              out[0] = { ...out[0], role: 'system', content: sys };\n            } else {\n              out.unshift({ role: 'system', content: sys });\n            }\n            return out;\n          };\n          if (!hasMdMarkers) {\n            (req.body as any).messages = replaceSystemInOpenAIMessagesSimple(messages, sysText) as any[];\n            try { res.setHeader('x-rc-system-prompt-source', 'static-file'); } catch { /* ignore */ }\n          }\n        }\n      } catch (error) {\n        // 注入失败不影响主流程\n        this.logger.logModule('ChatCompletionsHandler', 'system-prompt-static-skip', {\n          requestId,\n          reason: (error as Error)?.message || String(error)\n        });\n      }\n\n      // Strict protocol separation: Chat endpoint only accepts OpenAI Chat payload.\n      // Do NOT auto-convert Anthropic/Responses-shaped payloads here to avoid cross-path pollution.\n      try {\n        const detector = new ProtocolDetector();\n        const det = detector.detectFromRequest(req);\n        const looksAnthropicContent = Array.isArray(req.body?.messages) && (req.body.messages as any[]).some((m: any) => Array.isArray(m?.content) && m.content.some((c: any) => c && typeof c === 'object' && typeof c.type === 'string'));\n        if (det.protocol === 'anthropic' || det.protocol === 'responses' || looksAnthropicContent) {\n          throw new RouteCodexError('Chat endpoint only accepts OpenAI Chat payload (messages: string or OpenAI content parts). Use the Responses endpoint for Responses-shaped payloads.', 'invalid_protocol', 400);\n        }\n      } catch (e) {\n        if (e instanceof RouteCodexError) {throw e;}\n      }\n\n      // Tool guidance和归一化在 llmswitch-core 的 OpenAI 工具阶段统一处理\n      try { res.setHeader('x-rc-conversion-profile', 'openai-openai'); } catch { /* ignore */ }\n\n      // Skip strict request validation (preserve raw inputs)\n\n      // Process request through pipeline\n      const pipelineResponse = await this.processChatRequest(req, requestId);\n\n      // Handle streaming vs non-streaming response (also honor Accept: text/event-stream)\n      const accept = String(req.headers['accept'] || '').toLowerCase();\n      const wantsSSE = (accept.includes('text/event-stream') || req.body?.stream === true);\n      if (wantsSSE) {\n        const streamModel = (pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse)\n          ? ((pipelineResponse as any).data?.model ?? req.body.model)\n          : req.body.model;\n        await this.streamingManager.streamResponse(pipelineResponse, requestId, res, streamModel);\n        return;\n      }\n\n      // Return JSON response\n      const payload = pipelineResponse && typeof pipelineResponse === 'object' && 'data' in pipelineResponse\n        ? (pipelineResponse as Record<string, unknown>).data\n        : pipelineResponse;\n      // 工具文本→tool_calls 的归一化已在 llmswitch-core (openai-openai codec) 统一完成，这里不再做重复处理\n      const normalized = this.responseNormalizer.normalizeOpenAIResponse(payload, 'chat');\n      // Chat 路径不注入 Responses 的 required_action 结构，保持协议纯净\n      this.sendJsonResponse(res, normalized, requestId);\n\n      this.logCompletion(requestId, startTime, true);\n    } catch (error) {\n      this.logCompletion(requestId, startTime, false);\n      // If the client requested SSE streaming, emit an SSE error and end the stream\n      try {\n        const accept = String(req.headers['accept'] || '').toLowerCase();\n        const wantsSSE = (accept.includes('text/event-stream') || req.body?.stream === true);\n        if (wantsSSE && !res.writableEnded) {\n          try {\n            if (!res.headersSent) {\n              res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n              res.setHeader('Cache-Control', 'no-cache, no-transform');\n              res.setHeader('Connection', 'keep-alive');\n              try { res.setHeader('X-Accel-Buffering', 'no'); } catch { /* ignore */ }\n              res.setHeader('x-request-id', requestId);\n              try { (res as any).flushHeaders?.(); } catch { /* ignore */ }\n            }\n            const model = typeof req.body?.model === 'string' ? req.body.model : 'unknown';\n            const errMsg = (error instanceof Error ? error.message : String(error)) || 'stream error';\n            const errChunk = {\n              id: `chatcmpl-${Date.now()}`,\n              object: 'chat.completion.chunk',\n              created: Math.floor(Date.now() / 1000),\n              model,\n              choices: [{ index: 0, delta: { content: `Error: ${errMsg}` }, finish_reason: 'error' }]\n            } as any;\n            res.write(`data: ${JSON.stringify(errChunk)}\\n\\n`);\n            res.write(`data: [DONE]\\n\\n`);\n          } catch { /* ignore */ }\n          try { res.end(); } catch { /* ignore */ }\n          return;\n        }\n      } catch { /* ignore */ }\n\n      await this.handleEnhancedError(error as Error, res, requestId, req);\n    }\n  }\n\n  /**\n   * 增强的错误处理函数 - 符合9大架构原则\n   * 实现快速死亡和暴露问题原则\n   */\n  private async handleEnhancedError(error: Error, res: Response, requestId: string, req: Request): Promise<void> {\n    // 构建详细的错误上下文\n    const context: ErrorContext = {\n      module: 'ChatCompletionsHandler',\n      file: 'src/server/handlers/chat-completions.ts',\n      function: 'handleRequest',\n      line: ErrorContextBuilder.extractLineNumber(error),\n      requestId,\n      additional: {\n        endpoint: '/v1/chat/completions',\n        method: req.method,\n        url: req.url,\n        model: req.body?.model,\n        messageCount: req.body?.messages?.length,\n        hasTools: !!req.body?.tools,\n        streaming: req.body?.stream,\n        processingTime: Date.now() - (req as any).__startTime || Date.now(),\n        userAgent: req.headers['user-agent'],\n        contentType: req.headers['content-type']\n      }\n    };\n\n    // 如果错误已经是EnhancedRouteCodexError，直接使用\n    if (error instanceof EnhancedRouteCodexError) {\n      this.sendDetailedErrorResponse(res, error.detailedError);\n      return;\n    }\n\n    // 否则创建增强错误\n    const enhancedError = new EnhancedRouteCodexError(error, context);\n    this.sendDetailedErrorResponse(res, enhancedError.detailedError);\n  }\n\n  /**\n   * 发送详细的错误响应\n   * 符合暴露问题原则，提供完整调试信息\n   */\n  private sendDetailedErrorResponse(res: Response, detailedError: any): void {\n    try {\n      // 设置状态码和响应头\n      res.status(500).set({\n        'Content-Type': 'application/json',\n        'x-request-id': detailedError.error.context.requestId,\n        'x-error-source': detailedError.source,\n        'x-error-code': detailedError.error.code\n      });\n\n      // 发送详细的错误响应\n      res.json(detailedError);\n    } catch (responseError) {\n      // 如果连错误响应都失败了，返回最基本的错误信息\n      res.status(500).json({\n        error: {\n          code: 'CRITICAL_ERROR_RESPONSE_FAILURE',\n          message: '系统发生严重错误，无法提供详细错误信息',\n          type: 'CriticalError'\n        },\n        source: 'RouteCodex-Critical',\n        remediation: {\n          immediate: '检查服务器日志和系统状态',\n          support: '联系技术支持团队'\n        }\n      });\n    }\n  }\n\n  /**\n   * Process chat completion request\n   */\n  private async processChatRequest(req: Request, requestId: string): Promise<any> {\n    // Use pipeline manager only; no fallback allowed\n    if (this.shouldUsePipeline() && this.getRoutePools()) {\n      return await this.processWithPipeline(req, requestId);\n    }\n    throw new RouteCodexError('Chat pipeline unavailable (no route pools or pipeline manager)', 'pipeline_unavailable', 503);\n  }\n\n  /**\n   * Process request through pipeline\n   */\n  private async processWithPipeline(req: Request, requestId: string): Promise<any> {\n    const routeName = await this.decideRouteCategoryAsync(req);\n    const pipelineId = this.pickPipelineId(routeName);\n    const routeMeta = this.getRouteMeta();\n    const meta = routeMeta ? routeMeta[pipelineId] : undefined;\n    const providerId = meta?.providerId ?? 'unknown';\n    const modelId = meta?.modelId ?? 'unknown';\n\n    // Pre-convert OpenAI Responses payload to OpenAI Chat payload\n    let chatPayload = { ...(req.body || {}) } as Record<string, unknown>;\n    // 不在入口做跨路径的内容过滤；路径隔离在转换路由与编码器中保证。\n    // 角色分布快照（llmswitch 前/后），便于诊断“tool 历史泄露”等问题\n    const snapshotDir = (() => {\n      try {\n        const os = require('os'); const path = require('path'); const fs = require('fs');\n        const dir = path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat');\n        fs.mkdirSync(dir, { recursive: true });\n        return dir;\n      } catch { return null; }\n    })();\n    const roleCounts = (msgs: any[]): Record<string, number> => {\n      const c: Record<string, number> = { system: 0, user: 0, assistant: 0, tool: 0, unknown: 0 };\n      for (const m of Array.isArray(msgs) ? msgs : []) {\n        const r = String(m?.role || '').toLowerCase();\n        if (r === 'system' || r === 'user' || r === 'assistant' || r === 'tool') {c[r] += 1;} else {c.unknown += 1;}\n      }\n      return c;\n    };\n    const writeSnapshot = (kind: 'pre-llmswitch' | 'post-llmswitch', payload: any) => {\n      try {\n        if (!snapshotDir) {return;}\n        const fs = require('fs'); const path = require('path');\n        const file = path.join(snapshotDir, `${requestId}_${kind}.json`);\n        const msgs = Array.isArray(payload?.messages) ? payload.messages : [];\n        const counts = roleCounts(msgs);\n        const tc = msgs.filter((m: any) => m && m.role === 'assistant' && Array.isArray(m.tool_calls) && m.tool_calls.length);\n        const ctSummary = (() => {\n          const sum: Record<string, number> = {};\n          for (const m of tc) {\n            const t = (m.content === null) ? 'null' : (typeof m.content);\n            sum[t] = (sum[t] || 0) + 1;\n          }\n          return sum;\n        })();\n        const stats = { kind, counts, total: msgs.length, assistant_tool_calls: tc.length, content_types: ctSummary };\n        fs.writeFileSync(file, JSON.stringify({ requestId, stats }, null, 2), 'utf-8');\n      } catch { /* ignore */ }\n    };\n    try { writeSnapshot('pre-llmswitch', chatPayload); } catch { /* ignore */ }\n    // 不在入口执行 legacy OpenAI normalizer；统一由 pipeline conversion-router + canonicalizer 处理\n    chatPayload = { ...(req.body || {}) } as Record<string, unknown>;\n    try { writeSnapshot('post-llmswitch', chatPayload); } catch { /* ignore */ }\n\n    // Ensure payload model aligns with selected route meta\n    const normalizedPayload = { ...chatPayload, ...(modelId ? { model: modelId } : {}) } as Record<string, unknown>;\n    // Inject route.requestId into payload metadata for downstream providers (capture + pairing)\n    try {\n      const meta = (normalizedPayload as any)._metadata && typeof (normalizedPayload as any)._metadata === 'object'\n        ? { ...(normalizedPayload as any)._metadata }\n        : {};\n      (normalizedPayload as any)._metadata = { ...meta, requestId };\n    } catch { /* non-blocking */ }\n\n    const pipelineRequest = {\n      data: normalizedPayload,\n      route: {\n        providerId,\n        modelId,\n        requestId,\n        timestamp: Date.now(),\n        pipelineId,\n      },\n      metadata: {\n        method: req.method,\n        url: req.url,\n        headers: this.sanitizeHeaders(req.headers),\n        targetProtocol: 'openai',\n        endpoint: `${req.baseUrl || ''}${req.url || ''}`,\n        entryEndpoint: '/v1/chat/completions',\n      },\n      debug: {\n        enabled: this.config.enableMetrics ?? true,\n        stages: {\n          llmSwitch: true,\n          workflow: true,\n          compatibility: true,\n          provider: true,\n        },\n      },\n    };\n\n    const pipelineTimeoutMs = Number(\n      process.env.ROUTECODEX_TIMEOUT_MS ||\n      process.env.RCC_TIMEOUT_MS ||\n      process.env.ROUTECODEX_PIPELINE_MAX_WAIT_MS ||\n      300000\n    );\n    const pipelineResponse = await Promise.race([\n      this.getPipelineManager()?.processRequest?.(pipelineRequest) || Promise.reject(new Error('Pipeline manager not available')),\n      new Promise((_, reject) => setTimeout(() => reject(new Error(`Pipeline timeout after ${pipelineTimeoutMs}ms`)), Math.max(1, pipelineTimeoutMs)))\n    ]);\n\n    return pipelineResponse;\n  }\n\n}\n"
        },
        "src/server/streaming/anthropic-streamer.ts": {
          "path": "src/server/streaming/anthropic-streamer.ts",
          "size": 1773,
          "lines": 54,
          "imports": [
            "express"
          ],
          "exports": [
            "AnthropicStreamer"
          ],
          "classes": [
            "without",
            "AnthropicStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\n\nimport {\n  BaseStreamer,\n  type ProtocolHandlerConfig,\n  type StreamChunk,\n  type StreamingOptions\n} from './base-streamer.js';\n\n/**\n * Minimal Anthropic-compatible streamer that emits SSE payloads using the\n * generic formatting helpers from {@link BaseStreamer}. The implementation is\n * intentionally lightweight; when richer streaming support lands we can extend\n * this class without breaking consumers of the current refactor.\n */\nexport class AnthropicStreamer extends BaseStreamer {\n  constructor(config?: ProtocolHandlerConfig) {\n    super(config);\n  }\n\n  async streamResponse(response: unknown, options: StreamingOptions, res: Response): Promise<void> {\n    try {\n      this.validateOptions(options);\n      if (!this.isStreamingEnabled()) {\n        throw new Error('Streaming is disabled for this handler');\n      }\n\n      this.setupStreamingHeaders(res, options.requestId);\n      this.startStreaming();\n\n      const chunks: StreamChunk[] = Array.isArray(response)\n        ? (response as StreamChunk[])\n        : typeof response === 'object' && response !== null && 'data' in (response as Record<string, unknown>) && Array.isArray((response as any).data)\n          ? ((response as any).data as StreamChunk[])\n          : [];\n\n      for (const chunk of chunks) {\n        const payload = this.formatAnthropicChunk(chunk, options.requestId);\n        const wrote = this.sendSSEData(res, payload);\n        if (!wrote) {\n          break;\n        }\n        this.incrementChunkCount();\n        await this.applyChunkDelay(options.chunkDelay);\n      }\n\n      this.endStreaming(res);\n      this.logStreamingMetrics(options);\n    } catch (error) {\n      this.handleStreamingError(res, error as Error, options.requestId);\n    }\n  }\n}\n"
        },
        "src/server/streaming/base-streamer.ts": {
          "path": "src/server/streaming/base-streamer.ts",
          "size": 7549,
          "lines": 313,
          "imports": [
            "express"
          ],
          "exports": [
            "StreamChunk",
            "StreamingOptions",
            "ProtocolHandlerConfig"
          ],
          "classes": [
            "BaseStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\n\n/**\n * Stream chunk interface\n */\nexport interface StreamChunk {\n  content?: string;\n  done?: boolean;\n  // For OpenAI Chat SSE: include tool_calls delta when present\n  tool_calls?: Array<{\n    index: number;\n    id?: string;\n    type?: 'function';\n    function?: { name?: string; arguments?: string };\n  }>;\n  metadata?: {\n    model?: string;\n    usage?: {\n      prompt_tokens?: number;\n      completion_tokens?: number;\n      total_tokens?: number;\n    };\n    finish_reason?: string;\n  };\n}\n\n/**\n * Streaming options interface\n */\nexport interface StreamingOptions {\n  requestId: string;\n  model: string;\n  enableMetrics?: boolean;\n  chunkDelay?: number;\n  maxTokens?: number;\n}\n\n/**\n * Protocol handler configuration\n */\nexport interface ProtocolHandlerConfig {\n  enableStreaming?: boolean;\n  enableMetrics?: boolean;\n  targetUrl?: string;\n  timeout?: number;\n}\n\n/**\n * Base Streamer Abstract Class\n * Provides common streaming functionality for different protocols\n */\nexport abstract class BaseStreamer {\n  protected config: ProtocolHandlerConfig;\n  protected isStreaming = false;\n  protected startTime = 0;\n  protected chunkCount = 0;\n\n  constructor(config: ProtocolHandlerConfig = {}) {\n    this.config = {\n      enableStreaming: true,\n      enableMetrics: true,\n      timeout: 30000,\n      ...config,\n    };\n  }\n\n  /**\n   * Stream response - must be implemented by subclasses\n   */\n  abstract streamResponse(\n    response: any,\n    options: StreamingOptions,\n    res: Response\n  ): Promise<void>;\n\n  /**\n   * Send SSE (Server-Sent Events) data\n   */\n  protected sendSSEData(res: Response, data: string): boolean {\n    try {\n      if (res.writableEnded) {\n        return false;\n      }\n\n      res.write(`data: ${data}\\n\\n`);\n      return true;\n    } catch (error) {\n      console.error('Failed to send SSE data:', error);\n      return false;\n    }\n  }\n\n  /**\n   * Send SSE event with custom event name\n   */\n  protected sendSSEEvent(res: Response, event: string, data: string): boolean {\n    try {\n      if (res.writableEnded) {\n        return false;\n      }\n\n      res.write(`event: ${event}\\n`);\n      res.write(`data: ${data}\\n\\n`);\n      return true;\n    } catch (error) {\n      console.error('Failed to send SSE event:', error);\n      return false;\n    }\n  }\n\n  /**\n   * Set up streaming response headers\n   */\n  protected setupStreamingHeaders(res: Response, requestId: string): void {\n    // Avoid \"ERR_HTTP_HEADERS_SENT\" if a pre-heartbeat or another layer already set headers\n    if (res.headersSent) {\n      return;\n    }\n    try { res.setHeader('Content-Type', 'text/event-stream'); } catch { /* ignore */ }\n    try { res.setHeader('Cache-Control', 'no-cache'); } catch { /* ignore */ }\n    try { res.setHeader('Connection', 'keep-alive'); } catch { /* ignore */ }\n    try { res.setHeader('Access-Control-Allow-Origin', '*'); } catch { /* ignore */ }\n    try { res.setHeader('Access-Control-Allow-Headers', 'Cache-Control'); } catch { /* ignore */ }\n    try { res.setHeader('x-request-id', requestId); } catch { /* ignore */ }\n  }\n\n  /**\n   * Start streaming session\n   */\n  protected startStreaming(): void {\n    this.isStreaming = true;\n    this.startTime = Date.now();\n    this.chunkCount = 0;\n  }\n\n  /**\n   * End streaming session\n   */\n  protected endStreaming(res: Response): void {\n    this.isStreaming = false;\n\n    if (!res.writableEnded) {\n      try {\n        res.write('data: [DONE]\\n\\n');\n        res.end();\n      } catch (error) {\n        console.error('Error ending stream:', error);\n      }\n    }\n  }\n\n  /**\n   * Log streaming metrics\n   */\n  protected logStreamingMetrics(options: StreamingOptions): void {\n    if (!this.config.enableMetrics) {\n      return;\n    }\n\n    const duration = Date.now() - this.startTime;\n    const metrics = {\n      requestId: options.requestId,\n      model: options.model,\n      duration,\n      chunkCount: this.chunkCount,\n      chunksPerSecond: this.chunkCount > 0 ? (this.chunkCount / (duration / 1000)).toFixed(2) : 0,\n    };\n\n    console.log('Streaming metrics:', metrics);\n  }\n\n  /**\n   * Validate streaming options\n   */\n  protected validateOptions(options: StreamingOptions): void {\n    if (!options.requestId) {\n      throw new Error('Request ID is required for streaming');\n    }\n\n    if (!options.model) {\n      throw new Error('Model is required for streaming');\n    }\n  }\n\n  /**\n   * Handle streaming error\n   */\n  protected handleStreamingError(res: Response, error: Error, requestId: string): void {\n    console.error('Streaming error:', error);\n\n    if (!res.writableEnded) {\n      const errorData = {\n        error: {\n          message: error.message,\n          type: 'streaming_error',\n          code: 'STREAM_FAILED',\n        },\n        requestId,\n      };\n\n      this.sendSSEEvent(res, 'error', JSON.stringify(errorData));\n      res.end();\n    }\n  }\n\n  /**\n   * Check if streaming is enabled\n   */\n  protected isStreamingEnabled(): boolean {\n    return this.config.enableStreaming === true;\n  }\n\n  /**\n   * Get streaming statistics\n   */\n  protected getStreamingStats(): {\n    isStreaming: boolean;\n    duration: number;\n    chunkCount: number;\n  } {\n    return {\n      isStreaming: this.isStreaming,\n      duration: this.startTime > 0 ? Date.now() - this.startTime : 0,\n      chunkCount: this.chunkCount,\n    };\n  }\n\n  /**\n   * Increment chunk counter\n   */\n  protected incrementChunkCount(): void {\n    this.chunkCount++;\n  }\n\n  /**\n   * Apply chunk delay if configured\n   */\n  protected async applyChunkDelay(delayMs?: number): Promise<void> {\n    const ms = typeof delayMs === 'number' ? delayMs : 0;\n    if (ms > 0) {\n      await new Promise(resolve => setTimeout(resolve, ms));\n    }\n  }\n\n  /**\n   * Format OpenAI-style chunk\n   */\n  protected formatOpenAIChunk(chunk: StreamChunk, requestId: string): string {\n    const openAIChunk: any = {\n      id: `chatcmpl-${requestId}-${this.chunkCount}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model: chunk.metadata?.model || 'unknown',\n      choices: [{\n        index: 0,\n        delta: {\n          content: chunk.content ?? '',\n          // Attach tool_calls delta if present\n          ...(Array.isArray(chunk.tool_calls) && chunk.tool_calls.length\n            ? { tool_calls: chunk.tool_calls.map((tc) => ({\n                index: tc.index,\n                id: tc.id,\n                type: tc.type || 'function',\n                function: tc.function || {}\n              })) }\n            : {})\n        },\n        finish_reason: chunk.done ? chunk.metadata?.finish_reason || 'stop' : null,\n      }],\n    };\n\n    if (chunk.done && chunk.metadata?.usage) {\n      openAIChunk.usage = chunk.metadata.usage;\n    }\n\n    return JSON.stringify(openAIChunk);\n  }\n\n  /**\n   * Format Anthropic-style chunk\n   */\n  protected formatAnthropicChunk(chunk: StreamChunk, requestId: string): string {\n    if (chunk.done) {\n      return JSON.stringify({\n        type: 'message_stop',\n      });\n    }\n\n    return JSON.stringify({\n      type: 'content_block_delta',\n      index: 0,\n      delta: {\n        type: 'text_delta',\n        text: chunk.content ?? ''\n      }\n    });\n  }\n\n  /**\n   * Format Response-style chunk (generic)\n   */\n  protected formatResponseChunk(chunk: StreamChunk, requestId: string): string {\n    return JSON.stringify({\n      id: requestId,\n      object: 'response.chunk',\n      created: Math.floor(Date.now() / 1000),\n      content: chunk.content,\n      done: chunk.done,\n      metadata: chunk.metadata,\n    });\n  }\n}\n"
        },
        "src/server/streaming/openai-streamer.ts": {
          "path": "src/server/streaming/openai-streamer.ts",
          "size": 1450,
          "lines": 42,
          "imports": [
            "express",
            "./base-streamer.js"
          ],
          "exports": [
            "OpenAIStreamer"
          ],
          "classes": [
            "OpenAIStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\nimport { BaseStreamer, type StreamChunk, type StreamingOptions, type ProtocolHandlerConfig } from './base-streamer.js';\n\nexport class OpenAIStreamer extends BaseStreamer {\n  constructor(config?: ProtocolHandlerConfig) {\n    super(config);\n  }\n\n  async streamResponse(response: unknown, options: StreamingOptions, res: Response): Promise<void> {\n    try {\n      this.validateOptions(options);\n      if (!this.isStreamingEnabled()) {\n        throw new Error('Streaming is disabled for this handler');\n      }\n\n      this.setupStreamingHeaders(res, options.requestId);\n      this.startStreaming();\n\n      const chunks: StreamChunk[] = Array.isArray(response)\n        ? (response as StreamChunk[])\n        : typeof response === 'object' && response !== null && 'data' in (response as Record<string, unknown>) && Array.isArray((response as any).data)\n          ? ((response as any).data as StreamChunk[])\n          : [];\n\n      for (const chunk of chunks) {\n        const payload = this.formatOpenAIChunk(chunk, options.requestId);\n        const wrote = this.sendSSEData(res, payload);\n        if (!wrote) {\n          break;\n        }\n        this.incrementChunkCount();\n        await this.applyChunkDelay(options.chunkDelay);\n      }\n\n      this.endStreaming(res);\n      this.logStreamingMetrics(options);\n    } catch (error) {\n      this.handleStreamingError(res, error as Error, options.requestId);\n    }\n  }\n}\n"
        },
        "src/server/streaming/responses-streamer.ts": {
          "path": "src/server/streaming/responses-streamer.ts",
          "size": 2074,
          "lines": 65,
          "imports": [
            "express"
          ],
          "exports": [
            "ResponsesStreamer"
          ],
          "classes": [
            "ResponsesStreamer"
          ],
          "functions": [],
          "content": "import { type Response } from 'express';\n\nimport {\n  BaseStreamer,\n  type ProtocolHandlerConfig,\n  type StreamChunk,\n  type StreamingOptions\n} from './base-streamer.js';\n\n/**\n * Protocol-agnostic streamer that falls back to the generic response chunk\n * format. This keeps the new streaming layer compilable while the richer\n * implementation is still being designed.\n */\nexport class ResponsesStreamer extends BaseStreamer {\n  constructor(config?: ProtocolHandlerConfig) {\n    super(config);\n  }\n\n  async streamResponse(response: unknown, options: StreamingOptions, res: Response): Promise<void> {\n    try {\n      this.validateOptions(options);\n      if (!this.isStreamingEnabled()) {\n        throw new Error('Streaming is disabled for this handler');\n      }\n\n      this.setupStreamingHeaders(res, options.requestId);\n      this.startStreaming();\n\n      const chunks: StreamChunk[] = Array.isArray(response)\n        ? (response as StreamChunk[])\n        : typeof response === 'object' && response !== null && 'data' in (response as Record<string, unknown>) && Array.isArray((response as any).data)\n          ? ((response as any).data as StreamChunk[])\n          : [];\n\n      for (const chunk of chunks) {\n        const payload = this.formatResponseChunk(chunk, options.requestId);\n        const wrote = this.sendSSEData(res, payload);\n        if (!wrote) {\n          break;\n        }\n        this.incrementChunkCount();\n        await this.applyChunkDelay(options.chunkDelay);\n      }\n\n      // Emit Anthropic Responses-style completion signal before closing\n      try {\n        const completion = {\n          type: 'response.completed',\n          response: {\n            id: `resp_${options.requestId}`,\n            model: options.model,\n          },\n        };\n        this.sendSSEEvent(res, 'response.completed', JSON.stringify(completion));\n      } catch { /* ignore */ }\n\n      try { res.end(); } catch { /* ignore */ }\n      this.logStreamingMetrics(options);\n    } catch (error) {\n      this.handleStreamingError(res, error as Error, options.requestId);\n    }\n  }\n}\n"
        },
        "src/server/utils/streaming-manager.ts": {
          "path": "src/server/utils/streaming-manager.ts",
          "size": 30916,
          "lines": 791,
          "imports": [
            "express",
            "../handlers/base-handler.js",
            "../../modules/pipeline/utils/debug-logger.js",
            "./text-filters.js",
            "rcc-llmswitch-core/conversion/shared/tooling",
            "./error-context.js",
            "node:os",
            "node:path",
            "node:fs"
          ],
          "exports": [
            "StreamingChunk",
            "StreamingManager"
          ],
          "classes": [
            "StreamingManager"
          ],
          "functions": [
            "emitChatRequiredAction",
            "genCallId",
            "synthesizeFromResponse",
            "toolCalls",
            "pushChunk",
            "as",
            "as",
            "toolCalls",
            "filteredToolCalls",
            "cleanse",
            "writeBeat"
          ],
          "content": "/**\n * Streaming Manager Utility\n * Handles streaming responses for different protocols\n */\n\nimport { type Response } from 'express';\nimport type { ProtocolHandlerConfig } from '../handlers/base-handler.js';\nimport { PipelineDebugLogger } from '../../modules/pipeline/utils/debug-logger.js';\nimport { stripThinkingTags } from './text-filters.js';\nimport { chunkString } from 'rcc-llmswitch-core/conversion/shared/tooling';\nimport { ErrorContextBuilder, EnhancedRouteCodexError, type ErrorContext } from './error-context.js';\nimport os from 'node:os';\nimport path from 'node:path';\nimport fs from 'node:fs';\n\n/**\n * Streaming chunk interface\n */\nexport interface StreamingChunk {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    delta: {\n      content?: string;\n      role?: string;\n      tool_calls?: any[];\n    };\n    finish_reason?: string | null;\n  }>;\n}\n\n/**\n * Streaming Manager Class\n */\nexport class StreamingManager {\n  private config: ProtocolHandlerConfig;\n  private logger: PipelineDebugLogger;\n  private heartbeatTimers: Map<string, NodeJS.Timeout> = new Map();\n  // Track a short window of recent tool_call signatures per request to drop short-range duplicates across chunks\n  private lastToolCallKeys: Map<string, { set: Set<string>; order: string[] }> = new Map();\n  // Per-request SSE event logs\n  private sseLogWriters: Map<string, fs.WriteStream> = new Map();\n\n  constructor(config: ProtocolHandlerConfig) {\n    this.config = config;\n    this.logger = new PipelineDebugLogger(null, {\n      enableConsoleLogging: config.enableMetrics ?? true,\n      enableDebugCenter: false,\n    });\n  }\n\n  /**\n   * Stream response to client\n   */\n  async streamResponse(response: any, requestId: string, res: Response, model: string): Promise<void> {\n    try {\n      // Set appropriate headers for streaming\n      res.setHeader('Content-Type', 'text/event-stream; charset=utf-8');\n      res.setHeader('Cache-Control', 'no-cache, no-transform');\n      res.setHeader('Connection', 'keep-alive');\n      // Disable proxy buffering for Nginx/Cloudflare style proxies to avoid stuck streams\n      try { res.setHeader('X-Accel-Buffering', 'no'); } catch (error) {\n        // 快速死亡原则 - 暴露SSE响应头设置错误\n        throw new EnhancedRouteCodexError(error as Error, {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber(error as Error),\n          additional: {\n            operation: 'sse-buffering-header',\n            requestId,\n            model\n          }\n        });\n      }\n      res.setHeader('x-request-id', requestId);\n      // Flush headers to start the SSE stream immediately\n      try { (res as any).flushHeaders?.(); } catch (error) {\n        // 快速死亡原则 - 暴露SSE响应头刷新错误\n        throw new EnhancedRouteCodexError(error as Error, {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber(error as Error),\n          additional: {\n            operation: 'sse-header-flush',\n            requestId,\n            model\n          }\n        });\n      }\n\n      // Start streaming\n      if (!this.shouldStreamFromPipeline()) {\n        throw new Error('Streaming pipeline is disabled for this endpoint');\n      }\n\n      // Start SSE heartbeats (pre-heartbeat + periodic)\n      this.startHeartbeat(res, requestId, model);\n\n      // Open SSE event log file for Chat\n      try {\n        const base = path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat');\n        fs.mkdirSync(base, { recursive: true });\n        const file = path.join(base, `${requestId}_sse-events.log`);\n        const ws = fs.createWriteStream(file, { flags: 'a' });\n        this.sseLogWriters.set(requestId, ws);\n        this.writeSSELog(requestId, 'stream.start', { requestId, model });\n      } catch (error) {\n        // 快速死亡原则 - 暴露SSE日志初始化错误\n        throw new EnhancedRouteCodexError(error as Error, {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber(error as Error),\n          additional: {\n            operation: 'sse-log-initialization',\n            requestId,\n            model,\n            logDirectory: path.join(os.homedir(), '.routecodex', 'codex-samples', 'openai-chat')\n          }\n        });\n      }\n\n      await this.streamFromPipeline(response, requestId, res, model);\n\n    } catch (error) {\n      this.logger.logModule('StreamingManager', 'stream_error', {\n        requestId,\n        error: error instanceof Error ? error.message : String(error),\n        model,\n      });\n\n      // Send error chunk and close\n      this.sendErrorChunk(res, error, requestId);\n      this.stopHeartbeat(requestId);\n      res.end();\n      try {\n        this.writeSSELog(requestId, 'stream.error', { message: (error as any)?.message || String(error) });\n      } catch (logError) {\n        // 快速死亡原则 - 暴露SSE错误日志写入失败\n        throw new EnhancedRouteCodexError(logError instanceof Error ? logError : new Error(String(logError)), {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber((logError instanceof Error ? logError : new Error(String(logError))) as Error),\n          additional: {\n            operation: 'sse-error-log-write',\n            requestId,\n            originalError: (error as any)?.message || String(error)\n          }\n        });\n      }\n      try {\n        this.closeSSELog(requestId);\n      } catch (logCloseError) {\n        // 快速死亡原则 - 暴露SSE日志关闭失败\n        throw new EnhancedRouteCodexError(logCloseError instanceof Error ? logCloseError : new Error(String(logCloseError)), {\n          module: 'StreamingManager',\n          file: 'src/server/utils/streaming-manager.ts',\n          function: 'streamResponse',\n          line: ErrorContextBuilder.extractLineNumber((logCloseError instanceof Error ? logCloseError : new Error(String(logCloseError))) as Error),\n          additional: {\n            operation: 'sse-log-close',\n            requestId\n          }\n        });\n      }\n    }\n  }\n\n  /**\n   * Stream Anthropic-compatible responses (delegates to generic streaming)\n   */\n  async streamAnthropicResponse(response: any, requestId: string, res: Response, model: string): Promise<void> {\n    await this.streamResponse(response, requestId, res, model);\n  }\n\n  /**\n   * Check if should stream from pipeline\n   */\n  private shouldStreamFromPipeline(): boolean {\n    return this.config.enablePipeline ?? false;\n  }\n\n  /**\n   * Stream from pipeline\n   */\n  private async streamFromPipeline(\n    response: any,\n    requestId: string,\n    res: Response,\n    model: string\n  ): Promise<void> {\n    // Always synthesize streaming from non-stream JSON\n    if (!response || typeof response !== 'object' || !('data' in response)) {\n      throw new Error('Streaming pipeline response is missing data payload');\n    }\n    const data = (response as Record<string, unknown>).data as any;\n    await this.processStreamingData(data, requestId, res, model);\n  }\n\n  /**\n   * Process streaming data\n   */\n  private async processStreamingData(\n    data: any,\n    requestId: string,\n    res: Response,\n    model: string\n  ): Promise<void> {\n    // Helper: unwrap nested { data: {...} } wrappers until we reach an object with\n    // OpenAI Chat response shape (choices/message) or delta chunks.\n    const unwrapData = (obj: any): any => {\n      try {\n        let cur = obj;\n        const seen = new Set<any>();\n        while (cur && typeof cur === 'object' && !Array.isArray(cur) && !seen.has(cur)) {\n          seen.add(cur);\n          // Stop when the payload clearly looks like an OpenAI Chat response or a delta chunk\n          if (Array.isArray((cur as any).choices) || (cur as any).object === 'chat.completion.chunk') {break;}\n          if ('data' in cur && (cur as any).data && typeof (cur as any).data === 'object') {\n            cur = (cur as any).data; continue;\n          }\n          break;\n        }\n        return cur;\n      } catch { return obj; }\n    };\n    const finishReasons: string[] = [];\n    let sawToolCalls = false;\n    const emitChatRequiredAction = (_msg: any) => { /* removed Responses semantics from Chat SSE */ };\n    // Removed user-visible tool hint delta to avoid confusing client UIs.\n    const genCallId = (index: number) => `call_${requestId}_${index}_${Math.random().toString(36).slice(2, 8)}`;\n\n    // Helper: synthesize OpenAI-style streaming chunks from a non-stream JSON response\n    const synthesizeFromResponse = async (resp: any) => {\n      const chunks: any[] = [];\n      try {\n        const msg = resp?.choices?.[0]?.message || {};\n        // 文本→工具归一化已在 llmswitch-core 的 openai-openai codec 处理，这里不重复\n        const role = typeof msg?.role === 'string' ? msg.role : 'assistant';\n        const content = typeof msg?.content === 'string' ? msg.content : '';\n        const toolCallsFromArray = Array.isArray(msg?.tool_calls) ? msg.tool_calls : [];\n        const fnCall = (msg as any)?.function_call || null;\n        const toolCalls = (() => {\n          if (toolCallsFromArray.length) {return toolCallsFromArray;}\n          if (fnCall && typeof fnCall === 'object') {\n            const name = typeof fnCall.name === 'string' ? fnCall.name : undefined;\n            const args = typeof fnCall.arguments === 'string' ? fnCall.arguments : (fnCall.arguments != null ? JSON.stringify(fnCall.arguments) : undefined);\n            if (name || args) {\n              return [{ id: undefined, type: 'function', function: { ...(name?{name}:{ }), ...(args?{arguments: args}:{ }) } }];\n            }\n          }\n          return [] as any[];\n        })();\n\n        // Emit initial role delta to satisfy some clients' expectations\n        chunks.push({ role });\n\n        // Emit content when it不是工具JSON/补丁块回显\n        const isLikelyToolJson = (s: string): boolean => /\"version\"\\s*:\\s*\"rcc\\.tool\\.v1\"/i.test(s) || /rcc\\.tool\\.v1/i.test(s);\n        const isPatchBlock = (s: string): boolean => /\\*\\*\\*\\s*Begin\\s*Patch/i.test(s);\n        if (content.length > 0 && !isLikelyToolJson(content) && !isPatchBlock(content)) {\n          chunks.push({ content });\n        }\n\n        // Emit tool_calls as delta blocks: name then arguments (single shot)\n        if (toolCalls.length > 0) {\n          sawToolCalls = true;\n          for (let i = 0; i < toolCalls.length; i++) {\n            const tc = toolCalls[i] || {};\n            const fn = tc.function || {};\n            const id = typeof tc.id === 'string' && tc.id.trim() ? tc.id : genCallId(i);\n            const name = typeof fn.name === 'string' ? fn.name : undefined;\n            const args = typeof fn.arguments === 'string' ? fn.arguments : (fn.arguments != null ? JSON.stringify(fn.arguments) : undefined);\n            if (name) {\n              chunks.push({ tool_calls: [{ index: i, id, type: 'function', function: { name } }] });\n            }\n            if (typeof args === 'string' && args.length > 0) {\n              for (const d of chunkString(args)) {\n                chunks.push({ tool_calls: [{ index: i, id, type: 'function', function: { arguments: d } }] });\n              }\n            }\n          }\n        }\n\n        let finish = toolCalls.length > 0\n          ? 'tool_calls'\n          : (resp?.choices?.[0]?.finish_reason || resp?.finish_reason || 'stop');\n        // Guard: if upstream finish_reason=='tool_calls' but we didn't extract any,\n        // do not emit misleading tool_calls final\n        if (finish === 'tool_calls' && toolCalls.length === 0) {finish = 'stop';}\n        return { chunks, finish };\n      } catch {\n        return { chunks, finish: 'stop' };\n      }\n    };\n\n    const pushChunk = async (raw: any) => {\n      const normalized = this.normalizeChunk(raw, model, finishReasons);\n      await this.sendChunk(res, normalized, requestId, model);\n      await this.delay(10);\n    };\n\n    if (Array.isArray(data)) {\n      // If array of chunks, stream as-is. Ensure the first delta carries role=assistant\n      try {\n        const first = data[0];\n        if (first) {\n          const preview = this.normalizeChunk(first, model, []);\n          const hasRole = !!(preview?.choices && preview.choices[0]?.delta && typeof preview.choices[0].delta.role === 'string');\n          if (!hasRole) {\n            await this.sendChunk(res, { role: 'assistant' }, requestId, model);\n            await this.delay(10);\n          }\n        }\n      } catch { /* ignore */ }\n\n      for (const chunk of data) {\n        await pushChunk(chunk);\n      }\n      let finalReason = finishReasons.length ? finishReasons[finishReasons.length - 1] : undefined;\n      if (!finalReason && sawToolCalls) { finalReason = 'tool_calls'; }\n      // Removed hint delta to avoid confusing client UIs\n      this.sendFinalChunk(res, requestId, model, finalReason);\n      return;\n    }\n\n    if (typeof data === 'object' && data !== null) {\n      // Align with validated logic: unwrap nested data so we can reliably synthesize\n      const unwrapped = unwrapData(data);\n      // Non-stream JSON response: synthesize delta stream when choices[].message exists\n      if (Array.isArray((unwrapped as any).choices) && (unwrapped as any).choices.length > 0 && (unwrapped as any).choices[0]?.message) {\n        // Align with core: canonicalize response tools before synthesizing to ensure\n        // tool_calls are present when upstream varies shape\n        let source = unwrapped;\n        try {\n          const core = await import('rcc-llmswitch-core/api');\n          const canon = (core as any).processChatResponseTools?.(unwrapped);\n          if (canon && typeof canon === 'object') {source = canon;}\n        } catch { /* non-blocking */ }\n        const { chunks, finish } = await synthesizeFromResponse(source);\n        for (const c of chunks) {\n          await this.sendChunk(res, c, requestId, model);\n          await this.delay(10);\n        }\n        try { emitChatRequiredAction((data as any).choices[0]?.message); } catch { /* ignore */ }\n        this.sendFinalChunk(res, requestId, model, finish);\n        return;\n      }\n      // Fallback: treat as single chunk\n      await pushChunk(unwrapped);\n      const finalReason = finishReasons.length ? finishReasons[finishReasons.length - 1] : undefined;\n      this.sendFinalChunk(res, requestId, model, finalReason);\n      return;\n    }\n  }\n\n  /**\n   * Normalize chunk into OpenAI streaming shape\n   */\n  private normalizeChunk(chunk: any, model: string, finishReasons: string[]): any {\n    if (!chunk || typeof chunk !== 'object') {\n      return chunk;\n    }\n\n    const hasDelta =\n      chunk.object === 'chat.completion.chunk' ||\n      (Array.isArray(chunk.choices) && chunk.choices.some((choice: any) => choice?.delta));\n\n    if (hasDelta) {\n      // Map delta.function_call -> delta.tool_calls for OpenAI older shape\n      try {\n        const choices = Array.isArray(chunk.choices) ? chunk.choices : [];\n        const isImagePath = (p: any): boolean => {\n          try { const s = String(p || '').toLowerCase(); return /\\.(png|jpg|jpeg|gif|webp|bmp|svg|tiff?|ico|heic|jxl)$/.test(s); } catch { return false; }\n        };\n        const parseArgs = (args: any): any => {\n          if (typeof args === 'string') { try { return JSON.parse(args); } catch { return {}; } }\n          if (args && typeof args === 'object') {return args;}\n          return {};\n        };\n        for (const c of choices) {\n          const d = c?.delta || {};\n          const fc = d?.function_call;\n          if (fc && typeof fc === 'object') {\n            const name = typeof fc.name === 'string' ? fc.name : undefined;\n            const args = typeof fc.arguments === 'string' ? fc.arguments : (fc.arguments != null ? JSON.stringify(fc.arguments) : undefined);\n            const tc: any = { index: 0, type: 'function', function: {} as any };\n            if (name) { (tc.function as any).name = name; }\n            if (typeof args === 'string') { (tc.function as any).arguments = args; }\n            if (!Array.isArray(d.tool_calls)) { d.tool_calls = []; }\n            (d.tool_calls as any[]).push(tc);\n          }\n          // Filter invalid view_image tool_calls where path is not an image\n          if (Array.isArray(d.tool_calls)) {\n            const kept: any[] = [];\n            for (const tc of d.tool_calls as any[]) {\n              try {\n                const fn = tc?.function || {};\n                const nm = typeof fn?.name === 'string' ? fn.name : undefined;\n                if (nm === 'view_image') {\n                  const a = parseArgs(fn?.arguments);\n                  const p = (a && typeof a === 'object') ? (a as any).path : undefined;\n                  if (!isImagePath(p)) {\n                    // Replace with a brief hint in content; drop this tool_call\n                    const hint = typeof p === 'string' && p ? `提示：${p} 不是图片，请改用 shell: {\"command\":[\"cat\",\"${p}\"]}` : '提示：路径不是图片，请改用 shell: {\"command\":[\"cat\",\"<path>\"]}';\n                    if (typeof d.content === 'string' && d.content.length > 0) {\n                      d.content += `\\n${hint}`;\n                    } else {\n                      d.content = hint;\n                    }\n                    continue;\n                  }\n                }\n                kept.push(tc);\n              } catch { kept.push(tc); }\n            }\n            d.tool_calls = kept;\n          }\n        }\n      } catch { /* ignore */ }\n      this.captureFinishReason(chunk, finishReasons);\n      return chunk;\n    }\n\n    if (!Array.isArray(chunk.choices)) {\n      return chunk;\n    }\n\n    const normalizedChoices = chunk.choices.map((choice: any, index: number) => {\n      const message = choice?.message || {};\n      const toolCallsArr = Array.isArray(message.tool_calls) ? message.tool_calls : undefined;\n      const fnCall = (message as any)?.function_call || null;\n      const isImagePath = (p: any): boolean => {\n        try { const s = String(p || '').toLowerCase(); return /\\.(png|jpg|jpeg|gif|webp|bmp|svg|tiff?|ico|heic|jxl)$/.test(s); } catch { return false; }\n      };\n      const parseArgs = (args: any): any => {\n        if (typeof args === 'string') { try { return JSON.parse(args); } catch { return {}; } }\n        if (args && typeof args === 'object') {return args;}\n        return {};\n      };\n      const toolCalls = (() => {\n        if (toolCallsArr && toolCallsArr.length) {return toolCallsArr;}\n        if (fnCall && typeof fnCall === 'object') {\n          const name = typeof fnCall.name === 'string' ? fnCall.name : undefined;\n          const args = typeof fnCall.arguments === 'string' ? fnCall.arguments : (fnCall.arguments != null ? JSON.stringify(fnCall.arguments) : undefined);\n          if (name || args) {\n            return [{ id: undefined, type: 'function', function: { ...(name?{name}:{ }), ...(args?{arguments: args}:{ }) } }];\n          }\n        }\n        return undefined;\n      })();\n      // Filter invalid view_image tool_calls when arguments contain a non-image path\n      const filteredToolCalls = (() => {\n        if (!Array.isArray(toolCalls)) {return toolCalls;}\n        const kept: any[] = [];\n        for (const tc of toolCalls) {\n          try {\n            const fn = (tc as any)?.function || {};\n            const nm = typeof fn?.name === 'string' ? fn.name : undefined;\n            if (nm === 'view_image') {\n              const a = parseArgs((fn as any).arguments);\n              const p = (a && typeof a === 'object') ? (a as any).path : undefined;\n              if (!isImagePath(p)) {\n                const hint = typeof p === 'string' && p ? `提示：${p} 不是图片，请改用 shell: {\"command\":[\"cat\",\"${p}\"]}` : '提示：路径不是图片，请改用 shell: {\"command\":[\"cat\",\"<path>\"]}';\n                if (typeof message.content === 'string' && message.content.length > 0) {\n                  message.content += `\\n${hint}`;\n                } else {\n                  (message as any).content = hint;\n                }\n                continue;\n              }\n            }\n            kept.push(tc);\n          } catch { kept.push(tc); }\n        }\n        return kept;\n      })();\n      const delta: Record<string, unknown> = {};\n      if (message.role && typeof message.role === 'string') {\n        delta.role = message.role;\n      }\n      if (typeof message.content === 'string') {\n        const s = message.content as string;\n        const looksToolJson = /\"version\"\\s*:\\s*\"rcc\\.tool\\.v1\"/i.test(s) || /rcc\\.tool\\.v1/i.test(s);\n        const looksPatch = /\\*\\*\\*\\s*Begin\\s*Patch/i.test(s);\n        if (!looksToolJson && !looksPatch) {\n          delta.content = s;\n        }\n      }\n      if (filteredToolCalls) {\n        delta.tool_calls = filteredToolCalls;\n      }\n      if (typeof choice.content === 'string' && !delta.content) {\n        delta.content = choice.content;\n      }\n\n      const finishReason = choice?.finish_reason ?? (message?.finish_reason as string | undefined);\n      if (finishReason) {\n        finishReasons.push(finishReason);\n      }\n\n      return {\n        index: choice?.index ?? index,\n        delta,\n        finish_reason: null\n      };\n    });\n\n    return {\n      id: chunk.id ?? `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: chunk.created ?? Math.floor(Date.now() / 1000),\n      model: chunk.model ?? model,\n      choices: normalizedChoices\n    };\n  }\n\n  private captureFinishReason(chunk: any, finishReasons: string[]): void {\n    if (!chunk || typeof chunk !== 'object') {\n      return;\n    }\n    const choices = Array.isArray(chunk.choices) ? chunk.choices : [];\n    for (const choice of choices) {\n      const reason = choice?.finish_reason;\n      if (typeof reason === 'string' && reason.length > 0) {\n        finishReasons.push(reason);\n      }\n    }\n  }\n\n  /**\n   * Send chunk to response\n   */\n  private async sendChunk(\n    res: Response,\n    chunk: StreamingChunk | any,\n    requestId: string,\n    model: string\n  ): Promise<void> {\n    // Clean reasoning/thinking tags in content fields (delta.content and message.content)\n    const cleanse = (obj: any) => {\n      try {\n        if (!obj || typeof obj !== 'object') {return obj;}\n        const choices = Array.isArray(obj.choices) ? obj.choices : [];\n        for (const c of choices) {\n          if (c && typeof c === 'object') {\n            const d = c.delta || {};\n            if (typeof d.content === 'string') { d.content = stripThinkingTags(d.content); }\n            if (c.message && typeof c.message === 'object' && typeof c.message.content === 'string') {\n              c.message.content = stripThinkingTags(c.message.content);\n            }\n          }\n        }\n      } catch { /* ignore */ }\n      return obj;\n    };\n\n    // If passing a light delta object (e.g., { role }, { content }, { tool_calls }), clean its content\n    if (chunk && typeof chunk === 'object' && !('id' in chunk)) {\n      if (typeof (chunk as any).content === 'string') {\n        (chunk as any).content = stripThinkingTags((chunk as any).content);\n      }\n    }\n\n    const chunkData = typeof chunk === 'object' && chunk.id ? cleanse(chunk) : {\n      id: `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model,\n      choices: [{\n        index: 0,\n        delta: chunk,\n        finish_reason: null\n      }]\n    };\n\n    // Short-window duplicate tool_call dedupe (per request, across chunks)\n    try {\n      const choices = Array.isArray((chunkData as any).choices) ? (chunkData as any).choices : [];\n      for (const c of choices) {\n        const d = c?.delta || {};\n        if (Array.isArray(d.tool_calls) && d.tool_calls.length > 0) {\n          const kept: any[] = [];\n          const WINDOW = Math.max(1, Math.min(16, Number(process.env.ROUTECODEX_TOOL_CALL_DEDUPE_WINDOW || 4)));\n          const bucket = this.lastToolCallKeys.get(requestId) || { set: new Set<string>(), order: [] };\n          for (const tc of d.tool_calls as any[]) {\n            try {\n              const fn = tc?.function || {};\n              const name = typeof fn?.name === 'string' ? fn.name : '';\n              const args = fn?.arguments;\n              const argsStr = typeof args === 'string' ? args : (args != null ? JSON.stringify(args) : '');\n              const key = `${name}\\n${argsStr}`;\n              const hasBoth = name && argsStr;\n              if (hasBoth && bucket.set.has(key)) { continue; }\n              kept.push(tc);\n              if (hasBoth) {\n                bucket.set.add(key);\n                bucket.order.push(key);\n                if (bucket.order.length > WINDOW) {\n                  const old = bucket.order.shift();\n                  if (old) {bucket.set.delete(old);}\n                }\n              }\n            } catch { kept.push(tc); }\n          }\n          d.tool_calls = kept;\n          this.lastToolCallKeys.set(requestId, bucket);\n        }\n      }\n    } catch { /* ignore dedupe errors */ }\n\n    const sseData = `data: ${JSON.stringify(chunkData)}\\n\\n`;\n    res.write(sseData);\n    try { this.writeSSELog(requestId, 'chunk', chunkData); } catch { /* ignore */ }\n\n    try {\n      const LOG = String(process.env.ROUTECODEX_LOG_STREAM_CHUNKS || process.env.RCC_LOG_STREAM_CHUNKS || '0') === '1';\n      if (LOG) {\n        this.logger.logModule('StreamingManager', 'chunk_sent', {\n          requestId,\n          chunkId: chunkData.id,\n          model,\n        });\n      }\n    } catch { /* no-op */ }\n  }\n\n  /**\n   * Send final chunk\n   */\n  private sendFinalChunk(res: Response, requestId: string, model: string, finishReason?: string): void {\n    const finalChunk = {\n      id: `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model,\n      choices: [{\n        index: 0,\n        delta: {},\n        finish_reason: finishReason ?? 'stop'\n      }]\n    };\n\n    // 可选：发出 Responses 风格的 done 事件（默认关闭，以避免跨协议混用）\n    try {\n      const EMIT_RESP_DONE = String(process.env.ROUTECODEX_CHAT_RESP_DONE || '0') === '1';\n      if (EMIT_RESP_DONE) {\n        const evt = { type: 'response.done' } as Record<string, unknown>;\n        res.write(`event: response.done\\n`);\n        res.write(`data: ${JSON.stringify(evt)}\\n\\n`);\n      }\n    } catch { /* ignore */ }\n    const finalData = `data: ${JSON.stringify(finalChunk)}\\n\\ndata: [DONE]\\n\\n`;\n    res.write(finalData);\n    this.stopHeartbeat(requestId);\n    res.end();\n    // Reset short-window cache for this request\n    try { this.lastToolCallKeys.delete(requestId); } catch { /* ignore */ }\n    try {\n      this.writeSSELog(requestId, 'chunk.final', finalChunk);\n      this.writeSSELog(requestId, 'done', { requestId });\n      this.closeSSELog(requestId);\n    } catch { /* ignore */ }\n\n    try {\n      const LOG = String(process.env.ROUTECODEX_LOG_STREAM_CHUNKS || process.env.RCC_LOG_STREAM_CHUNKS || '0') === '1';\n      if (LOG) {\n        this.logger.logModule('StreamingManager', 'stream_complete', {\n          requestId,\n          model,\n        });\n      }\n    } catch { /* no-op */ }\n  }\n\n  /**\n   * Send error chunk\n   */\n  private sendErrorChunk(res: Response, error: any, requestId: string): void {\n    const errorChunk = {\n      id: `chatcmpl-${Date.now()}`,\n      object: 'chat.completion.chunk',\n      created: Math.floor(Date.now() / 1000),\n      model: 'unknown',\n      choices: [{\n        index: 0,\n        delta: {\n          content: `Error: ${error instanceof Error ? error.message : String(error)}`\n        },\n        finish_reason: 'error'\n      }]\n    };\n\n    try {\n      const evt = { type: 'response.done' } as Record<string, unknown>;\n      res.write(`event: response.done\\n`);\n      res.write(`data: ${JSON.stringify(evt)}\\n\\n`);\n    } catch { /* ignore */ }\n    const errorData = `data: ${JSON.stringify(errorChunk)}\\n\\ndata: [DONE]\\n\\n`;\n    res.write(errorData);\n    this.stopHeartbeat(requestId);\n    try {\n      this.writeSSELog(requestId, 'error', errorChunk);\n      this.writeSSELog(requestId, 'done', { requestId });\n      this.closeSSELog(requestId);\n    } catch { /* ignore */ }\n  }\n\n  // Write one SSE log record line\n  private writeSSELog(requestId: string, event: string, data: any): void {\n    try {\n      const ws = this.sseLogWriters.get(requestId);\n      if (!ws) {return;}\n      ws.write(`${JSON.stringify({ ts: Date.now(), requestId, event, data })  }\\n`);\n    } catch { /* ignore */ }\n  }\n\n  private closeSSELog(requestId: string): void {\n    try {\n      const ws = this.sseLogWriters.get(requestId);\n      if (ws) {\n        try { ws.end(); } catch { /* ignore */ }\n      }\n    } catch { /* ignore */ }\n    finally { this.sseLogWriters.delete(requestId); }\n  }\n\n  /**\n   * Delay utility\n   */\n  private delay(ms: number): Promise<void> {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n\n  // Heartbeat helpers\n  private startHeartbeat(res: Response, requestId: string, model: string): void {\n    try {\n      const iv = Math.max(1000, Number(process.env.ROUTECODEX_STREAM_HEARTBEAT_MS || process.env.RCC_STREAM_HEARTBEAT_MS || 15000));\n      const pre = String(process.env.ROUTECODEX_STREAM_PRE_HEARTBEAT || process.env.RCC_STREAM_PRE_HEARTBEAT || '1') === '1';\n      const writeBeat = () => {\n        try {\n          const payload = { type: 'heartbeat', ts: Date.now(), model };\n          res.write(`data: ${JSON.stringify(payload)}\\n\\n`);\n        } catch { /* ignore */ }\n      };\n      if (pre) {writeBeat();}\n      const timer = setInterval(writeBeat, iv);\n      this.heartbeatTimers.set(requestId, timer);\n      try { res.on('close', () => this.stopHeartbeat(requestId)); } catch { /* ignore */ }\n    } catch { /* ignore */ }\n  }\n\n  private stopHeartbeat(requestId: string): void {\n    try {\n      const t = this.heartbeatTimers.get(requestId);\n      if (t) { clearInterval(t); this.heartbeatTimers.delete(requestId); }\n    } catch { /* ignore */ }\n  }\n\n  /**\n   * Check if response is streamable\n   */\n  isStreamable(response: any): boolean {\n    return (\n      response?.stream === true ||\n      (typeof response?.pipe === 'function') ||\n      (Array.isArray(response?.data)) ||\n      (this.config.enableStreaming === true)\n    );\n  }\n\n  /**\n   * Get streaming statistics\n   */\n  getStreamingStats(): {\n    enabled: boolean;\n    config: ProtocolHandlerConfig;\n  } {\n    return {\n      enabled: this.config.enableStreaming ?? false,\n      config: this.config,\n    };\n  }\n}\n"
        }
      }
    },
    "v2": {
      "path": "./src/server-v2",
      "totalFiles": 8,
      "totalLines": 2309,
      "totalSize": 64530,
      "modules": {
        "src/server-v2/core/route-codex-server-v2.ts": {
          "path": "src/server-v2/core/route-codex-server-v2.ts",
          "size": 18139,
          "lines": 654,
          "imports": [
            "express",
            "cors",
            "helmet",
            "rcc-basemodule",
            "rcc-errorhandling",
            "rcc-debugcenter",
            "../../types/common-types.js",
            "../hooks/server-v2-hook-integration.js",
            "../hooks/impl/server-request-logging-hook.js",
            "../hooks/impl/server-response-enhancement-hook.js",
            "../hooks/impl/server-performance-monitoring-hook.js"
          ],
          "exports": [
            "ServerConfigV2",
            "ServerStatusV2",
            "RequestContextV2",
            "RouteCodexServerV2"
          ],
          "classes": [
            "RouteCodexServerV2"
          ],
          "functions": [],
          "content": "/**\n * RouteCodex Server V2 - 渐进式重构版本\n *\n * 核心特性：\n * - 与现有V1服务器完全并行\n * - 集成系统hooks模块\n * - 模块化设计，职责分离\n * - 保持API兼容性\n */\n\nimport express, { type Application, type Request, type Response, type NextFunction } from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { BaseModule, type ModuleInfo } from 'rcc-basemodule';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport type { UnknownObject } from '../../types/common-types.js';\n// TODO: Fix hook type issues and re-enable\n// import { ServerV2HookIntegration, type ServerV2HookContext } from '../hooks/server-v2-hook-integration.js';\n// import { ServerRequestLoggingHook } from '../hooks/impl/server-request-logging-hook.js';\n// import { ServerResponseEnhancementHook } from '../hooks/impl/server-response-enhancement-hook.js';\n// import { ServerPerformanceMonitoringHook } from '../hooks/impl/server-performance-monitoring-hook.js';\n\n/**\n * V2服务器配置接口\n */\nexport interface ServerConfigV2 {\n  server: {\n    port: number;\n    host: string;\n    cors?: {\n      origin: string | string[];\n      credentials?: boolean;\n    };\n    timeout?: number;\n    bodyLimit?: string;\n    useV2?: boolean;  // V2特定配置\n  };\n  logging: {\n    level: 'debug' | 'info' | 'warn' | 'error';\n    enableConsole?: boolean;\n    enableFile?: boolean;\n    filePath?: string;\n  };\n  providers: Record<string, UnknownObject>;\n  v2Config?: {\n    enableHooks?: boolean;\n    enableMiddleware?: boolean;\n    hookStages?: string[];\n  };\n}\n\n/**\n * V2服务器状态\n */\nexport interface ServerStatusV2 {\n  initialized: boolean;\n  running: boolean;\n  port: number;\n  host: string;\n  uptime: number;\n  memory: NodeJS.MemoryUsage;\n  version: 'v2';\n  hooksEnabled: boolean;\n  middlewareEnabled: boolean;\n}\n\n/**\n * 请求上下文接口\n */\nexport interface RequestContextV2 {\n  requestId: string;\n  timestamp: number;\n  method: string;\n  url: string;\n  userAgent?: string;\n  ip?: string;\n  endpoint: string;\n}\n\n/**\n * RouteCodex Server V2\n *\n * 与V1完全并行实现，集成系统hooks\n */\nexport class RouteCodexServerV2 extends BaseModule {\n  private app: Application;\n  private server?: unknown;\n  private config: ServerConfigV2;\n  private errorHandling: ErrorHandlingCenter;\n  private debugEventBus: DebugEventBus;\n  private _isInitialized: boolean = false;\n  private _isRunning: boolean = false;\n\n  // V2特性\n  private hooksEnabled: boolean = false;\n  private middlewareEnabled: boolean = false;\n  // private hookIntegration?: ServerV2HookIntegration; // TODO: Re-enable after type fixes\n\n  constructor(config: ServerConfigV2) {\n    const moduleInfo: ModuleInfo = {\n      id: 'routecodex-server-v2',\n      name: 'RouteCodexServerV2',\n      version: '2.0.0',\n      description: 'RouteCodex Server V2 with enhanced hooks and middleware',\n      type: 'server',\n    };\n\n    super(moduleInfo);\n\n    this.config = config;\n    this.app = express();\n    this.errorHandling = new ErrorHandlingCenter();\n    this.debugEventBus = DebugEventBus.getInstance();\n\n    // V2特性初始化\n    this.hooksEnabled = config.v2Config?.enableHooks ?? false;\n    this.middlewareEnabled = config.v2Config?.enableMiddleware ?? false;\n\n    // 初始化Hook集成系统 (如果启用)\n    if (this.hooksEnabled) {\n      console.log('[RouteCodexServerV2] Hooks enabled, but integration temporarily disabled for testing');\n      // this.initializeHookIntegration(); // 暂时禁用避免类型问题\n    }\n\n    console.log(`[RouteCodexServerV2] Initialized with hooks: ${this.hooksEnabled}, middleware: ${this.middlewareEnabled}`);\n  }\n\n  /**\n   * 初始化服务器\n   */\n  public async initialize(): Promise<void> {\n    try {\n      console.log('[RouteCodexServerV2] Starting initialization...');\n\n      // 初始化错误处理\n      await this.errorHandling.initialize();\n\n      // 设置调试事件监听\n      this.setupDebugLogging();\n\n      // 设置中间件\n      await this.setupMiddleware();\n\n      // 设置路由\n      await this.setupRoutes();\n\n      // 设置错误处理\n      this.setupErrorHandling();\n\n      this._isInitialized = true;\n\n      this.logEvent('server-v2', 'initialized', {\n        port: this.config.server.port,\n        host: this.config.server.host,\n        hooksEnabled: this.hooksEnabled,\n        middlewareEnabled: this.middlewareEnabled,\n        version: '2.0.0'\n      });\n\n      console.log('[RouteCodexServerV2] Initialization completed successfully');\n\n    } catch (error) {\n      await this.handleError(error as Error, 'initialization');\n      throw error;\n    }\n  }\n\n  /**\n   * 启动服务器\n   */\n  public async start(): Promise<void> {\n    if (!this._isInitialized) {\n      await this.initialize();\n    }\n\n    return new Promise((resolve, reject) => {\n      this.server = this.app.listen(this.config.server.port, this.config.server.host, () => {\n        this._isRunning = true;\n\n        this.logEvent('server-v2', 'started', {\n          port: this.config.server.port,\n          host: this.config.server.host,\n          version: 'v2'\n        });\n\n        console.log(`[RouteCodexServerV2] Server started on ${this.config.server.host}:${this.config.server.port}`);\n        resolve();\n      });\n\n      (this.server as any).on('error', async (error: Error) => {\n        await this.handleError(error, 'server_start');\n        reject(error);\n      });\n    });\n  }\n\n  /**\n   * 停止服务器\n   */\n  public async stop(): Promise<void> {\n    if (this.server) {\n      return new Promise(resolve => {\n        (this.server as any)?.close(async () => {\n          this._isRunning = false;\n\n          this.logEvent('server-v2', 'stopped', {});\n\n          // 清理资源\n          await this.errorHandling.destroy();\n\n          console.log('[RouteCodexServerV2] Server stopped');\n          resolve();\n        });\n      });\n    }\n  }\n\n  /**\n   * 获取服务器状态\n   */\n  public getStatus(): ServerStatusV2 {\n    return {\n      initialized: this._isInitialized,\n      running: this._isRunning,\n      port: this.config.server.port,\n      host: this.config.server.host,\n      uptime: process.uptime(),\n      memory: process.memoryUsage(),\n      version: 'v2',\n      hooksEnabled: this.hooksEnabled,\n      middlewareEnabled: this.middlewareEnabled\n    };\n  }\n\n  /**\n   * V1兼容接口：检查是否已初始化\n   */\n  public isInitialized(): boolean {\n    return this._isInitialized;\n  }\n\n  /**\n   * V1兼容接口：检查是否正在运行\n   */\n  public isRunning(): boolean {\n    return this._isRunning;\n  }\n\n  /**\n   * 设置中间件\n   */\n  private async setupMiddleware(): Promise<void> {\n    console.log('[RouteCodexServerV2] Setting up middleware...');\n\n    // 基础安全中间件\n    this.app.use(helmet());\n\n    // CORS中间件\n    if (this.config.server.cors) {\n      this.app.use(cors(this.config.server.cors));\n    } else {\n      this.app.use(cors({\n        origin: '*',\n        credentials: true,\n      }));\n    }\n\n    // 请求解析中间件\n    this.app.use(express.json({ limit: this.config.server.bodyLimit || '10mb' }));\n    this.app.use(express.urlencoded({ extended: true, limit: this.config.server.bodyLimit || '10mb' }));\n\n    // 请求日志中间件\n    this.app.use((req: Request, res: Response, next: NextFunction) => {\n      const start = Date.now();\n      const requestId = this.generateRequestId();\n\n      // 添加请求ID到请求对象\n      (req as any).__requestId = requestId;\n\n      res.on('finish', () => {\n        const duration = Date.now() - start;\n        this.logEvent('request-v2', 'completed', {\n          requestId,\n          method: req.method,\n          url: req.url,\n          status: res.statusCode,\n          duration,\n          userAgent: req.get('user-agent'),\n          ip: req.ip\n        });\n      });\n\n      next();\n    });\n\n    console.log('[RouteCodexServerV2] Middleware setup completed');\n  }\n\n  /**\n   * 设置路由\n   */\n  private async setupRoutes(): Promise<void> {\n    console.log('[RouteCodexServerV2] Setting up routes...');\n\n    // V2健康检查端点 (不同端口避免冲突)\n    this.app.get('/health-v2', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        status: status.running ? 'healthy' : 'unhealthy',\n        version: 'v2',\n        timestamp: new Date().toISOString(),\n        uptime: status.uptime,\n        memory: status.memory,\n        hooksEnabled: status.hooksEnabled,\n        middlewareEnabled: status.middlewareEnabled\n      });\n    });\n\n    // V1兼容的健康检查\n    this.app.get('/health', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        status: status.running ? 'healthy' : 'unhealthy',\n        version: 'v2',\n        timestamp: new Date().toISOString(),\n        uptime: status.uptime,\n        memory: status.memory\n      });\n    });\n\n    // V2专用端点 (用于测试)\n    this.app.post('/v2/chat/completions', this.handleChatCompletionsV2.bind(this));\n\n    // 状态端点\n    this.app.get('/status-v2', (req: Request, res: Response) => {\n      res.json(this.getStatus());\n    });\n\n    // V1兼容的状态端点\n    this.app.get('/status', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        initialized: status.initialized,\n        running: status.running,\n        port: status.port,\n        host: status.host,\n        uptime: status.uptime,\n        memory: status.memory\n      });\n    });\n\n    // 模型列表端点 (V1兼容)\n    this.app.get('/v1/models', this.handleModels.bind(this));\n\n    // 404处理器\n    this.app.use('*', (req: Request, res: Response) => {\n      res.status(404).json({\n        error: {\n          message: 'Not Found',\n          type: 'not_found_error',\n          code: 'not_found',\n        },\n      });\n    });\n\n    console.log('[RouteCodexServerV2] Routes setup completed');\n  }\n\n  /**\n   * V2专用Chat Completions处理器\n   */\n  private async handleChatCompletionsV2(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = (req as any).__requestId || this.generateRequestId();\n\n    const context: RequestContextV2 = {\n      requestId,\n      timestamp: startTime,\n      method: req.method,\n      url: req.url,\n      userAgent: req.get('user-agent'),\n      ip: req.ip,\n      endpoint: '/v2/chat/completions'\n    };\n\n    this.logEvent('api-v2', 'chat_completions_request', {\n      requestId,\n      model: req.body.model,\n      messageCount: req.body.messages?.length || 0,\n      streaming: req.body.stream || false,\n      tools: !!req.body.tools\n    });\n\n    try {\n      // V2处理逻辑 (Hook集成暂时禁用)\n      if (this.hooksEnabled) {\n        console.log('[RouteCodexServerV2] Hooks would be executed here (temporarily disabled)');\n      }\n\n      // 这里将来会连接Pipeline系统，现在使用占位符响应\n      const response = {\n        id: `chatcmpl-${requestId}`,\n        object: 'chat.completion',\n        created: Math.floor(Date.now() / 1000),\n        model: req.body.model || 'gpt-3.5-turbo',\n        choices: [{\n          index: 0,\n          message: {\n            role: 'assistant',\n            content: '[V2 Response] This is a response from RouteCodex Server V2. Hooks and snapshot integration are temporarily disabled for testing.'\n          },\n          finish_reason: 'stop'\n        }],\n        usage: {\n          prompt_tokens: 10,\n          completion_tokens: 25,\n          total_tokens: 35\n        },\n        // V2增强标识\n        serverV2Enhanced: true,\n        processingTime: Date.now() - startTime\n      };\n\n      res.json(response);\n\n      this.logEvent('api-v2', 'chat_completions_success', {\n        requestId,\n        duration: Date.now() - startTime,\n        model: response.model\n      });\n\n    } catch (error) {\n      await this.handleError(error as Error, 'chat_completions_v2');\n\n      res.status(500).json({\n        error: {\n          message: 'Internal Server Error',\n          type: 'internal_error',\n          code: 'internal_error'\n        }\n      });\n    }\n  }\n\n  /**\n   * 处理模型列表 (V1兼容)\n   */\n  private async handleModels(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api-v2', 'models_request', {});\n\n      // 从配置中获取模型列表\n      const models = [];\n      for (const [providerId, providerConfig] of Object.entries(this.config.providers)) {\n        if (providerConfig.enabled && providerConfig.models) {\n          for (const [modelId, _modelConfig] of Object.entries(providerConfig.models)) {\n            models.push({\n              id: modelId,\n              object: 'model',\n              created: Math.floor(Date.now() / 1000),\n              owned_by: providerId,\n            });\n          }\n        }\n      }\n\n      res.json({\n        object: 'list',\n        data: models,\n      });\n    } catch (error) {\n      await this.handleError(error as Error, 'models_handler_v2');\n      throw error;\n    }\n  }\n\n  /**\n   * 设置错误处理\n   */\n  private setupErrorHandling(): void {\n    this.app.use((error: UnknownObject, req: Request, res: Response, _next: NextFunction) => {\n      this.handleError(error as unknown as Error, 'request_handler').catch(() => {\n        // 忽略处理器错误，直接发送响应\n      });\n\n      const errorStatus = (error as any).status || 500;\n      res.status(errorStatus).json({\n        error: {\n          message: (error as any).message || 'Internal Server Error',\n          type: (error as any).type || 'internal_error',\n          code: (error as any).code || 'internal_error',\n        },\n      });\n    });\n  }\n\n  /**\n   * 设置调试日志\n   */\n  private setupDebugLogging(): void {\n    this.debugEventBus.subscribe('*', (event: UnknownObject) => {\n      if (this.config.logging.level === 'debug') {\n        const timestamp = new Date(event.timestamp as any).toISOString();\n        const eventData = event.data as Record<string, unknown>;\n        const category = (eventData?.category as string) || 'general';\n        const action = (eventData?.action as string) || 'unknown';\n\n        const logMessage = `[${timestamp}] [DEBUG-V2] ${event.operationId}: ${JSON.stringify(\n          {\n            category,\n            action,\n            moduleId: event.moduleId,\n            data: event.data,\n          },\n          null,\n          2\n        )}\\n`;\n\n        console.log(`[DEBUG-V2] ${event.operationId}:`, {\n          category,\n          action,\n          timestamp,\n          moduleId: event.moduleId,\n          data: event.data,\n        });\n      }\n    });\n  }\n\n  /**\n   * 记录事件到调试中心\n   */\n  private logEvent(category: string, action: string, data: UnknownObject): void {\n    try {\n      this.debugEventBus.publish({\n        sessionId: `session_v2_${Date.now()}`,\n        moduleId: this.getModuleInfo().id,\n        operationId: `${category}_${action}_v2`,\n        timestamp: Date.now(),\n        type: 'start',\n        position: 'middle',\n        data: {\n          category,\n          action,\n          version: 'v2',\n          ...data,\n        },\n      });\n    } catch (error) {\n      console.error('[RouteCodexServerV2] Failed to log event:', error);\n    }\n  }\n\n  /**\n   * 处理错误\n   */\n  private async handleError(error: Error, context: string): Promise<void> {\n    try {\n      await this.errorHandling.handleError({\n        error: error.message,\n        source: `${this.getModuleInfo().id}.${context}`,\n        severity: 'medium',\n        timestamp: Date.now(),\n        moduleId: this.getModuleInfo().id,\n        context: {\n          stack: error.stack,\n          name: error.name,\n          version: 'v2'\n        },\n      });\n    } catch (handlerError) {\n      console.error('[RouteCodexServerV2] Failed to handle error:', handlerError);\n      console.error('[RouteCodexServerV2] Original error:', error);\n    }\n  }\n\n  /**\n   * 初始化Hook集成系统 (暂时禁用)\n   */\n  private initializeHookIntegration(): void {\n    console.log('[RouteCodexServerV2] Hook Integration temporarily disabled for testing');\n    // TODO: Fix hook type issues and re-enable\n    /*\n    this.hookIntegration = new ServerV2HookIntegration({\n      enabled: true,\n      snapshot: {\n        enabled: true,\n        level: 'normal',\n        phases: ['server-entry', 'server-pre-process', 'server-post-process', 'server-response', 'server-final']\n      },\n      hooks: {\n        enabled: true,\n        timeout: 5000,\n        parallel: false,\n        retryAttempts: 2\n      }\n    });\n\n    this.hookIntegration.registerHook(new ServerRequestLoggingHook());\n    this.hookIntegration.registerHook(new ServerResponseEnhancementHook());\n    this.hookIntegration.registerHook(new ServerPerformanceMonitoringHook());\n    */\n  }\n\n  /**\n   * 创建Hook上下文 (暂时禁用)\n   */\n  private createHookContext(requestId: string, endpoint: string, originalRequest?: UnknownObject): any {\n    // TODO: Re-enable after type fixes\n    return {\n      serverVersion: 'v2',\n      endpoint,\n      requestId,\n      originalRequest,\n      metadata: {\n        startTime: Date.now()\n      }\n    };\n  }\n\n  /**\n   * 执行Hook并记录快照 (暂时禁用)\n   */\n  private async executeHooksWithSnapshot(\n    phase: 'server-entry' | 'server-pre-process' | 'server-post-process' | 'server-response' | 'server-final',\n    data: UnknownObject,\n    context: any\n  ): Promise<{ data: UnknownObject; executionTime: number }> {\n    // TODO: Re-enable after type fixes\n    console.log(`[RouteCodexServerV2] Hook execution for phase ${phase} temporarily disabled`);\n    return { data, executionTime: 0 };\n  }\n\n  /**\n   * 生成请求ID\n   */\n  private generateRequestId(): string {\n    return `req-v2-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  /**\n   * 获取模块信息\n   */\n  public getModuleInfo(): ModuleInfo {\n    return {\n      id: 'routecodex-server-v2',\n      name: 'RouteCodexServerV2',\n      version: '2.0.0',\n      description: 'RouteCodex Server V2 with enhanced hooks and middleware',\n      type: 'server',\n    };\n  }\n}"
        },
        "src/server-v2/handlers/chat-completions-v2.ts": {
          "path": "src/server-v2/handlers/chat-completions-v2.ts",
          "size": 8368,
          "lines": 283,
          "imports": [
            "express",
            "../core/route-codex-server-v2.js"
          ],
          "exports": [
            "ChatCompletionsHandlerV2"
          ],
          "classes": [
            "BaseHandlerV2",
            "ChatCompletionsHandlerV2"
          ],
          "functions": [],
          "content": "/**\n * Chat Completions Handler V2\n *\n * V2版本的Chat Completions处理器\n * 集成hooks系统，模块化设计\n */\n\nimport { type Request, type Response } from 'express';\nimport type { RequestContextV2 } from '../core/route-codex-server-v2.js';\n\n/**\n * V2处理器基类\n */\nexport abstract class BaseHandlerV2 {\n  protected generateRequestId(): string {\n    return `req-v2-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  protected createContext(req: Request): RequestContextV2 {\n    return {\n      requestId: (req as any).__requestId || this.generateRequestId(),\n      timestamp: Date.now(),\n      method: req.method,\n      url: req.url,\n      userAgent: req.get('user-agent'),\n      ip: req.ip,\n      endpoint: req.path\n    };\n  }\n\n  protected sendJsonResponse(res: Response, data: any, context: RequestContextV2): void {\n    try {\n      res.setHeader('x-request-id', context.requestId);\n      res.setHeader('x-server-version', 'v2');\n      res.json(data);\n    } catch (error) {\n      console.error('[BaseHandlerV2] Failed to send response:', error);\n      res.status(500).json({\n        error: {\n          message: 'Internal Server Error',\n          type: 'response_error',\n          code: 'response_error'\n        }\n      });\n    }\n  }\n}\n\n/**\n * Chat Completions Handler V2\n */\nexport class ChatCompletionsHandlerV2 extends BaseHandlerV2 {\n  private hooksEnabled: boolean;\n  private pipelineEnabled: boolean;\n\n  constructor(hooksEnabled: boolean = true, pipelineEnabled: boolean = false) {\n    super();\n    this.hooksEnabled = hooksEnabled;\n    this.pipelineEnabled = pipelineEnabled;\n    console.log(`[ChatCompletionsHandlerV2] Initialized (hooks: ${hooksEnabled}, pipeline: ${pipelineEnabled})`);\n  }\n\n  /**\n   * 处理Chat Completions请求\n   */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const context = this.createContext(req);\n    const startTime = Date.now();\n\n    console.log(`[ChatCompletionsHandlerV2] Processing request ${context.requestId}`);\n\n    try {\n      // 执行请求前Hooks (预留)\n      if (this.hooksEnabled) {\n        await this.executePreRequestHooks(req, context);\n      }\n\n      // 验证请求\n      await this.validateRequest(req, context);\n\n      // 处理请求\n      const response = await this.processRequest(req, context);\n\n      // 执行响应后Hooks (预留)\n      if (this.hooksEnabled) {\n        await this.executePostResponseHooks(response, context);\n      }\n\n      // 发送响应\n      this.sendJsonResponse(res, response, context);\n\n      const duration = Date.now() - startTime;\n      console.log(`[ChatCompletionsHandlerV2] Request ${context.requestId} completed in ${duration}ms`);\n\n    } catch (error) {\n      await this.handleError(error as Error, res, context);\n    }\n  }\n\n  /**\n   * 验证请求\n   */\n  private async validateRequest(req: Request, context: RequestContextV2): Promise<void> {\n    const body = req.body;\n\n    if (!body) {\n      throw new Error('Request body is required');\n    }\n\n    if (!body.model) {\n      throw new Error('Model is required');\n    }\n\n    if (!body.messages || !Array.isArray(body.messages)) {\n      throw new Error('Messages must be an array');\n    }\n\n    if (body.messages.length === 0) {\n      throw new Error('Messages array cannot be empty');\n    }\n\n    // 验证消息格式\n    for (const [index, message] of body.messages.entries()) {\n      if (!message.role || !message.content) {\n        throw new Error(`Message at index ${index} must have 'role' and 'content' fields`);\n      }\n\n      const validRoles = ['system', 'user', 'assistant', 'tool'];\n      if (!validRoles.includes(message.role)) {\n        throw new Error(`Invalid role '${message.role}' in message at index ${index}`);\n      }\n    }\n\n    console.log(`[ChatCompletionsHandlerV2] Request validation passed for ${context.requestId}`);\n  }\n\n  /**\n   * 处理请求\n   */\n  private async processRequest(req: Request, context: RequestContextV2): Promise<any> {\n    const body = req.body;\n\n    // 如果启用了Pipeline集成\n    if (this.pipelineEnabled) {\n      return await this.processWithPipeline(body, context);\n    }\n\n    // 否则返回模拟响应\n    return this.createMockResponse(body, context);\n  }\n\n  /**\n   * 创建模拟响应\n   */\n  private createMockResponse(body: any, context: RequestContextV2): any {\n    return {\n      id: `chatcmpl-${context.requestId}`,\n      object: 'chat.completion',\n      created: Math.floor(Date.now() / 1000),\n      model: body.model,\n      choices: [{\n        index: 0,\n        message: {\n          role: 'assistant',\n          content: `[V2 Mock Response] This is a placeholder response from RouteCodex Server V2. Request ID: ${context.requestId}. Model: ${body.model}. Messages: ${body.messages.length}.`\n        },\n        finish_reason: 'stop'\n      }],\n      usage: {\n        prompt_tokens: this.estimateTokens(body.messages),\n        completion_tokens: 50,\n        total_tokens: this.estimateTokens(body.messages) + 50\n      }\n    };\n  }\n\n  /**\n   * 使用Pipeline处理请求 (预留)\n   */\n  private async processWithPipeline(body: any, context: RequestContextV2): Promise<any> {\n    console.log(`[ChatCompletionsHandlerV2] Pipeline processing not yet implemented for ${context.requestId}`);\n\n    // TODO: 集成Pipeline系统\n    // 目前返回模拟响应\n    return this.createMockResponse(body, context);\n  }\n\n  /**\n   * 执行请求前Hooks (预留)\n   */\n  private async executePreRequestHooks(req: Request, context: RequestContextV2): Promise<void> {\n    console.log(`[ChatCompletionsHandlerV2] Executing pre-request hooks for ${context.requestId}`);\n\n    // TODO: 集成系统hooks\n    // 这里将调用系统hooks模块的request_preprocessing hooks\n  }\n\n  /**\n   * 执行响应后Hooks (预留)\n   */\n  private async executePostResponseHooks(response: any, context: RequestContextV2): Promise<void> {\n    console.log(`[ChatCompletionsHandlerV2] Executing post-response hooks for ${context.requestId}`);\n\n    // TODO: 集成系统hooks\n    // 这里将调用系统hooks模块的response_postprocessing hooks\n  }\n\n  /**\n   * 错误处理\n   */\n  private async handleError(error: Error, res: Response, context: RequestContextV2): Promise<void> {\n    console.error(`[ChatCompletionsHandlerV2] Error processing request ${context.requestId}:`, error);\n\n    // 错误hooks处理 (预留)\n    if (this.hooksEnabled) {\n      try {\n        await this.executeErrorHooks(error, context);\n      } catch (hookError) {\n        console.error(`[ChatCompletionsHandlerV2] Error in error hooks:`, hookError);\n      }\n    }\n\n    // 发送错误响应\n    try {\n      res.setHeader('x-request-id', context.requestId);\n      res.setHeader('x-server-version', 'v2');\n\n      if (error.message.includes('required')) {\n        res.status(400).json({\n          error: {\n            message: error.message,\n            type: 'validation_error',\n            code: 'validation_error'\n          }\n        });\n      } else {\n        res.status(500).json({\n          error: {\n            message: 'Internal Server Error',\n            type: 'internal_error',\n            code: 'internal_error'\n          }\n        });\n      }\n    } catch (responseError) {\n      console.error(`[ChatCompletionsHandlerV2] Failed to send error response:`, responseError);\n      res.status(500).send('Internal Server Error');\n    }\n  }\n\n  /**\n   * 执行错误Hooks (预留)\n   */\n  private async executeErrorHooks(error: Error, context: RequestContextV2): Promise<void> {\n    console.log(`[ChatCompletionsHandlerV2] Executing error hooks for ${context.requestId}`);\n\n    // TODO: 集成系统hooks\n    // 这里将调用系统hooks模块的error_handling hooks\n  }\n\n  /**\n   * 估算Token数量 (简单实现)\n   */\n  private estimateTokens(messages: any[]): number {\n    let totalTokens = 0;\n\n    for (const message of messages) {\n      if (typeof message.content === 'string') {\n        // 简单估算：4个字符 ≈ 1个token\n        totalTokens += Math.ceil(message.content.length / 4);\n      } else if (Array.isArray(message.content)) {\n        // 多模态内容\n        for (const content of message.content) {\n          if (content.type === 'text' && content.text) {\n            totalTokens += Math.ceil(content.text.length / 4);\n          }\n        }\n      }\n    }\n\n    return Math.max(totalTokens, 1); // 至少1个token\n  }\n}"
        },
        "src/server-v2/hooks/impl/server-performance-monitoring-hook.ts": {
          "path": "src/server-v2/hooks/impl/server-performance-monitoring-hook.ts",
          "size": 6049,
          "lines": 219,
          "imports": [
            "../../../types/common-types.js",
            "../server-v2-hook-integration.js"
          ],
          "exports": [
            "ServerPerformanceMonitoringHook"
          ],
          "classes": [
            "PerformanceMonitor",
            "ServerPerformanceMonitoringHook"
          ],
          "functions": [],
          "content": "/**\n * Server Performance Monitoring Hook\n *\n * 按照现有规范实现的性能监控Hook\n * 命名: server.03.performance-monitoring\n */\n\nimport type { UnknownObject } from '../../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookResult\n} from '../../../modules/hooks/types/hook-types.js';\nimport type { ServerV2HookContext } from '../server-v2-hook-integration.js';\n\n/**\n * 性能指标接口\n */\ninterface PerformanceMetrics {\n  requestId: string;\n  endpoint: string;\n  stage: string;\n  timestamp: number;\n  memoryUsage: NodeJS.MemoryUsage;\n  cpuUsage?: NodeJS.CpuUsage;\n  processingTime: number;\n  dataSize: number;\n}\n\n/**\n * 性能监控数据存储\n */\nclass PerformanceMonitor {\n  private metrics: Map<string, PerformanceMetrics[]> = new Map();\n  private maxEntries: number = 1000;\n\n  recordMetrics(metrics: PerformanceMetrics): void {\n    const key = `${metrics.requestId}_${metrics.endpoint}`;\n    const existing = this.metrics.get(key) || [];\n\n    existing.push(metrics);\n\n    // 保持最大条目数限制\n    if (existing.length > this.maxEntries) {\n      existing.splice(0, existing.length - this.maxEntries);\n    }\n\n    this.metrics.set(key, existing);\n  }\n\n  getMetrics(requestId: string, endpoint: string): PerformanceMetrics[] {\n    const key = `${requestId}_${endpoint}`;\n    return this.metrics.get(key) || [];\n  }\n\n  getAllMetrics(): PerformanceMetrics[] {\n    const allMetrics: PerformanceMetrics[] = [];\n    for (const metrics of this.metrics.values()) {\n      allMetrics.push(...metrics);\n    }\n    return allMetrics;\n  }\n\n  clearMetrics(): void {\n    this.metrics.clear();\n  }\n\n  getPerformanceSummary(): {\n    totalRequests: number;\n    avgProcessingTime: number;\n    avgMemoryUsage: number;\n    slowestRequest: { requestId: string; time: number } | null;\n  } {\n    const allMetrics = this.getAllMetrics();\n\n    if (allMetrics.length === 0) {\n      return {\n        totalRequests: 0,\n        avgProcessingTime: 0,\n        avgMemoryUsage: 0,\n        slowestRequest: null\n      };\n    }\n\n    const totalProcessingTime = allMetrics.reduce((sum, m) => sum + m.processingTime, 0);\n    const totalMemoryUsage = allMetrics.reduce((sum, m) => sum + m.memoryUsage.heapUsed, 0);\n    const slowest = allMetrics.reduce((max, current) =>\n      current.processingTime > max.processingTime ? current : max\n    , allMetrics[0]);\n\n    return {\n      totalRequests: new Set(allMetrics.map(m => m.requestId)).size,\n      avgProcessingTime: Math.round(totalProcessingTime / allMetrics.length * 100) / 100,\n      avgMemoryUsage: Math.round(totalMemoryUsage / allMetrics.length),\n      slowestRequest: {\n        requestId: slowest.requestId,\n        time: slowest.processingTime\n      }\n    };\n  }\n}\n\n// 全局性能监控实例\nconst performanceMonitor = new PerformanceMonitor();\n\n/**\n * Server性能监控Hook\n */\nexport class ServerPerformanceMonitoringHook implements IBidirectionalHook {\n  readonly name = 'server.03.performance-monitoring';\n  readonly stage: UnifiedHookStage = 'RESPONSE_VALIDATION';\n  readonly priority = 30;\n  readonly isDebugHook = true;\n\n  async execute(context: HookExecutionContext, data: { data: UnknownObject; metadata: UnknownObject }): Promise<HookResult> {\n    const serverContext = context as ServerV2HookContext;\n    const startTime = serverContext.metadata?.startTime as number || Date.now();\n    const processingTime = Date.now() - startTime;\n\n    // 计算数据大小\n    const dataSize = this.calculateDataSize(data.data);\n\n    // 记录性能指标\n    const metrics: PerformanceMetrics = {\n      requestId: serverContext.requestId,\n      endpoint: serverContext.endpoint,\n      stage: this.stage,\n      timestamp: Date.now(),\n      memoryUsage: process.memoryUsage(),\n      cpuUsage: process.cpuUsage(),\n      processingTime,\n      dataSize\n    };\n\n    performanceMonitor.recordMetrics(metrics);\n\n    console.log(`[ServerPerformanceMonitoringHook] Performance metrics for ${serverContext.requestId}:`, {\n      processingTime: `${processingTime}ms`,\n      memoryUsage: `${Math.round(metrics.memoryUsage.heapUsed / 1024 / 1024)}MB`,\n      dataSize: `${Math.round(dataSize / 1024)}KB`\n    });\n\n    // 检查性能警告\n    const warnings = this.checkPerformanceWarnings(metrics);\n    if (warnings.length > 0) {\n      console.warn(`[ServerPerformanceMonitoringHook] Performance warnings for ${serverContext.requestId}:`, warnings);\n    }\n\n    // 在元数据中添加性能信息\n    const enrichedMetadata = {\n      ...data.metadata,\n      performance: {\n        processingTime,\n        memoryUsage: metrics.memoryUsage,\n        dataSize,\n        warnings\n      }\n    };\n\n    return {\n      success: true,\n      data: data.data, // 不修改原始数据\n      metadata: enrichedMetadata\n    };\n  }\n\n  /**\n   * 计算数据大小\n   */\n  private calculateDataSize(data: UnknownObject): number {\n    try {\n      const jsonString = JSON.stringify(data);\n      return Buffer.byteLength(jsonString, 'utf-8');\n    } catch {\n      return 0;\n    }\n  }\n\n  /**\n   * 检查性能警告\n   */\n  private checkPerformanceWarnings(metrics: PerformanceMetrics): string[] {\n    const warnings: string[] = [];\n\n    // 处理时间警告\n    if (metrics.processingTime > 5000) { // 5秒\n      warnings.push(`Slow processing: ${metrics.processingTime}ms`);\n    }\n\n    // 内存使用警告\n    const memoryUsageMB = metrics.memoryUsage.heapUsed / 1024 / 1024;\n    if (memoryUsageMB > 500) { // 500MB\n      warnings.push(`High memory usage: ${Math.round(memoryUsageMB)}MB`);\n    }\n\n    // 数据大小警告\n    const dataSizeKB = metrics.dataSize / 1024;\n    if (dataSizeKB > 1024) { // 1MB\n      warnings.push(`Large data size: ${Math.round(dataSizeKB)}KB`);\n    }\n\n    return warnings;\n  }\n\n  /**\n   * 获取性能监控器实例\n   */\n  static getPerformanceMonitor(): PerformanceMonitor {\n    return performanceMonitor;\n  }\n\n  /**\n   * 获取性能统计\n   */\n  static getPerformanceStats() {\n    return performanceMonitor.getPerformanceSummary();\n  }\n}"
        },
        "src/server-v2/hooks/impl/server-request-logging-hook.ts": {
          "path": "src/server-v2/hooks/impl/server-request-logging-hook.ts",
          "size": 1804,
          "lines": 58,
          "imports": [
            "../../../types/common-types.js",
            "../server-v2-hook-integration.js"
          ],
          "exports": [
            "ServerRequestLoggingHook"
          ],
          "classes": [
            "ServerRequestLoggingHook"
          ],
          "functions": [],
          "content": "/**\n * Server Request Logging Hook\n *\n * 按照现有规范实现的请求日志Hook\n * 命名: server.01.request-logging\n */\n\nimport type { UnknownObject } from '../../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookResult\n} from '../../../modules/hooks/types/hook-types.js';\nimport type { ServerV2HookContext } from '../server-v2-hook-integration.js';\n\n/**\n * Server请求日志Hook\n */\nexport class ServerRequestLoggingHook implements IBidirectionalHook {\n  readonly name = 'server.01.request-logging';\n  readonly stage: UnifiedHookStage = 'PIPELINE_PREPROCESSING';\n  readonly priority = 10;\n  readonly isDebugHook = true;\n\n  async execute(context: HookExecutionContext, data: { data: UnknownObject; metadata: UnknownObject }): Promise<HookResult> {\n    const serverContext = context as ServerV2HookContext;\n    const requestData = data.data;\n\n    console.log(`[ServerRequestLoggingHook] Processing request:`, {\n      requestId: serverContext.requestId,\n      endpoint: serverContext.endpoint,\n      method: (requestData as any)?.method,\n      url: (requestData as any)?.url,\n      userAgent: (requestData as any)?.headers?.['user-agent'],\n      timestamp: new Date().toISOString()\n    });\n\n    // 记录请求基本信息到元数据\n    const enrichedMetadata = {\n      ...data.metadata,\n      requestInfo: {\n        requestId: serverContext.requestId,\n        endpoint: serverContext.endpoint,\n        method: (requestData as any)?.method || 'unknown',\n        url: (requestData as any)?.url || 'unknown',\n        timestamp: Date.now(),\n        serverVersion: serverContext.serverVersion\n      }\n    };\n\n    return {\n      success: true,\n      data: requestData, // 不修改原始数据\n      metadata: enrichedMetadata\n    };\n  }\n}"
        },
        "src/server-v2/hooks/impl/server-response-enhancement-hook.ts": {
          "path": "src/server-v2/hooks/impl/server-response-enhancement-hook.ts",
          "size": 2170,
          "lines": 71,
          "imports": [
            "../../../types/common-types.js",
            "../server-v2-hook-integration.js"
          ],
          "exports": [
            "ServerResponseEnhancementHook"
          ],
          "classes": [
            "ServerResponseEnhancementHook"
          ],
          "functions": [],
          "content": "/**\n * Server Response Enhancement Hook\n *\n * 按照现有规范实现的响应增强Hook\n * 命名: server.02.response-enhancement\n */\n\nimport type { UnknownObject } from '../../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookResult\n} from '../../../modules/hooks/types/hook-types.js';\nimport type { ServerV2HookContext } from '../server-v2-hook-integration.js';\n\n/**\n * Server响应增强Hook\n */\nexport class ServerResponseEnhancementHook implements IBidirectionalHook {\n  readonly name = 'server.02.response-enhancement';\n  readonly stage: UnifiedHookStage = 'RESPONSE_POSTPROCESSING';\n  readonly priority = 20;\n  readonly isDebugHook = false;\n\n  async execute(context: HookExecutionContext, data: { data: UnknownObject; metadata: UnknownObject }): Promise<HookResult> {\n    const serverContext = context as ServerV2HookContext;\n    const responseData = data.data;\n\n    // 增强响应数据\n    if (responseData && typeof responseData === 'object') {\n      const enhancedResponse = { ...responseData };\n\n      // 添加服务器版本信息\n      (enhancedResponse as any).serverInfo = {\n        version: serverContext.serverVersion,\n        requestId: serverContext.requestId,\n        processedAt: new Date().toISOString(),\n        processingTime: Date.now() - (serverContext.metadata?.startTime as number || Date.now())\n      };\n\n      // 添加Hook执行统计（如果可用）\n      if (serverContext.metadata?.hookStats) {\n        (enhancedResponse as any).hookStats = serverContext.metadata.hookStats;\n      }\n\n      console.log(`[ServerResponseEnhancementHook] Enhanced response for request: ${serverContext.requestId}`);\n\n      return {\n        success: true,\n        data: enhancedResponse,\n        metadata: {\n          ...data.metadata,\n          responseEnhanced: true,\n          enhancementTime: Date.now()\n        }\n      };\n    }\n\n    // 如果不是对象类型，直接返回\n    return {\n      success: true,\n      data: responseData,\n      metadata: {\n        ...data.metadata,\n        responseEnhanced: false,\n        reason: 'Response data is not an object'\n      }\n    };\n  }\n}"
        },
        "src/server-v2/hooks/server-hook-manager.ts": {
          "path": "src/server-v2/hooks/server-hook-manager.ts",
          "size": 9998,
          "lines": 375,
          "imports": [
            "../../types/common-types.js",
            "../core/route-codex-server-v2.js"
          ],
          "exports": [
            "HookExecutionContext",
            "HookExecutionResult",
            "ServerHookManager"
          ],
          "classes": [
            "ServerHookManager"
          ],
          "functions": [],
          "content": "/**\n * Server Hook Manager V2\n *\n * 集成系统hooks模块的Hook管理器\n * 为Server V2提供统一的Hook执行接口\n */\n\nimport type { UnknownObject } from '../../types/common-types.js';\nimport type { RequestContextV2 } from '../core/route-codex-server-v2.js';\n\n/**\n * Hook执行上下文\n */\nexport interface HookExecutionContext {\n  requestId: string;\n  stage: string;\n  timestamp: number;\n  data: UnknownObject;\n}\n\n/**\n * Hook执行结果\n */\nexport interface HookExecutionResult {\n  success: boolean;\n  data?: UnknownObject;\n  error?: string;\n  executionTime: number;\n  observations?: string[];\n}\n\n/**\n * Server Hook管理器\n */\nexport class ServerHookManager {\n  private enabled: boolean;\n  private hooks: Map<string, Function[]> = new Map();\n  private executionStats: Map<string, { count: number; totalTime: number; errors: number }> = new Map();\n\n  constructor(enabled: boolean = true) {\n    this.enabled = enabled;\n    console.log(`[ServerHookManager] Initialized (enabled: ${enabled})`);\n\n    if (enabled) {\n      this.initializeDefaultHooks();\n    }\n  }\n\n  /**\n   * 初始化默认Hooks\n   */\n  private initializeDefaultHooks(): void {\n    console.log('[ServerHookManager] Initializing default hooks');\n\n    // 请求预处理Hooks\n    this.registerHook('request_preprocessing', this.createRequestLoggingHook());\n    this.registerHook('request_preprocessing', this.createRequestValidationHook());\n\n    // 响应后处理Hooks\n    this.registerHook('response_postprocessing', this.createResponseLoggingHook());\n    this.registerHook('response_postprocessing', this.createResponseMetricsHook());\n\n    // 错误处理Hooks\n    this.registerHook('error_handling', this.createErrorLoggingHook());\n    this.registerHook('error_handling', this.createErrorMetricsHook());\n  }\n\n  /**\n   * 注册Hook\n   */\n  registerHook(stage: string, hook: Function): void {\n    if (!this.enabled) {\n      console.warn(`[ServerHookManager] Hooks are disabled, ignoring registration for stage: ${stage}`);\n      return;\n    }\n\n    if (!this.hooks.has(stage)) {\n      this.hooks.set(stage, []);\n    }\n\n    this.hooks.get(stage)!.push(hook);\n    console.log(`[ServerHookManager] Registered hook for stage: ${stage}`);\n  }\n\n  /**\n   * 执行Hooks\n   */\n  async executeHooks(\n    stage: string,\n    data: UnknownObject,\n    context: RequestContextV2\n  ): Promise<HookExecutionResult> {\n    if (!this.enabled) {\n      return {\n        success: true,\n        data,\n        executionTime: 0,\n        observations: ['Hooks are disabled']\n      };\n    }\n\n    const startTime = Date.now();\n    const hooks = this.hooks.get(stage) || [];\n\n    if (hooks.length === 0) {\n      return {\n        success: true,\n        data,\n        executionTime: Date.now() - startTime,\n        observations: ['No hooks registered for stage: ' + stage]\n      };\n    }\n\n    console.log(`[ServerHookManager] Executing ${hooks.length} hooks for stage: ${stage}, request: ${context.requestId}`);\n\n    try {\n      let currentData = data;\n      const observations: string[] = [];\n\n      // 按顺序执行所有hooks\n      for (let i = 0; i < hooks.length; i++) {\n        const hook = hooks[i];\n        const hookName = `${stage}_hook_${i + 1}`;\n\n        try {\n          const hookResult = await hook(currentData, context);\n\n          if (hookResult && typeof hookResult === 'object') {\n            currentData = hookResult;\n            observations.push(`Hook ${hookName} executed successfully`);\n          } else {\n            observations.push(`Hook ${hookName} returned no data`);\n          }\n\n          // 更新统计\n          this.updateStats(stage, Date.now() - startTime, false);\n\n        } catch (hookError) {\n          const error = hookError as Error;\n          const errorMessage = `Hook ${hookName} failed: ${error.message}`;\n          console.error(`[ServerHookManager] ${errorMessage}`);\n          observations.push(errorMessage);\n\n          // 更新错误统计\n          this.updateStats(stage, Date.now() - startTime, true);\n\n          // 根据配置决定是否继续执行\n          if (this.shouldStopOnHookError(stage, error)) {\n            throw new Error(`Hook execution stopped at ${hookName}: ${error.message}`);\n          }\n        }\n      }\n\n      const executionTime = Date.now() - startTime;\n\n      return {\n        success: true,\n        data: currentData,\n        executionTime,\n        observations\n      };\n\n    } catch (error) {\n      const executionTime = Date.now() - startTime;\n      const err = error as Error;\n      this.updateStats(stage, executionTime, true);\n\n      return {\n        success: false,\n        error: err.message,\n        executionTime,\n        observations: [`Hook chain failed: ${err.message}`]\n      };\n    }\n  }\n\n  /**\n   * 创建请求日志Hook\n   */\n  private createRequestLoggingHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      const logData = {\n        requestId: context.requestId,\n        method: context.method,\n        url: context.url,\n        endpoint: context.endpoint,\n        timestamp: context.timestamp,\n        userAgent: context.userAgent,\n        ip: context.ip\n      };\n\n      console.log(`[Hook:RequestLogging] ${JSON.stringify(logData, null, 2)}`);\n      return data;\n    };\n  }\n\n  /**\n   * 创建请求验证Hook\n   */\n  private createRequestValidationHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      // 基础验证\n      if (!data || typeof data !== 'object') {\n        throw new Error('Request data must be an object');\n      }\n\n      const body = data as any;\n\n      // 检查必要字段\n      if (!body.model) {\n        console.warn(`[Hook:RequestValidation] Missing model field for request ${context.requestId}`);\n      }\n\n      if (!body.messages || !Array.isArray(body.messages)) {\n        throw new Error('Messages field must be an array');\n      }\n\n      console.log(`[Hook:RequestValidation] Request validation passed for ${context.requestId}`);\n      return data;\n    };\n  }\n\n  /**\n   * 创建响应日志Hook\n   */\n  private createResponseLoggingHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      const logData = {\n        requestId: context.requestId,\n        hasResponse: !!data,\n        responseModel: (data as any)?.model,\n        responseChoices: (data as any)?.choices?.length || 0,\n        timestamp: Date.now()\n      };\n\n      console.log(`[Hook:ResponseLogging] ${JSON.stringify(logData, null, 2)}`);\n      return data;\n    };\n  }\n\n  /**\n   * 创建响应指标Hook\n   */\n  private createResponseMetricsHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      const response = data as any;\n\n      // 添加响应指标\n      if (response && typeof response === 'object') {\n        response._metrics = {\n          serverVersion: 'v2',\n          processedAt: Date.now(),\n          requestId: context.requestId,\n          hookProcessed: true\n        };\n      }\n\n      return data;\n    };\n  }\n\n  /**\n   * 创建错误日志Hook\n   */\n  private createErrorLoggingHook(): Function {\n    return async (error: Error, context: RequestContextV2): Promise<void> => {\n      const errorData = {\n        requestId: context.requestId,\n        error: {\n          message: error.message,\n          name: error.name,\n          stack: error.stack\n        },\n        context: {\n          method: context.method,\n          url: context.url,\n          endpoint: context.endpoint,\n          timestamp: context.timestamp\n        }\n      };\n\n      console.error(`[Hook:ErrorLogging] ${JSON.stringify(errorData, null, 2)}`);\n    };\n  }\n\n  /**\n   * 创建错误指标Hook\n   */\n  private createErrorMetricsHook(): Function {\n    return async (error: Error, context: RequestContextV2): Promise<void> => {\n      // 这里可以发送错误指标到监控系统\n      console.log(`[Hook:ErrorMetrics] Error recorded for request ${context.requestId}: ${error.message}`);\n    };\n  }\n\n  /**\n   * 更新Hook执行统计\n   */\n  private updateStats(stage: string, executionTime: number, isError: boolean): void {\n    if (!this.executionStats.has(stage)) {\n      this.executionStats.set(stage, { count: 0, totalTime: 0, errors: 0 });\n    }\n\n    const stats = this.executionStats.get(stage)!;\n    stats.count++;\n    stats.totalTime += executionTime;\n\n    if (isError) {\n      stats.errors++;\n    }\n  }\n\n  /**\n   * 判断是否应该在Hook错误时停止执行\n   */\n  private shouldStopOnHookError(stage: string, error: Error): boolean {\n    // 对于关键阶段，错误时停止执行\n    const criticalStages = ['request_validation', 'authentication'];\n    return criticalStages.includes(stage);\n  }\n\n  /**\n   * 获取Hook执行统计\n   */\n  getExecutionStats(): UnknownObject {\n    const stats: UnknownObject = {};\n\n    for (const [stage, data] of this.executionStats.entries()) {\n      stats[stage] = {\n        executions: data.count,\n        totalTime: data.totalTime,\n        averageTime: data.count > 0 ? data.totalTime / data.count : 0,\n        errors: data.errors,\n        errorRate: data.count > 0 ? data.errors / data.count : 0\n      };\n    }\n\n    return stats;\n  }\n\n  /**\n   * 清理Hook\n   */\n  clearHooks(stage?: string): void {\n    if (stage) {\n      this.hooks.delete(stage);\n      console.log(`[ServerHookManager] Cleared hooks for stage: ${stage}`);\n    } else {\n      this.hooks.clear();\n      console.log('[ServerHookManager] Cleared all hooks');\n    }\n  }\n\n  /**\n   * 获取已注册的Hook阶段\n   */\n  getRegisteredStages(): string[] {\n    return Array.from(this.hooks.keys());\n  }\n\n  /**\n   * 启用/禁用Hook管理器\n   */\n  setEnabled(enabled: boolean): void {\n    this.enabled = enabled;\n    console.log(`[ServerHookManager] ${enabled ? 'Enabled' : 'Disabled'}`);\n  }\n\n  /**\n   * 检查是否启用\n   */\n  isEnabled(): boolean {\n    return this.enabled;\n  }\n}"
        },
        "src/server-v2/hooks/server-v2-hook-integration.ts": {
          "path": "src/server-v2/hooks/server-v2-hook-integration.ts",
          "size": 12460,
          "lines": 448,
          "imports": [
            "../../types/common-types.js",
            "../utils/server-v2-snapshot-writer.js"
          ],
          "exports": [
            "ServerV2HookContext",
            "ServerV2HookIntegrationConfig",
            "ServerV2HookIntegration"
          ],
          "classes": [
            "ServerV2HookIntegration"
          ],
          "functions": [],
          "content": "/**\n * Server V2 Hook Integration\n *\n * 按照现有的provider和llmswitch-core规范实现\n * 集成系统hooks模块和snapshot服务\n */\n\nimport type { UnknownObject } from '../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookExecutionResult,\n  HookResult\n} from '../../modules/hooks/types/hook-types.js';\nimport { writeServerV2Snapshot, type ServerV2SnapshotPhase } from '../utils/server-v2-snapshot-writer.js';\n\n/**\n * Server V2 Hook阶段映射\n */\nconst SERVER_V2_HOOK_STAGE_MAP: Record<string, UnifiedHookStage> = {\n  'server-entry': 'PIPELINE_PREPROCESSING',\n  'server-pre-process': 'REQUEST_PREPROCESSING',\n  'server-post-process': 'RESPONSE_POSTPROCESSING',\n  'server-response': 'RESPONSE_VALIDATION',\n  'server-error': 'ERROR_HANDLING',\n  'server-final': 'FINALIZATION'\n} as const;\n\n/**\n * Server V2 Hook执行上下文\n */\nexport interface ServerV2HookContext extends HookExecutionContext {\n  serverVersion: 'v2';\n  endpoint: string;\n  requestId: string;\n  originalRequest?: UnknownObject;\n  metadata?: {\n    [key: string]: unknown;\n  };\n}\n\n/**\n * Server V2 Hook集成配置\n */\nexport interface ServerV2HookIntegrationConfig {\n  enabled: boolean;\n  snapshot: {\n    enabled: boolean;\n    level: 'verbose' | 'normal' | 'silent';\n    phases: ServerV2SnapshotPhase[];\n  };\n  hooks: {\n    enabled: boolean;\n    timeout: number;\n    parallel: boolean;\n    retryAttempts: number;\n  };\n}\n\n/**\n * 默认配置\n */\nconst DEFAULT_CONFIG: ServerV2HookIntegrationConfig = {\n  enabled: true,\n  snapshot: {\n    enabled: true,\n    level: 'normal',\n    phases: ['server-entry', 'server-pre-process', 'server-post-process', 'server-response', 'server-final']\n  },\n  hooks: {\n    enabled: true,\n    timeout: 5000,\n    parallel: false,\n    retryAttempts: 2\n  }\n};\n\n/**\n * Server V2 Hook集成管理器\n */\nexport class ServerV2HookIntegration {\n  private config: ServerV2HookIntegrationConfig;\n  private hooks: Map<string, IBidirectionalHook> = new Map();\n  private hookStats: Map<string, { executions: number; errors: number; totalTime: number }> = new Map();\n\n  constructor(config: Partial<ServerV2HookIntegrationConfig> = {}) {\n    this.config = { ...DEFAULT_CONFIG, ...config };\n    console.log('[ServerV2HookIntegration] Initialized with config:', {\n      enabled: this.config.enabled,\n      snapshotEnabled: this.config.snapshot.enabled,\n      hooksEnabled: this.config.hooks.enabled,\n      level: this.config.snapshot.level\n    });\n  }\n\n  /**\n   * 注册Hook\n   */\n  registerHook(hook: IBidirectionalHook): void {\n    if (!this.config.hooks.enabled) {\n      console.warn('[ServerV2HookIntegration] Hooks are disabled, skipping registration');\n      return;\n    }\n\n    const hookKey = `${hook.stage}_${hook.name}`;\n    this.hooks.set(hookKey, hook);\n    this.hookStats.set(hookKey, { executions: 0, errors: 0, totalTime: 0 });\n\n    console.log(`[ServerV2HookIntegration] Registered hook: ${hook.name} at stage ${hook.stage}`);\n  }\n\n  /**\n   * 注销Hook\n   */\n  unregisterHook(stage: UnifiedHookStage, name: string): void {\n    const hookKey = `${stage}_${name}`;\n    const removed = this.hooks.delete(hookKey);\n    this.hookStats.delete(hookKey);\n\n    if (removed) {\n      console.log(`[ServerV2HookIntegration] Unregistered hook: ${name} at stage ${stage}`);\n    }\n  }\n\n  /**\n   * 执行指定阶段的Hook\n   */\n  async executeHooks(\n    stage: UnifiedHookStage,\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<{\n    data: UnknownObject;\n    results: HookExecutionResult[];\n    executionTime: number;\n  }> {\n    if (!this.config.enabled || !this.config.hooks.enabled) {\n      return {\n        data,\n        results: [],\n        executionTime: 0\n      };\n    }\n\n    const startTime = Date.now();\n    const results: HookExecutionResult[] = [];\n    let currentData = data;\n\n    try {\n      // 查找匹配的Hook\n      const stageHooks = Array.from(this.hooks.entries())\n        .filter(([_, hook]) => hook.stage === stage)\n        .sort(([_, a], [__, b]) => a.priority - b.priority);\n\n      console.log(`[ServerV2HookIntegration] Executing ${stageHooks.length} hooks for stage: ${stage}`);\n\n      // 执行Hook\n      for (const [hookKey, hook] of stageHooks) {\n        const hookStartTime = Date.now();\n        const stats = this.hookStats.get(hookKey)!;\n\n        try {\n          stats.executions++;\n\n          // 执行Hook\n          const result = await this.executeSingleHook(hook, currentData, context);\n\n          if (result.success && result.data !== undefined) {\n            currentData = result.data;\n          }\n\n          const hookExecutionTime = Date.now() - hookStartTime;\n          stats.totalTime += hookExecutionTime;\n\n          results.push({\n            hookName: hook.name,\n            stage: hook.stage,\n            success: true,\n            executionTime: hookExecutionTime,\n            input: data,\n            output: result.data,\n            metadata: result.metadata\n          });\n\n          console.log(`[ServerV2HookIntegration] Hook ${hook.name} completed in ${hookExecutionTime}ms`);\n\n        } catch (error) {\n          stats.errors++;\n          const hookExecutionTime = Date.now() - hookStartTime;\n          stats.totalTime += hookExecutionTime;\n\n          const errorResult = {\n            hookName: hook.name,\n            stage: hook.stage,\n            success: false,\n            executionTime: hookExecutionTime,\n            error: (error as Error).message,\n            input: currentData,\n            output: null\n          };\n\n          results.push(errorResult);\n\n          console.error(`[ServerV2HookIntegration] Hook ${hook.name} failed:`, error);\n\n          // 根据配置决定是否继续执行\n          if (this.shouldStopOnHookError(hook, error as Error)) {\n            throw new Error(`Hook execution stopped at ${hook.name}: ${(error as Error).message}`);\n          }\n        }\n      }\n\n      const totalExecutionTime = Date.now() - startTime;\n\n      return {\n        data: currentData,\n        results,\n        executionTime: totalExecutionTime\n      };\n\n    } catch (error) {\n      const totalExecutionTime = Date.now() - startTime;\n\n      return {\n        data: currentData,\n        results,\n        executionTime: totalExecutionTime\n      };\n    }\n  }\n\n  /**\n   * 执行单个Hook\n   */\n  private async executeSingleHook(\n    hook: IBidirectionalHook,\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<HookResult> {\n    const timeoutPromise = new Promise<never>((_, reject) => {\n      setTimeout(() => reject(new Error(`Hook ${hook.name} timed out after ${this.config.hooks.timeout}ms`)), this.config.hooks.timeout);\n    });\n\n    const hookPromise = hook.execute(context, { data, metadata: {} });\n\n    return Promise.race([hookPromise, timeoutPromise]);\n  }\n\n  /**\n   * 执行Hook并记录快照\n   */\n  async executeHooksWithSnapshot(\n    phase: ServerV2SnapshotPhase,\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<{\n    data: UnknownObject;\n    results: HookExecutionResult[];\n    executionTime: number;\n  }> {\n    const stage = SERVER_V2_HOOK_STAGE_MAP[phase];\n\n    // 执行Hook\n    const hookResult = await this.executeHooks(stage, data, context);\n\n    // 记录快照\n    if (this.config.snapshot.enabled && this.config.snapshot.phases.includes(phase)) {\n      await this.writeSnapshot(phase, data, hookResult, context);\n    }\n\n    return hookResult;\n  }\n\n  /**\n   * 执行错误Hook并记录快照\n   */\n  async executeErrorHooksWithSnapshot(\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<void> {\n    const stage = 'ERROR_HANDLING' as UnifiedHookStage;\n\n    try {\n      // 执行错误处理Hook\n      const hookResult = await this.executeHooks(stage, data, context);\n\n      // 记录错误快照\n      if (this.config.snapshot.enabled) {\n        await this.writeErrorSnapshot(data, hookResult, context);\n      }\n    } catch (error) {\n      console.error('[ServerV2HookIntegration] Error in error hooks:', error);\n    }\n  }\n\n  /**\n   * 写入快照\n   */\n  private async writeSnapshot(\n    phase: ServerV2SnapshotPhase,\n    data: UnknownObject,\n    hookResult: { data: UnknownObject; results: HookExecutionResult[]; executionTime: number },\n    context: ServerV2HookContext\n  ): Promise<void> {\n    try {\n      const snapshotData = {\n        originalData: data,\n        processedData: hookResult.data,\n        hookResults: hookResult.results,\n        hookExecutionTime: hookResult.executionTime,\n        context: {\n          requestId: context.requestId,\n          endpoint: context.endpoint,\n          serverVersion: context.serverVersion\n        }\n      };\n\n      await writeServerV2Snapshot({\n        phase,\n        requestId: context.requestId,\n        data: snapshotData,\n        entryEndpoint: context.endpoint,\n        metadata: {\n          level: this.config.snapshot.level,\n          hookCount: hookResult.results.length,\n          successCount: hookResult.results.filter(r => r.success).length,\n          errorCount: hookResult.results.filter(r => !r.success).length\n        }\n      });\n\n    } catch (error) {\n      console.error(`[ServerV2HookIntegration] Failed to write snapshot for phase ${phase}:`, error);\n    }\n  }\n\n  /**\n   * 写入错误快照\n   */\n  private async writeErrorSnapshot(\n    data: UnknownObject,\n    hookResult: { data: UnknownObject; results: HookExecutionResult[]; executionTime: number },\n    context: ServerV2HookContext\n  ): Promise<void> {\n    try {\n      const snapshotData = {\n        originalData: data,\n        processedData: hookResult.data,\n        hookResults: hookResult.results,\n        hookExecutionTime: hookResult.executionTime,\n        context: {\n          requestId: context.requestId,\n          endpoint: context.endpoint,\n          serverVersion: context.serverVersion\n        }\n      };\n\n      await writeServerV2Snapshot({\n        phase: 'server-entry' as ServerV2SnapshotPhase, // 使用一个有效的phase\n        requestId: context.requestId,\n        data: snapshotData,\n        entryEndpoint: context.endpoint,\n        metadata: {\n          level: this.config.snapshot.level,\n          hookCount: hookResult.results.length,\n          successCount: hookResult.results.filter(r => r.success).length,\n          errorCount: hookResult.results.filter(r => !r.success).length,\n          isErrorSnapshot: true\n        }\n      });\n\n    } catch (error) {\n      console.error('[ServerV2HookIntegration] Failed to write error snapshot:', error);\n    }\n  }\n\n  /**\n   * 判断是否应该在Hook错误时停止执行\n   */\n  private shouldStopOnHookError(hook: IBidirectionalHook, error: Error): boolean {\n    // 对于debug hook，不停止执行\n    if (hook.isDebugHook) {\n      return false;\n    }\n\n    // 对于关键阶段，停止执行\n    const criticalStages: UnifiedHookStage[] = [\n      'REQUEST_VALIDATION',\n      'AUTHENTICATION',\n      'HTTP_REQUEST'\n    ];\n\n    return criticalStages.includes(hook.stage);\n  }\n\n  /**\n   * 获取Hook统计信息\n   */\n  getHookStats(): Record<string, { executions: number; errors: number; avgTime: number; successRate: number }> {\n    const stats: Record<string, any> = {};\n\n    for (const [hookKey, stat] of this.hookStats.entries()) {\n      const successRate = stat.executions > 0 ? ((stat.executions - stat.errors) / stat.executions) * 100 : 0;\n      const avgTime = stat.executions > 0 ? stat.totalTime / stat.executions : 0;\n\n      stats[hookKey] = {\n        executions: stat.executions,\n        errors: stat.errors,\n        avgTime: Math.round(avgTime * 100) / 100,\n        successRate: Math.round(successRate * 100) / 100\n      };\n    }\n\n    return stats;\n  }\n\n  /**\n   * 重置Hook统计信息\n   */\n  resetHookStats(): void {\n    for (const stats of this.hookStats.values()) {\n      stats.executions = 0;\n      stats.errors = 0;\n      stats.totalTime = 0;\n    }\n\n    console.log('[ServerV2HookIntegration] Hook statistics reset');\n  }\n\n  /**\n   * 获取配置信息\n   */\n  getConfig(): ServerV2HookIntegrationConfig {\n    return { ...this.config };\n  }\n\n  /**\n   * 更新配置\n   */\n  updateConfig(newConfig: Partial<ServerV2HookIntegrationConfig>): void {\n    this.config = { ...this.config, ...newConfig };\n    console.log('[ServerV2HookIntegration] Configuration updated:', {\n      enabled: this.config.enabled,\n      snapshotEnabled: this.config.snapshot.enabled,\n      hooksEnabled: this.config.hooks.enabled\n    });\n  }\n}"
        },
        "src/server-v2/utils/server-v2-snapshot-writer.ts": {
          "path": "src/server-v2/utils/server-v2-snapshot-writer.ts",
          "size": 5542,
          "lines": 201,
          "imports": [
            "fs",
            "path",
            "os",
            "../../types/common-types.js"
          ],
          "exports": [
            "ServerV2SnapshotOptions"
          ],
          "classes": [],
          "functions": [
            "mapEndpointToFolder",
            "maskSensitiveData",
            "generateSnapshotPath",
            "writeServerV2Snapshot",
            "writeBatchServerV2Snapshots",
            "cleanupExpiredSnapshots"
          ],
          "content": "/**\n * Server V2 Snapshot Writer\n *\n * 按照现有provider和llmswitch-core的snapshot规范实现\n * 支持端点映射和敏感数据遮蔽\n */\n\nimport { promises as fs } from 'fs';\nimport { join, dirname } from 'path';\nimport { homedir } from 'os';\nimport type { UnknownObject } from '../../types/common-types.js';\n\n/**\n * Server V2 Snapshot阶段定义\n */\nexport type ServerV2SnapshotPhase =\n  | 'server-entry'           // 请求进入服务器\n  | 'server-pre-process'     // 服务器预处理前\n  | 'server-post-process'    // 服务器处理后\n  | 'server-response'        // 服务器响应前\n  | 'server-error'           // 服务器错误\n  | 'server-final';          // 最终响应\n\n/**\n * Snapshot写入选项\n */\nexport interface ServerV2SnapshotOptions {\n  phase: ServerV2SnapshotPhase;\n  requestId: string;\n  data: UnknownObject;\n  entryEndpoint?: string;\n  error?: Error;\n  metadata?: {\n    [key: string]: unknown;\n  };\n}\n\n/**\n * 根据端点映射到文件夹\n */\nfunction mapEndpointToFolder(entryEndpoint?: string): string {\n  const ep = String(entryEndpoint || '').toLowerCase();\n  if (ep.includes('/v1/responses')) return 'openai-responses';\n  if (ep.includes('/v1/messages') || ep.includes('/anthropic')) return 'anthropic-messages';\n  if (ep.includes('/v2/chat/completions')) return 'server-v2-chat';\n  return 'openai-chat'; // 默认\n}\n\n/**\n * 敏感数据遮蔽\n */\nfunction maskSensitiveData(data: UnknownObject): UnknownObject {\n  if (typeof data !== 'object' || data === null) {\n    return data;\n  }\n\n  const result = { ...data };\n\n  // 遮蔽常见的敏感字段\n  const sensitiveFields = ['authorization', 'x-api-key', 'api-key', 'password', 'token'];\n\n  if (result.headers && typeof result.headers === 'object') {\n    const headers = { ...result.headers } as Record<string, unknown>;\n    for (const [key, value] of Object.entries(headers)) {\n      const lowerKey = key.toLowerCase();\n      if (sensitiveFields.some(field => lowerKey.includes(field))) {\n        const str = String(value ?? '');\n        const masked = str.length > 12 ? `${str.slice(0, 6)}****${str.slice(-6)}` : '****';\n        headers[key] = masked;\n      }\n    }\n    result.headers = headers;\n  }\n\n  return result;\n}\n\n/**\n * 生成快照文件路径\n */\nfunction generateSnapshotPath(\n  phase: ServerV2SnapshotPhase,\n  requestId: string,\n  entryEndpoint?: string\n): string {\n  const baseDir = join(homedir(), '.routecodex', 'codex-samples');\n  const endpointFolder = mapEndpointToFolder(entryEndpoint);\n  const fileName = `${requestId}_server-v2-${phase}.json`;\n\n  return join(baseDir, endpointFolder, fileName);\n}\n\n/**\n * 写入Server V2快照\n */\nexport async function writeServerV2Snapshot(options: ServerV2SnapshotOptions): Promise<void> {\n  try {\n    const { phase, requestId, data, entryEndpoint, error, metadata } = options;\n\n    // 创建快照数据结构\n    const snapshotData = {\n      timestamp: new Date().toISOString(),\n      phase,\n      requestId,\n      endpoint: entryEndpoint || 'unknown',\n      serverVersion: 'v2',\n      data: maskSensitiveData(data),\n      metadata: {\n        ...metadata,\n        phase,\n        writtenAt: Date.now(),\n        ...(error && {\n          error: {\n            name: error.name,\n            message: error.message,\n            stack: error.stack\n          }\n        })\n      }\n    };\n\n    // 错误信息已经在metadata中处理了\n\n    // 生成文件路径\n    const filePath = generateSnapshotPath(phase, requestId, entryEndpoint);\n\n    // 确保目录存在\n    await fs.mkdir(dirname(filePath), { recursive: true });\n\n    // 写入快照文件\n    await fs.writeFile(filePath, JSON.stringify(snapshotData, null, 2), 'utf-8');\n\n    console.log(`[ServerV2Snapshot] Snapshot written: ${filePath}`);\n\n  } catch (writeError) {\n    console.error('[ServerV2Snapshot] Failed to write snapshot:', writeError);\n    // 快照写入失败不应影响主流程\n  }\n}\n\n/**\n * 批量写入快照（用于多个阶段的快照）\n */\nexport async function writeBatchServerV2Snapshots(\n  requestId: string,\n  snapshots: Array<Omit<ServerV2SnapshotOptions, 'requestId'>>\n): Promise<void> {\n  const writePromises = snapshots.map(snapshot =>\n    writeServerV2Snapshot({\n      ...snapshot,\n      requestId\n    })\n  );\n\n  await Promise.allSettled(writePromises);\n}\n\n/**\n * 清理过期快照文件\n */\nexport async function cleanupExpiredSnapshots(\n  maxAge: number = 24 * 60 * 60 * 1000, // 默认24小时\n  baseDir?: string\n): Promise<void> {\n  try {\n    const snapshotDir = baseDir || join(homedir(), '.routecodex', 'codex-samples');\n    const files = await fs.readdir(snapshotDir, { recursive: true });\n\n    const now = Date.now();\n    const expiredFiles: string[] = [];\n\n    for (const file of files) {\n      const filePath = join(snapshotDir, file);\n      try {\n        const stats = await fs.stat(filePath);\n        if (now - stats.mtime.getTime() > maxAge) {\n          expiredFiles.push(filePath);\n        }\n      } catch {\n        // 忽略无法访问的文件\n      }\n    }\n\n    // 删除过期文件\n    for (const expiredFile of expiredFiles) {\n      try {\n        await fs.unlink(expiredFile);\n        console.log(`[ServerV2Snapshot] Cleaned up expired snapshot: ${expiredFile}`);\n      } catch {\n        // 忽略删除失败\n      }\n    }\n\n    if (expiredFiles.length > 0) {\n      console.log(`[ServerV2Snapshot] Cleaned up ${expiredFiles.length} expired snapshot files`);\n    }\n\n  } catch (error) {\n    console.error('[ServerV2Snapshot] Failed to cleanup snapshots:', error);\n  }\n}"
        }
      },
      "keyFiles": {},
      "architecture": {
        "core": {
          "src/server-v2/core/route-codex-server-v2.ts": {
            "path": "src/server-v2/core/route-codex-server-v2.ts",
            "size": 18139,
            "lines": 654,
            "imports": [
              "express",
              "cors",
              "helmet",
              "rcc-basemodule",
              "rcc-errorhandling",
              "rcc-debugcenter",
              "../../types/common-types.js",
              "../hooks/server-v2-hook-integration.js",
              "../hooks/impl/server-request-logging-hook.js",
              "../hooks/impl/server-response-enhancement-hook.js",
              "../hooks/impl/server-performance-monitoring-hook.js"
            ],
            "exports": [
              "ServerConfigV2",
              "ServerStatusV2",
              "RequestContextV2",
              "RouteCodexServerV2"
            ],
            "classes": [
              "RouteCodexServerV2"
            ],
            "functions": [],
            "content": "/**\n * RouteCodex Server V2 - 渐进式重构版本\n *\n * 核心特性：\n * - 与现有V1服务器完全并行\n * - 集成系统hooks模块\n * - 模块化设计，职责分离\n * - 保持API兼容性\n */\n\nimport express, { type Application, type Request, type Response, type NextFunction } from 'express';\nimport cors from 'cors';\nimport helmet from 'helmet';\nimport { BaseModule, type ModuleInfo } from 'rcc-basemodule';\nimport { ErrorHandlingCenter } from 'rcc-errorhandling';\nimport { DebugEventBus } from 'rcc-debugcenter';\nimport type { UnknownObject } from '../../types/common-types.js';\n// TODO: Fix hook type issues and re-enable\n// import { ServerV2HookIntegration, type ServerV2HookContext } from '../hooks/server-v2-hook-integration.js';\n// import { ServerRequestLoggingHook } from '../hooks/impl/server-request-logging-hook.js';\n// import { ServerResponseEnhancementHook } from '../hooks/impl/server-response-enhancement-hook.js';\n// import { ServerPerformanceMonitoringHook } from '../hooks/impl/server-performance-monitoring-hook.js';\n\n/**\n * V2服务器配置接口\n */\nexport interface ServerConfigV2 {\n  server: {\n    port: number;\n    host: string;\n    cors?: {\n      origin: string | string[];\n      credentials?: boolean;\n    };\n    timeout?: number;\n    bodyLimit?: string;\n    useV2?: boolean;  // V2特定配置\n  };\n  logging: {\n    level: 'debug' | 'info' | 'warn' | 'error';\n    enableConsole?: boolean;\n    enableFile?: boolean;\n    filePath?: string;\n  };\n  providers: Record<string, UnknownObject>;\n  v2Config?: {\n    enableHooks?: boolean;\n    enableMiddleware?: boolean;\n    hookStages?: string[];\n  };\n}\n\n/**\n * V2服务器状态\n */\nexport interface ServerStatusV2 {\n  initialized: boolean;\n  running: boolean;\n  port: number;\n  host: string;\n  uptime: number;\n  memory: NodeJS.MemoryUsage;\n  version: 'v2';\n  hooksEnabled: boolean;\n  middlewareEnabled: boolean;\n}\n\n/**\n * 请求上下文接口\n */\nexport interface RequestContextV2 {\n  requestId: string;\n  timestamp: number;\n  method: string;\n  url: string;\n  userAgent?: string;\n  ip?: string;\n  endpoint: string;\n}\n\n/**\n * RouteCodex Server V2\n *\n * 与V1完全并行实现，集成系统hooks\n */\nexport class RouteCodexServerV2 extends BaseModule {\n  private app: Application;\n  private server?: unknown;\n  private config: ServerConfigV2;\n  private errorHandling: ErrorHandlingCenter;\n  private debugEventBus: DebugEventBus;\n  private _isInitialized: boolean = false;\n  private _isRunning: boolean = false;\n\n  // V2特性\n  private hooksEnabled: boolean = false;\n  private middlewareEnabled: boolean = false;\n  // private hookIntegration?: ServerV2HookIntegration; // TODO: Re-enable after type fixes\n\n  constructor(config: ServerConfigV2) {\n    const moduleInfo: ModuleInfo = {\n      id: 'routecodex-server-v2',\n      name: 'RouteCodexServerV2',\n      version: '2.0.0',\n      description: 'RouteCodex Server V2 with enhanced hooks and middleware',\n      type: 'server',\n    };\n\n    super(moduleInfo);\n\n    this.config = config;\n    this.app = express();\n    this.errorHandling = new ErrorHandlingCenter();\n    this.debugEventBus = DebugEventBus.getInstance();\n\n    // V2特性初始化\n    this.hooksEnabled = config.v2Config?.enableHooks ?? false;\n    this.middlewareEnabled = config.v2Config?.enableMiddleware ?? false;\n\n    // 初始化Hook集成系统 (如果启用)\n    if (this.hooksEnabled) {\n      console.log('[RouteCodexServerV2] Hooks enabled, but integration temporarily disabled for testing');\n      // this.initializeHookIntegration(); // 暂时禁用避免类型问题\n    }\n\n    console.log(`[RouteCodexServerV2] Initialized with hooks: ${this.hooksEnabled}, middleware: ${this.middlewareEnabled}`);\n  }\n\n  /**\n   * 初始化服务器\n   */\n  public async initialize(): Promise<void> {\n    try {\n      console.log('[RouteCodexServerV2] Starting initialization...');\n\n      // 初始化错误处理\n      await this.errorHandling.initialize();\n\n      // 设置调试事件监听\n      this.setupDebugLogging();\n\n      // 设置中间件\n      await this.setupMiddleware();\n\n      // 设置路由\n      await this.setupRoutes();\n\n      // 设置错误处理\n      this.setupErrorHandling();\n\n      this._isInitialized = true;\n\n      this.logEvent('server-v2', 'initialized', {\n        port: this.config.server.port,\n        host: this.config.server.host,\n        hooksEnabled: this.hooksEnabled,\n        middlewareEnabled: this.middlewareEnabled,\n        version: '2.0.0'\n      });\n\n      console.log('[RouteCodexServerV2] Initialization completed successfully');\n\n    } catch (error) {\n      await this.handleError(error as Error, 'initialization');\n      throw error;\n    }\n  }\n\n  /**\n   * 启动服务器\n   */\n  public async start(): Promise<void> {\n    if (!this._isInitialized) {\n      await this.initialize();\n    }\n\n    return new Promise((resolve, reject) => {\n      this.server = this.app.listen(this.config.server.port, this.config.server.host, () => {\n        this._isRunning = true;\n\n        this.logEvent('server-v2', 'started', {\n          port: this.config.server.port,\n          host: this.config.server.host,\n          version: 'v2'\n        });\n\n        console.log(`[RouteCodexServerV2] Server started on ${this.config.server.host}:${this.config.server.port}`);\n        resolve();\n      });\n\n      (this.server as any).on('error', async (error: Error) => {\n        await this.handleError(error, 'server_start');\n        reject(error);\n      });\n    });\n  }\n\n  /**\n   * 停止服务器\n   */\n  public async stop(): Promise<void> {\n    if (this.server) {\n      return new Promise(resolve => {\n        (this.server as any)?.close(async () => {\n          this._isRunning = false;\n\n          this.logEvent('server-v2', 'stopped', {});\n\n          // 清理资源\n          await this.errorHandling.destroy();\n\n          console.log('[RouteCodexServerV2] Server stopped');\n          resolve();\n        });\n      });\n    }\n  }\n\n  /**\n   * 获取服务器状态\n   */\n  public getStatus(): ServerStatusV2 {\n    return {\n      initialized: this._isInitialized,\n      running: this._isRunning,\n      port: this.config.server.port,\n      host: this.config.server.host,\n      uptime: process.uptime(),\n      memory: process.memoryUsage(),\n      version: 'v2',\n      hooksEnabled: this.hooksEnabled,\n      middlewareEnabled: this.middlewareEnabled\n    };\n  }\n\n  /**\n   * V1兼容接口：检查是否已初始化\n   */\n  public isInitialized(): boolean {\n    return this._isInitialized;\n  }\n\n  /**\n   * V1兼容接口：检查是否正在运行\n   */\n  public isRunning(): boolean {\n    return this._isRunning;\n  }\n\n  /**\n   * 设置中间件\n   */\n  private async setupMiddleware(): Promise<void> {\n    console.log('[RouteCodexServerV2] Setting up middleware...');\n\n    // 基础安全中间件\n    this.app.use(helmet());\n\n    // CORS中间件\n    if (this.config.server.cors) {\n      this.app.use(cors(this.config.server.cors));\n    } else {\n      this.app.use(cors({\n        origin: '*',\n        credentials: true,\n      }));\n    }\n\n    // 请求解析中间件\n    this.app.use(express.json({ limit: this.config.server.bodyLimit || '10mb' }));\n    this.app.use(express.urlencoded({ extended: true, limit: this.config.server.bodyLimit || '10mb' }));\n\n    // 请求日志中间件\n    this.app.use((req: Request, res: Response, next: NextFunction) => {\n      const start = Date.now();\n      const requestId = this.generateRequestId();\n\n      // 添加请求ID到请求对象\n      (req as any).__requestId = requestId;\n\n      res.on('finish', () => {\n        const duration = Date.now() - start;\n        this.logEvent('request-v2', 'completed', {\n          requestId,\n          method: req.method,\n          url: req.url,\n          status: res.statusCode,\n          duration,\n          userAgent: req.get('user-agent'),\n          ip: req.ip\n        });\n      });\n\n      next();\n    });\n\n    console.log('[RouteCodexServerV2] Middleware setup completed');\n  }\n\n  /**\n   * 设置路由\n   */\n  private async setupRoutes(): Promise<void> {\n    console.log('[RouteCodexServerV2] Setting up routes...');\n\n    // V2健康检查端点 (不同端口避免冲突)\n    this.app.get('/health-v2', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        status: status.running ? 'healthy' : 'unhealthy',\n        version: 'v2',\n        timestamp: new Date().toISOString(),\n        uptime: status.uptime,\n        memory: status.memory,\n        hooksEnabled: status.hooksEnabled,\n        middlewareEnabled: status.middlewareEnabled\n      });\n    });\n\n    // V1兼容的健康检查\n    this.app.get('/health', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        status: status.running ? 'healthy' : 'unhealthy',\n        version: 'v2',\n        timestamp: new Date().toISOString(),\n        uptime: status.uptime,\n        memory: status.memory\n      });\n    });\n\n    // V2专用端点 (用于测试)\n    this.app.post('/v2/chat/completions', this.handleChatCompletionsV2.bind(this));\n\n    // 状态端点\n    this.app.get('/status-v2', (req: Request, res: Response) => {\n      res.json(this.getStatus());\n    });\n\n    // V1兼容的状态端点\n    this.app.get('/status', (req: Request, res: Response) => {\n      const status = this.getStatus();\n      res.json({\n        initialized: status.initialized,\n        running: status.running,\n        port: status.port,\n        host: status.host,\n        uptime: status.uptime,\n        memory: status.memory\n      });\n    });\n\n    // 模型列表端点 (V1兼容)\n    this.app.get('/v1/models', this.handleModels.bind(this));\n\n    // 404处理器\n    this.app.use('*', (req: Request, res: Response) => {\n      res.status(404).json({\n        error: {\n          message: 'Not Found',\n          type: 'not_found_error',\n          code: 'not_found',\n        },\n      });\n    });\n\n    console.log('[RouteCodexServerV2] Routes setup completed');\n  }\n\n  /**\n   * V2专用Chat Completions处理器\n   */\n  private async handleChatCompletionsV2(req: Request, res: Response): Promise<void> {\n    const startTime = Date.now();\n    const requestId = (req as any).__requestId || this.generateRequestId();\n\n    const context: RequestContextV2 = {\n      requestId,\n      timestamp: startTime,\n      method: req.method,\n      url: req.url,\n      userAgent: req.get('user-agent'),\n      ip: req.ip,\n      endpoint: '/v2/chat/completions'\n    };\n\n    this.logEvent('api-v2', 'chat_completions_request', {\n      requestId,\n      model: req.body.model,\n      messageCount: req.body.messages?.length || 0,\n      streaming: req.body.stream || false,\n      tools: !!req.body.tools\n    });\n\n    try {\n      // V2处理逻辑 (Hook集成暂时禁用)\n      if (this.hooksEnabled) {\n        console.log('[RouteCodexServerV2] Hooks would be executed here (temporarily disabled)');\n      }\n\n      // 这里将来会连接Pipeline系统，现在使用占位符响应\n      const response = {\n        id: `chatcmpl-${requestId}`,\n        object: 'chat.completion',\n        created: Math.floor(Date.now() / 1000),\n        model: req.body.model || 'gpt-3.5-turbo',\n        choices: [{\n          index: 0,\n          message: {\n            role: 'assistant',\n            content: '[V2 Response] This is a response from RouteCodex Server V2. Hooks and snapshot integration are temporarily disabled for testing.'\n          },\n          finish_reason: 'stop'\n        }],\n        usage: {\n          prompt_tokens: 10,\n          completion_tokens: 25,\n          total_tokens: 35\n        },\n        // V2增强标识\n        serverV2Enhanced: true,\n        processingTime: Date.now() - startTime\n      };\n\n      res.json(response);\n\n      this.logEvent('api-v2', 'chat_completions_success', {\n        requestId,\n        duration: Date.now() - startTime,\n        model: response.model\n      });\n\n    } catch (error) {\n      await this.handleError(error as Error, 'chat_completions_v2');\n\n      res.status(500).json({\n        error: {\n          message: 'Internal Server Error',\n          type: 'internal_error',\n          code: 'internal_error'\n        }\n      });\n    }\n  }\n\n  /**\n   * 处理模型列表 (V1兼容)\n   */\n  private async handleModels(req: Request, res: Response): Promise<void> {\n    try {\n      this.logEvent('api-v2', 'models_request', {});\n\n      // 从配置中获取模型列表\n      const models = [];\n      for (const [providerId, providerConfig] of Object.entries(this.config.providers)) {\n        if (providerConfig.enabled && providerConfig.models) {\n          for (const [modelId, _modelConfig] of Object.entries(providerConfig.models)) {\n            models.push({\n              id: modelId,\n              object: 'model',\n              created: Math.floor(Date.now() / 1000),\n              owned_by: providerId,\n            });\n          }\n        }\n      }\n\n      res.json({\n        object: 'list',\n        data: models,\n      });\n    } catch (error) {\n      await this.handleError(error as Error, 'models_handler_v2');\n      throw error;\n    }\n  }\n\n  /**\n   * 设置错误处理\n   */\n  private setupErrorHandling(): void {\n    this.app.use((error: UnknownObject, req: Request, res: Response, _next: NextFunction) => {\n      this.handleError(error as unknown as Error, 'request_handler').catch(() => {\n        // 忽略处理器错误，直接发送响应\n      });\n\n      const errorStatus = (error as any).status || 500;\n      res.status(errorStatus).json({\n        error: {\n          message: (error as any).message || 'Internal Server Error',\n          type: (error as any).type || 'internal_error',\n          code: (error as any).code || 'internal_error',\n        },\n      });\n    });\n  }\n\n  /**\n   * 设置调试日志\n   */\n  private setupDebugLogging(): void {\n    this.debugEventBus.subscribe('*', (event: UnknownObject) => {\n      if (this.config.logging.level === 'debug') {\n        const timestamp = new Date(event.timestamp as any).toISOString();\n        const eventData = event.data as Record<string, unknown>;\n        const category = (eventData?.category as string) || 'general';\n        const action = (eventData?.action as string) || 'unknown';\n\n        const logMessage = `[${timestamp}] [DEBUG-V2] ${event.operationId}: ${JSON.stringify(\n          {\n            category,\n            action,\n            moduleId: event.moduleId,\n            data: event.data,\n          },\n          null,\n          2\n        )}\\n`;\n\n        console.log(`[DEBUG-V2] ${event.operationId}:`, {\n          category,\n          action,\n          timestamp,\n          moduleId: event.moduleId,\n          data: event.data,\n        });\n      }\n    });\n  }\n\n  /**\n   * 记录事件到调试中心\n   */\n  private logEvent(category: string, action: string, data: UnknownObject): void {\n    try {\n      this.debugEventBus.publish({\n        sessionId: `session_v2_${Date.now()}`,\n        moduleId: this.getModuleInfo().id,\n        operationId: `${category}_${action}_v2`,\n        timestamp: Date.now(),\n        type: 'start',\n        position: 'middle',\n        data: {\n          category,\n          action,\n          version: 'v2',\n          ...data,\n        },\n      });\n    } catch (error) {\n      console.error('[RouteCodexServerV2] Failed to log event:', error);\n    }\n  }\n\n  /**\n   * 处理错误\n   */\n  private async handleError(error: Error, context: string): Promise<void> {\n    try {\n      await this.errorHandling.handleError({\n        error: error.message,\n        source: `${this.getModuleInfo().id}.${context}`,\n        severity: 'medium',\n        timestamp: Date.now(),\n        moduleId: this.getModuleInfo().id,\n        context: {\n          stack: error.stack,\n          name: error.name,\n          version: 'v2'\n        },\n      });\n    } catch (handlerError) {\n      console.error('[RouteCodexServerV2] Failed to handle error:', handlerError);\n      console.error('[RouteCodexServerV2] Original error:', error);\n    }\n  }\n\n  /**\n   * 初始化Hook集成系统 (暂时禁用)\n   */\n  private initializeHookIntegration(): void {\n    console.log('[RouteCodexServerV2] Hook Integration temporarily disabled for testing');\n    // TODO: Fix hook type issues and re-enable\n    /*\n    this.hookIntegration = new ServerV2HookIntegration({\n      enabled: true,\n      snapshot: {\n        enabled: true,\n        level: 'normal',\n        phases: ['server-entry', 'server-pre-process', 'server-post-process', 'server-response', 'server-final']\n      },\n      hooks: {\n        enabled: true,\n        timeout: 5000,\n        parallel: false,\n        retryAttempts: 2\n      }\n    });\n\n    this.hookIntegration.registerHook(new ServerRequestLoggingHook());\n    this.hookIntegration.registerHook(new ServerResponseEnhancementHook());\n    this.hookIntegration.registerHook(new ServerPerformanceMonitoringHook());\n    */\n  }\n\n  /**\n   * 创建Hook上下文 (暂时禁用)\n   */\n  private createHookContext(requestId: string, endpoint: string, originalRequest?: UnknownObject): any {\n    // TODO: Re-enable after type fixes\n    return {\n      serverVersion: 'v2',\n      endpoint,\n      requestId,\n      originalRequest,\n      metadata: {\n        startTime: Date.now()\n      }\n    };\n  }\n\n  /**\n   * 执行Hook并记录快照 (暂时禁用)\n   */\n  private async executeHooksWithSnapshot(\n    phase: 'server-entry' | 'server-pre-process' | 'server-post-process' | 'server-response' | 'server-final',\n    data: UnknownObject,\n    context: any\n  ): Promise<{ data: UnknownObject; executionTime: number }> {\n    // TODO: Re-enable after type fixes\n    console.log(`[RouteCodexServerV2] Hook execution for phase ${phase} temporarily disabled`);\n    return { data, executionTime: 0 };\n  }\n\n  /**\n   * 生成请求ID\n   */\n  private generateRequestId(): string {\n    return `req-v2-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  /**\n   * 获取模块信息\n   */\n  public getModuleInfo(): ModuleInfo {\n    return {\n      id: 'routecodex-server-v2',\n      name: 'RouteCodexServerV2',\n      version: '2.0.0',\n      description: 'RouteCodex Server V2 with enhanced hooks and middleware',\n      type: 'server',\n    };\n  }\n}"
          }
        },
        "handlers": {
          "src/server-v2/handlers/chat-completions-v2.ts": {
            "path": "src/server-v2/handlers/chat-completions-v2.ts",
            "size": 8368,
            "lines": 283,
            "imports": [
              "express",
              "../core/route-codex-server-v2.js"
            ],
            "exports": [
              "ChatCompletionsHandlerV2"
            ],
            "classes": [
              "BaseHandlerV2",
              "ChatCompletionsHandlerV2"
            ],
            "functions": [],
            "content": "/**\n * Chat Completions Handler V2\n *\n * V2版本的Chat Completions处理器\n * 集成hooks系统，模块化设计\n */\n\nimport { type Request, type Response } from 'express';\nimport type { RequestContextV2 } from '../core/route-codex-server-v2.js';\n\n/**\n * V2处理器基类\n */\nexport abstract class BaseHandlerV2 {\n  protected generateRequestId(): string {\n    return `req-v2-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  protected createContext(req: Request): RequestContextV2 {\n    return {\n      requestId: (req as any).__requestId || this.generateRequestId(),\n      timestamp: Date.now(),\n      method: req.method,\n      url: req.url,\n      userAgent: req.get('user-agent'),\n      ip: req.ip,\n      endpoint: req.path\n    };\n  }\n\n  protected sendJsonResponse(res: Response, data: any, context: RequestContextV2): void {\n    try {\n      res.setHeader('x-request-id', context.requestId);\n      res.setHeader('x-server-version', 'v2');\n      res.json(data);\n    } catch (error) {\n      console.error('[BaseHandlerV2] Failed to send response:', error);\n      res.status(500).json({\n        error: {\n          message: 'Internal Server Error',\n          type: 'response_error',\n          code: 'response_error'\n        }\n      });\n    }\n  }\n}\n\n/**\n * Chat Completions Handler V2\n */\nexport class ChatCompletionsHandlerV2 extends BaseHandlerV2 {\n  private hooksEnabled: boolean;\n  private pipelineEnabled: boolean;\n\n  constructor(hooksEnabled: boolean = true, pipelineEnabled: boolean = false) {\n    super();\n    this.hooksEnabled = hooksEnabled;\n    this.pipelineEnabled = pipelineEnabled;\n    console.log(`[ChatCompletionsHandlerV2] Initialized (hooks: ${hooksEnabled}, pipeline: ${pipelineEnabled})`);\n  }\n\n  /**\n   * 处理Chat Completions请求\n   */\n  async handleRequest(req: Request, res: Response): Promise<void> {\n    const context = this.createContext(req);\n    const startTime = Date.now();\n\n    console.log(`[ChatCompletionsHandlerV2] Processing request ${context.requestId}`);\n\n    try {\n      // 执行请求前Hooks (预留)\n      if (this.hooksEnabled) {\n        await this.executePreRequestHooks(req, context);\n      }\n\n      // 验证请求\n      await this.validateRequest(req, context);\n\n      // 处理请求\n      const response = await this.processRequest(req, context);\n\n      // 执行响应后Hooks (预留)\n      if (this.hooksEnabled) {\n        await this.executePostResponseHooks(response, context);\n      }\n\n      // 发送响应\n      this.sendJsonResponse(res, response, context);\n\n      const duration = Date.now() - startTime;\n      console.log(`[ChatCompletionsHandlerV2] Request ${context.requestId} completed in ${duration}ms`);\n\n    } catch (error) {\n      await this.handleError(error as Error, res, context);\n    }\n  }\n\n  /**\n   * 验证请求\n   */\n  private async validateRequest(req: Request, context: RequestContextV2): Promise<void> {\n    const body = req.body;\n\n    if (!body) {\n      throw new Error('Request body is required');\n    }\n\n    if (!body.model) {\n      throw new Error('Model is required');\n    }\n\n    if (!body.messages || !Array.isArray(body.messages)) {\n      throw new Error('Messages must be an array');\n    }\n\n    if (body.messages.length === 0) {\n      throw new Error('Messages array cannot be empty');\n    }\n\n    // 验证消息格式\n    for (const [index, message] of body.messages.entries()) {\n      if (!message.role || !message.content) {\n        throw new Error(`Message at index ${index} must have 'role' and 'content' fields`);\n      }\n\n      const validRoles = ['system', 'user', 'assistant', 'tool'];\n      if (!validRoles.includes(message.role)) {\n        throw new Error(`Invalid role '${message.role}' in message at index ${index}`);\n      }\n    }\n\n    console.log(`[ChatCompletionsHandlerV2] Request validation passed for ${context.requestId}`);\n  }\n\n  /**\n   * 处理请求\n   */\n  private async processRequest(req: Request, context: RequestContextV2): Promise<any> {\n    const body = req.body;\n\n    // 如果启用了Pipeline集成\n    if (this.pipelineEnabled) {\n      return await this.processWithPipeline(body, context);\n    }\n\n    // 否则返回模拟响应\n    return this.createMockResponse(body, context);\n  }\n\n  /**\n   * 创建模拟响应\n   */\n  private createMockResponse(body: any, context: RequestContextV2): any {\n    return {\n      id: `chatcmpl-${context.requestId}`,\n      object: 'chat.completion',\n      created: Math.floor(Date.now() / 1000),\n      model: body.model,\n      choices: [{\n        index: 0,\n        message: {\n          role: 'assistant',\n          content: `[V2 Mock Response] This is a placeholder response from RouteCodex Server V2. Request ID: ${context.requestId}. Model: ${body.model}. Messages: ${body.messages.length}.`\n        },\n        finish_reason: 'stop'\n      }],\n      usage: {\n        prompt_tokens: this.estimateTokens(body.messages),\n        completion_tokens: 50,\n        total_tokens: this.estimateTokens(body.messages) + 50\n      }\n    };\n  }\n\n  /**\n   * 使用Pipeline处理请求 (预留)\n   */\n  private async processWithPipeline(body: any, context: RequestContextV2): Promise<any> {\n    console.log(`[ChatCompletionsHandlerV2] Pipeline processing not yet implemented for ${context.requestId}`);\n\n    // TODO: 集成Pipeline系统\n    // 目前返回模拟响应\n    return this.createMockResponse(body, context);\n  }\n\n  /**\n   * 执行请求前Hooks (预留)\n   */\n  private async executePreRequestHooks(req: Request, context: RequestContextV2): Promise<void> {\n    console.log(`[ChatCompletionsHandlerV2] Executing pre-request hooks for ${context.requestId}`);\n\n    // TODO: 集成系统hooks\n    // 这里将调用系统hooks模块的request_preprocessing hooks\n  }\n\n  /**\n   * 执行响应后Hooks (预留)\n   */\n  private async executePostResponseHooks(response: any, context: RequestContextV2): Promise<void> {\n    console.log(`[ChatCompletionsHandlerV2] Executing post-response hooks for ${context.requestId}`);\n\n    // TODO: 集成系统hooks\n    // 这里将调用系统hooks模块的response_postprocessing hooks\n  }\n\n  /**\n   * 错误处理\n   */\n  private async handleError(error: Error, res: Response, context: RequestContextV2): Promise<void> {\n    console.error(`[ChatCompletionsHandlerV2] Error processing request ${context.requestId}:`, error);\n\n    // 错误hooks处理 (预留)\n    if (this.hooksEnabled) {\n      try {\n        await this.executeErrorHooks(error, context);\n      } catch (hookError) {\n        console.error(`[ChatCompletionsHandlerV2] Error in error hooks:`, hookError);\n      }\n    }\n\n    // 发送错误响应\n    try {\n      res.setHeader('x-request-id', context.requestId);\n      res.setHeader('x-server-version', 'v2');\n\n      if (error.message.includes('required')) {\n        res.status(400).json({\n          error: {\n            message: error.message,\n            type: 'validation_error',\n            code: 'validation_error'\n          }\n        });\n      } else {\n        res.status(500).json({\n          error: {\n            message: 'Internal Server Error',\n            type: 'internal_error',\n            code: 'internal_error'\n          }\n        });\n      }\n    } catch (responseError) {\n      console.error(`[ChatCompletionsHandlerV2] Failed to send error response:`, responseError);\n      res.status(500).send('Internal Server Error');\n    }\n  }\n\n  /**\n   * 执行错误Hooks (预留)\n   */\n  private async executeErrorHooks(error: Error, context: RequestContextV2): Promise<void> {\n    console.log(`[ChatCompletionsHandlerV2] Executing error hooks for ${context.requestId}`);\n\n    // TODO: 集成系统hooks\n    // 这里将调用系统hooks模块的error_handling hooks\n  }\n\n  /**\n   * 估算Token数量 (简单实现)\n   */\n  private estimateTokens(messages: any[]): number {\n    let totalTokens = 0;\n\n    for (const message of messages) {\n      if (typeof message.content === 'string') {\n        // 简单估算：4个字符 ≈ 1个token\n        totalTokens += Math.ceil(message.content.length / 4);\n      } else if (Array.isArray(message.content)) {\n        // 多模态内容\n        for (const content of message.content) {\n          if (content.type === 'text' && content.text) {\n            totalTokens += Math.ceil(content.text.length / 4);\n          }\n        }\n      }\n    }\n\n    return Math.max(totalTokens, 1); // 至少1个token\n  }\n}"
          }
        },
        "hooks": {
          "src/server-v2/hooks/impl/server-performance-monitoring-hook.ts": {
            "path": "src/server-v2/hooks/impl/server-performance-monitoring-hook.ts",
            "size": 6049,
            "lines": 219,
            "imports": [
              "../../../types/common-types.js",
              "../server-v2-hook-integration.js"
            ],
            "exports": [
              "ServerPerformanceMonitoringHook"
            ],
            "classes": [
              "PerformanceMonitor",
              "ServerPerformanceMonitoringHook"
            ],
            "functions": [],
            "content": "/**\n * Server Performance Monitoring Hook\n *\n * 按照现有规范实现的性能监控Hook\n * 命名: server.03.performance-monitoring\n */\n\nimport type { UnknownObject } from '../../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookResult\n} from '../../../modules/hooks/types/hook-types.js';\nimport type { ServerV2HookContext } from '../server-v2-hook-integration.js';\n\n/**\n * 性能指标接口\n */\ninterface PerformanceMetrics {\n  requestId: string;\n  endpoint: string;\n  stage: string;\n  timestamp: number;\n  memoryUsage: NodeJS.MemoryUsage;\n  cpuUsage?: NodeJS.CpuUsage;\n  processingTime: number;\n  dataSize: number;\n}\n\n/**\n * 性能监控数据存储\n */\nclass PerformanceMonitor {\n  private metrics: Map<string, PerformanceMetrics[]> = new Map();\n  private maxEntries: number = 1000;\n\n  recordMetrics(metrics: PerformanceMetrics): void {\n    const key = `${metrics.requestId}_${metrics.endpoint}`;\n    const existing = this.metrics.get(key) || [];\n\n    existing.push(metrics);\n\n    // 保持最大条目数限制\n    if (existing.length > this.maxEntries) {\n      existing.splice(0, existing.length - this.maxEntries);\n    }\n\n    this.metrics.set(key, existing);\n  }\n\n  getMetrics(requestId: string, endpoint: string): PerformanceMetrics[] {\n    const key = `${requestId}_${endpoint}`;\n    return this.metrics.get(key) || [];\n  }\n\n  getAllMetrics(): PerformanceMetrics[] {\n    const allMetrics: PerformanceMetrics[] = [];\n    for (const metrics of this.metrics.values()) {\n      allMetrics.push(...metrics);\n    }\n    return allMetrics;\n  }\n\n  clearMetrics(): void {\n    this.metrics.clear();\n  }\n\n  getPerformanceSummary(): {\n    totalRequests: number;\n    avgProcessingTime: number;\n    avgMemoryUsage: number;\n    slowestRequest: { requestId: string; time: number } | null;\n  } {\n    const allMetrics = this.getAllMetrics();\n\n    if (allMetrics.length === 0) {\n      return {\n        totalRequests: 0,\n        avgProcessingTime: 0,\n        avgMemoryUsage: 0,\n        slowestRequest: null\n      };\n    }\n\n    const totalProcessingTime = allMetrics.reduce((sum, m) => sum + m.processingTime, 0);\n    const totalMemoryUsage = allMetrics.reduce((sum, m) => sum + m.memoryUsage.heapUsed, 0);\n    const slowest = allMetrics.reduce((max, current) =>\n      current.processingTime > max.processingTime ? current : max\n    , allMetrics[0]);\n\n    return {\n      totalRequests: new Set(allMetrics.map(m => m.requestId)).size,\n      avgProcessingTime: Math.round(totalProcessingTime / allMetrics.length * 100) / 100,\n      avgMemoryUsage: Math.round(totalMemoryUsage / allMetrics.length),\n      slowestRequest: {\n        requestId: slowest.requestId,\n        time: slowest.processingTime\n      }\n    };\n  }\n}\n\n// 全局性能监控实例\nconst performanceMonitor = new PerformanceMonitor();\n\n/**\n * Server性能监控Hook\n */\nexport class ServerPerformanceMonitoringHook implements IBidirectionalHook {\n  readonly name = 'server.03.performance-monitoring';\n  readonly stage: UnifiedHookStage = 'RESPONSE_VALIDATION';\n  readonly priority = 30;\n  readonly isDebugHook = true;\n\n  async execute(context: HookExecutionContext, data: { data: UnknownObject; metadata: UnknownObject }): Promise<HookResult> {\n    const serverContext = context as ServerV2HookContext;\n    const startTime = serverContext.metadata?.startTime as number || Date.now();\n    const processingTime = Date.now() - startTime;\n\n    // 计算数据大小\n    const dataSize = this.calculateDataSize(data.data);\n\n    // 记录性能指标\n    const metrics: PerformanceMetrics = {\n      requestId: serverContext.requestId,\n      endpoint: serverContext.endpoint,\n      stage: this.stage,\n      timestamp: Date.now(),\n      memoryUsage: process.memoryUsage(),\n      cpuUsage: process.cpuUsage(),\n      processingTime,\n      dataSize\n    };\n\n    performanceMonitor.recordMetrics(metrics);\n\n    console.log(`[ServerPerformanceMonitoringHook] Performance metrics for ${serverContext.requestId}:`, {\n      processingTime: `${processingTime}ms`,\n      memoryUsage: `${Math.round(metrics.memoryUsage.heapUsed / 1024 / 1024)}MB`,\n      dataSize: `${Math.round(dataSize / 1024)}KB`\n    });\n\n    // 检查性能警告\n    const warnings = this.checkPerformanceWarnings(metrics);\n    if (warnings.length > 0) {\n      console.warn(`[ServerPerformanceMonitoringHook] Performance warnings for ${serverContext.requestId}:`, warnings);\n    }\n\n    // 在元数据中添加性能信息\n    const enrichedMetadata = {\n      ...data.metadata,\n      performance: {\n        processingTime,\n        memoryUsage: metrics.memoryUsage,\n        dataSize,\n        warnings\n      }\n    };\n\n    return {\n      success: true,\n      data: data.data, // 不修改原始数据\n      metadata: enrichedMetadata\n    };\n  }\n\n  /**\n   * 计算数据大小\n   */\n  private calculateDataSize(data: UnknownObject): number {\n    try {\n      const jsonString = JSON.stringify(data);\n      return Buffer.byteLength(jsonString, 'utf-8');\n    } catch {\n      return 0;\n    }\n  }\n\n  /**\n   * 检查性能警告\n   */\n  private checkPerformanceWarnings(metrics: PerformanceMetrics): string[] {\n    const warnings: string[] = [];\n\n    // 处理时间警告\n    if (metrics.processingTime > 5000) { // 5秒\n      warnings.push(`Slow processing: ${metrics.processingTime}ms`);\n    }\n\n    // 内存使用警告\n    const memoryUsageMB = metrics.memoryUsage.heapUsed / 1024 / 1024;\n    if (memoryUsageMB > 500) { // 500MB\n      warnings.push(`High memory usage: ${Math.round(memoryUsageMB)}MB`);\n    }\n\n    // 数据大小警告\n    const dataSizeKB = metrics.dataSize / 1024;\n    if (dataSizeKB > 1024) { // 1MB\n      warnings.push(`Large data size: ${Math.round(dataSizeKB)}KB`);\n    }\n\n    return warnings;\n  }\n\n  /**\n   * 获取性能监控器实例\n   */\n  static getPerformanceMonitor(): PerformanceMonitor {\n    return performanceMonitor;\n  }\n\n  /**\n   * 获取性能统计\n   */\n  static getPerformanceStats() {\n    return performanceMonitor.getPerformanceSummary();\n  }\n}"
          },
          "src/server-v2/hooks/impl/server-request-logging-hook.ts": {
            "path": "src/server-v2/hooks/impl/server-request-logging-hook.ts",
            "size": 1804,
            "lines": 58,
            "imports": [
              "../../../types/common-types.js",
              "../server-v2-hook-integration.js"
            ],
            "exports": [
              "ServerRequestLoggingHook"
            ],
            "classes": [
              "ServerRequestLoggingHook"
            ],
            "functions": [],
            "content": "/**\n * Server Request Logging Hook\n *\n * 按照现有规范实现的请求日志Hook\n * 命名: server.01.request-logging\n */\n\nimport type { UnknownObject } from '../../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookResult\n} from '../../../modules/hooks/types/hook-types.js';\nimport type { ServerV2HookContext } from '../server-v2-hook-integration.js';\n\n/**\n * Server请求日志Hook\n */\nexport class ServerRequestLoggingHook implements IBidirectionalHook {\n  readonly name = 'server.01.request-logging';\n  readonly stage: UnifiedHookStage = 'PIPELINE_PREPROCESSING';\n  readonly priority = 10;\n  readonly isDebugHook = true;\n\n  async execute(context: HookExecutionContext, data: { data: UnknownObject; metadata: UnknownObject }): Promise<HookResult> {\n    const serverContext = context as ServerV2HookContext;\n    const requestData = data.data;\n\n    console.log(`[ServerRequestLoggingHook] Processing request:`, {\n      requestId: serverContext.requestId,\n      endpoint: serverContext.endpoint,\n      method: (requestData as any)?.method,\n      url: (requestData as any)?.url,\n      userAgent: (requestData as any)?.headers?.['user-agent'],\n      timestamp: new Date().toISOString()\n    });\n\n    // 记录请求基本信息到元数据\n    const enrichedMetadata = {\n      ...data.metadata,\n      requestInfo: {\n        requestId: serverContext.requestId,\n        endpoint: serverContext.endpoint,\n        method: (requestData as any)?.method || 'unknown',\n        url: (requestData as any)?.url || 'unknown',\n        timestamp: Date.now(),\n        serverVersion: serverContext.serverVersion\n      }\n    };\n\n    return {\n      success: true,\n      data: requestData, // 不修改原始数据\n      metadata: enrichedMetadata\n    };\n  }\n}"
          },
          "src/server-v2/hooks/impl/server-response-enhancement-hook.ts": {
            "path": "src/server-v2/hooks/impl/server-response-enhancement-hook.ts",
            "size": 2170,
            "lines": 71,
            "imports": [
              "../../../types/common-types.js",
              "../server-v2-hook-integration.js"
            ],
            "exports": [
              "ServerResponseEnhancementHook"
            ],
            "classes": [
              "ServerResponseEnhancementHook"
            ],
            "functions": [],
            "content": "/**\n * Server Response Enhancement Hook\n *\n * 按照现有规范实现的响应增强Hook\n * 命名: server.02.response-enhancement\n */\n\nimport type { UnknownObject } from '../../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookResult\n} from '../../../modules/hooks/types/hook-types.js';\nimport type { ServerV2HookContext } from '../server-v2-hook-integration.js';\n\n/**\n * Server响应增强Hook\n */\nexport class ServerResponseEnhancementHook implements IBidirectionalHook {\n  readonly name = 'server.02.response-enhancement';\n  readonly stage: UnifiedHookStage = 'RESPONSE_POSTPROCESSING';\n  readonly priority = 20;\n  readonly isDebugHook = false;\n\n  async execute(context: HookExecutionContext, data: { data: UnknownObject; metadata: UnknownObject }): Promise<HookResult> {\n    const serverContext = context as ServerV2HookContext;\n    const responseData = data.data;\n\n    // 增强响应数据\n    if (responseData && typeof responseData === 'object') {\n      const enhancedResponse = { ...responseData };\n\n      // 添加服务器版本信息\n      (enhancedResponse as any).serverInfo = {\n        version: serverContext.serverVersion,\n        requestId: serverContext.requestId,\n        processedAt: new Date().toISOString(),\n        processingTime: Date.now() - (serverContext.metadata?.startTime as number || Date.now())\n      };\n\n      // 添加Hook执行统计（如果可用）\n      if (serverContext.metadata?.hookStats) {\n        (enhancedResponse as any).hookStats = serverContext.metadata.hookStats;\n      }\n\n      console.log(`[ServerResponseEnhancementHook] Enhanced response for request: ${serverContext.requestId}`);\n\n      return {\n        success: true,\n        data: enhancedResponse,\n        metadata: {\n          ...data.metadata,\n          responseEnhanced: true,\n          enhancementTime: Date.now()\n        }\n      };\n    }\n\n    // 如果不是对象类型，直接返回\n    return {\n      success: true,\n      data: responseData,\n      metadata: {\n        ...data.metadata,\n        responseEnhanced: false,\n        reason: 'Response data is not an object'\n      }\n    };\n  }\n}"
          },
          "src/server-v2/hooks/server-hook-manager.ts": {
            "path": "src/server-v2/hooks/server-hook-manager.ts",
            "size": 9998,
            "lines": 375,
            "imports": [
              "../../types/common-types.js",
              "../core/route-codex-server-v2.js"
            ],
            "exports": [
              "HookExecutionContext",
              "HookExecutionResult",
              "ServerHookManager"
            ],
            "classes": [
              "ServerHookManager"
            ],
            "functions": [],
            "content": "/**\n * Server Hook Manager V2\n *\n * 集成系统hooks模块的Hook管理器\n * 为Server V2提供统一的Hook执行接口\n */\n\nimport type { UnknownObject } from '../../types/common-types.js';\nimport type { RequestContextV2 } from '../core/route-codex-server-v2.js';\n\n/**\n * Hook执行上下文\n */\nexport interface HookExecutionContext {\n  requestId: string;\n  stage: string;\n  timestamp: number;\n  data: UnknownObject;\n}\n\n/**\n * Hook执行结果\n */\nexport interface HookExecutionResult {\n  success: boolean;\n  data?: UnknownObject;\n  error?: string;\n  executionTime: number;\n  observations?: string[];\n}\n\n/**\n * Server Hook管理器\n */\nexport class ServerHookManager {\n  private enabled: boolean;\n  private hooks: Map<string, Function[]> = new Map();\n  private executionStats: Map<string, { count: number; totalTime: number; errors: number }> = new Map();\n\n  constructor(enabled: boolean = true) {\n    this.enabled = enabled;\n    console.log(`[ServerHookManager] Initialized (enabled: ${enabled})`);\n\n    if (enabled) {\n      this.initializeDefaultHooks();\n    }\n  }\n\n  /**\n   * 初始化默认Hooks\n   */\n  private initializeDefaultHooks(): void {\n    console.log('[ServerHookManager] Initializing default hooks');\n\n    // 请求预处理Hooks\n    this.registerHook('request_preprocessing', this.createRequestLoggingHook());\n    this.registerHook('request_preprocessing', this.createRequestValidationHook());\n\n    // 响应后处理Hooks\n    this.registerHook('response_postprocessing', this.createResponseLoggingHook());\n    this.registerHook('response_postprocessing', this.createResponseMetricsHook());\n\n    // 错误处理Hooks\n    this.registerHook('error_handling', this.createErrorLoggingHook());\n    this.registerHook('error_handling', this.createErrorMetricsHook());\n  }\n\n  /**\n   * 注册Hook\n   */\n  registerHook(stage: string, hook: Function): void {\n    if (!this.enabled) {\n      console.warn(`[ServerHookManager] Hooks are disabled, ignoring registration for stage: ${stage}`);\n      return;\n    }\n\n    if (!this.hooks.has(stage)) {\n      this.hooks.set(stage, []);\n    }\n\n    this.hooks.get(stage)!.push(hook);\n    console.log(`[ServerHookManager] Registered hook for stage: ${stage}`);\n  }\n\n  /**\n   * 执行Hooks\n   */\n  async executeHooks(\n    stage: string,\n    data: UnknownObject,\n    context: RequestContextV2\n  ): Promise<HookExecutionResult> {\n    if (!this.enabled) {\n      return {\n        success: true,\n        data,\n        executionTime: 0,\n        observations: ['Hooks are disabled']\n      };\n    }\n\n    const startTime = Date.now();\n    const hooks = this.hooks.get(stage) || [];\n\n    if (hooks.length === 0) {\n      return {\n        success: true,\n        data,\n        executionTime: Date.now() - startTime,\n        observations: ['No hooks registered for stage: ' + stage]\n      };\n    }\n\n    console.log(`[ServerHookManager] Executing ${hooks.length} hooks for stage: ${stage}, request: ${context.requestId}`);\n\n    try {\n      let currentData = data;\n      const observations: string[] = [];\n\n      // 按顺序执行所有hooks\n      for (let i = 0; i < hooks.length; i++) {\n        const hook = hooks[i];\n        const hookName = `${stage}_hook_${i + 1}`;\n\n        try {\n          const hookResult = await hook(currentData, context);\n\n          if (hookResult && typeof hookResult === 'object') {\n            currentData = hookResult;\n            observations.push(`Hook ${hookName} executed successfully`);\n          } else {\n            observations.push(`Hook ${hookName} returned no data`);\n          }\n\n          // 更新统计\n          this.updateStats(stage, Date.now() - startTime, false);\n\n        } catch (hookError) {\n          const error = hookError as Error;\n          const errorMessage = `Hook ${hookName} failed: ${error.message}`;\n          console.error(`[ServerHookManager] ${errorMessage}`);\n          observations.push(errorMessage);\n\n          // 更新错误统计\n          this.updateStats(stage, Date.now() - startTime, true);\n\n          // 根据配置决定是否继续执行\n          if (this.shouldStopOnHookError(stage, error)) {\n            throw new Error(`Hook execution stopped at ${hookName}: ${error.message}`);\n          }\n        }\n      }\n\n      const executionTime = Date.now() - startTime;\n\n      return {\n        success: true,\n        data: currentData,\n        executionTime,\n        observations\n      };\n\n    } catch (error) {\n      const executionTime = Date.now() - startTime;\n      const err = error as Error;\n      this.updateStats(stage, executionTime, true);\n\n      return {\n        success: false,\n        error: err.message,\n        executionTime,\n        observations: [`Hook chain failed: ${err.message}`]\n      };\n    }\n  }\n\n  /**\n   * 创建请求日志Hook\n   */\n  private createRequestLoggingHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      const logData = {\n        requestId: context.requestId,\n        method: context.method,\n        url: context.url,\n        endpoint: context.endpoint,\n        timestamp: context.timestamp,\n        userAgent: context.userAgent,\n        ip: context.ip\n      };\n\n      console.log(`[Hook:RequestLogging] ${JSON.stringify(logData, null, 2)}`);\n      return data;\n    };\n  }\n\n  /**\n   * 创建请求验证Hook\n   */\n  private createRequestValidationHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      // 基础验证\n      if (!data || typeof data !== 'object') {\n        throw new Error('Request data must be an object');\n      }\n\n      const body = data as any;\n\n      // 检查必要字段\n      if (!body.model) {\n        console.warn(`[Hook:RequestValidation] Missing model field for request ${context.requestId}`);\n      }\n\n      if (!body.messages || !Array.isArray(body.messages)) {\n        throw new Error('Messages field must be an array');\n      }\n\n      console.log(`[Hook:RequestValidation] Request validation passed for ${context.requestId}`);\n      return data;\n    };\n  }\n\n  /**\n   * 创建响应日志Hook\n   */\n  private createResponseLoggingHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      const logData = {\n        requestId: context.requestId,\n        hasResponse: !!data,\n        responseModel: (data as any)?.model,\n        responseChoices: (data as any)?.choices?.length || 0,\n        timestamp: Date.now()\n      };\n\n      console.log(`[Hook:ResponseLogging] ${JSON.stringify(logData, null, 2)}`);\n      return data;\n    };\n  }\n\n  /**\n   * 创建响应指标Hook\n   */\n  private createResponseMetricsHook(): Function {\n    return async (data: UnknownObject, context: RequestContextV2): Promise<UnknownObject> => {\n      const response = data as any;\n\n      // 添加响应指标\n      if (response && typeof response === 'object') {\n        response._metrics = {\n          serverVersion: 'v2',\n          processedAt: Date.now(),\n          requestId: context.requestId,\n          hookProcessed: true\n        };\n      }\n\n      return data;\n    };\n  }\n\n  /**\n   * 创建错误日志Hook\n   */\n  private createErrorLoggingHook(): Function {\n    return async (error: Error, context: RequestContextV2): Promise<void> => {\n      const errorData = {\n        requestId: context.requestId,\n        error: {\n          message: error.message,\n          name: error.name,\n          stack: error.stack\n        },\n        context: {\n          method: context.method,\n          url: context.url,\n          endpoint: context.endpoint,\n          timestamp: context.timestamp\n        }\n      };\n\n      console.error(`[Hook:ErrorLogging] ${JSON.stringify(errorData, null, 2)}`);\n    };\n  }\n\n  /**\n   * 创建错误指标Hook\n   */\n  private createErrorMetricsHook(): Function {\n    return async (error: Error, context: RequestContextV2): Promise<void> => {\n      // 这里可以发送错误指标到监控系统\n      console.log(`[Hook:ErrorMetrics] Error recorded for request ${context.requestId}: ${error.message}`);\n    };\n  }\n\n  /**\n   * 更新Hook执行统计\n   */\n  private updateStats(stage: string, executionTime: number, isError: boolean): void {\n    if (!this.executionStats.has(stage)) {\n      this.executionStats.set(stage, { count: 0, totalTime: 0, errors: 0 });\n    }\n\n    const stats = this.executionStats.get(stage)!;\n    stats.count++;\n    stats.totalTime += executionTime;\n\n    if (isError) {\n      stats.errors++;\n    }\n  }\n\n  /**\n   * 判断是否应该在Hook错误时停止执行\n   */\n  private shouldStopOnHookError(stage: string, error: Error): boolean {\n    // 对于关键阶段，错误时停止执行\n    const criticalStages = ['request_validation', 'authentication'];\n    return criticalStages.includes(stage);\n  }\n\n  /**\n   * 获取Hook执行统计\n   */\n  getExecutionStats(): UnknownObject {\n    const stats: UnknownObject = {};\n\n    for (const [stage, data] of this.executionStats.entries()) {\n      stats[stage] = {\n        executions: data.count,\n        totalTime: data.totalTime,\n        averageTime: data.count > 0 ? data.totalTime / data.count : 0,\n        errors: data.errors,\n        errorRate: data.count > 0 ? data.errors / data.count : 0\n      };\n    }\n\n    return stats;\n  }\n\n  /**\n   * 清理Hook\n   */\n  clearHooks(stage?: string): void {\n    if (stage) {\n      this.hooks.delete(stage);\n      console.log(`[ServerHookManager] Cleared hooks for stage: ${stage}`);\n    } else {\n      this.hooks.clear();\n      console.log('[ServerHookManager] Cleared all hooks');\n    }\n  }\n\n  /**\n   * 获取已注册的Hook阶段\n   */\n  getRegisteredStages(): string[] {\n    return Array.from(this.hooks.keys());\n  }\n\n  /**\n   * 启用/禁用Hook管理器\n   */\n  setEnabled(enabled: boolean): void {\n    this.enabled = enabled;\n    console.log(`[ServerHookManager] ${enabled ? 'Enabled' : 'Disabled'}`);\n  }\n\n  /**\n   * 检查是否启用\n   */\n  isEnabled(): boolean {\n    return this.enabled;\n  }\n}"
          },
          "src/server-v2/hooks/server-v2-hook-integration.ts": {
            "path": "src/server-v2/hooks/server-v2-hook-integration.ts",
            "size": 12460,
            "lines": 448,
            "imports": [
              "../../types/common-types.js",
              "../utils/server-v2-snapshot-writer.js"
            ],
            "exports": [
              "ServerV2HookContext",
              "ServerV2HookIntegrationConfig",
              "ServerV2HookIntegration"
            ],
            "classes": [
              "ServerV2HookIntegration"
            ],
            "functions": [],
            "content": "/**\n * Server V2 Hook Integration\n *\n * 按照现有的provider和llmswitch-core规范实现\n * 集成系统hooks模块和snapshot服务\n */\n\nimport type { UnknownObject } from '../../types/common-types.js';\nimport type {\n  IBidirectionalHook,\n  UnifiedHookStage,\n  HookExecutionContext,\n  HookExecutionResult,\n  HookResult\n} from '../../modules/hooks/types/hook-types.js';\nimport { writeServerV2Snapshot, type ServerV2SnapshotPhase } from '../utils/server-v2-snapshot-writer.js';\n\n/**\n * Server V2 Hook阶段映射\n */\nconst SERVER_V2_HOOK_STAGE_MAP: Record<string, UnifiedHookStage> = {\n  'server-entry': 'PIPELINE_PREPROCESSING',\n  'server-pre-process': 'REQUEST_PREPROCESSING',\n  'server-post-process': 'RESPONSE_POSTPROCESSING',\n  'server-response': 'RESPONSE_VALIDATION',\n  'server-error': 'ERROR_HANDLING',\n  'server-final': 'FINALIZATION'\n} as const;\n\n/**\n * Server V2 Hook执行上下文\n */\nexport interface ServerV2HookContext extends HookExecutionContext {\n  serverVersion: 'v2';\n  endpoint: string;\n  requestId: string;\n  originalRequest?: UnknownObject;\n  metadata?: {\n    [key: string]: unknown;\n  };\n}\n\n/**\n * Server V2 Hook集成配置\n */\nexport interface ServerV2HookIntegrationConfig {\n  enabled: boolean;\n  snapshot: {\n    enabled: boolean;\n    level: 'verbose' | 'normal' | 'silent';\n    phases: ServerV2SnapshotPhase[];\n  };\n  hooks: {\n    enabled: boolean;\n    timeout: number;\n    parallel: boolean;\n    retryAttempts: number;\n  };\n}\n\n/**\n * 默认配置\n */\nconst DEFAULT_CONFIG: ServerV2HookIntegrationConfig = {\n  enabled: true,\n  snapshot: {\n    enabled: true,\n    level: 'normal',\n    phases: ['server-entry', 'server-pre-process', 'server-post-process', 'server-response', 'server-final']\n  },\n  hooks: {\n    enabled: true,\n    timeout: 5000,\n    parallel: false,\n    retryAttempts: 2\n  }\n};\n\n/**\n * Server V2 Hook集成管理器\n */\nexport class ServerV2HookIntegration {\n  private config: ServerV2HookIntegrationConfig;\n  private hooks: Map<string, IBidirectionalHook> = new Map();\n  private hookStats: Map<string, { executions: number; errors: number; totalTime: number }> = new Map();\n\n  constructor(config: Partial<ServerV2HookIntegrationConfig> = {}) {\n    this.config = { ...DEFAULT_CONFIG, ...config };\n    console.log('[ServerV2HookIntegration] Initialized with config:', {\n      enabled: this.config.enabled,\n      snapshotEnabled: this.config.snapshot.enabled,\n      hooksEnabled: this.config.hooks.enabled,\n      level: this.config.snapshot.level\n    });\n  }\n\n  /**\n   * 注册Hook\n   */\n  registerHook(hook: IBidirectionalHook): void {\n    if (!this.config.hooks.enabled) {\n      console.warn('[ServerV2HookIntegration] Hooks are disabled, skipping registration');\n      return;\n    }\n\n    const hookKey = `${hook.stage}_${hook.name}`;\n    this.hooks.set(hookKey, hook);\n    this.hookStats.set(hookKey, { executions: 0, errors: 0, totalTime: 0 });\n\n    console.log(`[ServerV2HookIntegration] Registered hook: ${hook.name} at stage ${hook.stage}`);\n  }\n\n  /**\n   * 注销Hook\n   */\n  unregisterHook(stage: UnifiedHookStage, name: string): void {\n    const hookKey = `${stage}_${name}`;\n    const removed = this.hooks.delete(hookKey);\n    this.hookStats.delete(hookKey);\n\n    if (removed) {\n      console.log(`[ServerV2HookIntegration] Unregistered hook: ${name} at stage ${stage}`);\n    }\n  }\n\n  /**\n   * 执行指定阶段的Hook\n   */\n  async executeHooks(\n    stage: UnifiedHookStage,\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<{\n    data: UnknownObject;\n    results: HookExecutionResult[];\n    executionTime: number;\n  }> {\n    if (!this.config.enabled || !this.config.hooks.enabled) {\n      return {\n        data,\n        results: [],\n        executionTime: 0\n      };\n    }\n\n    const startTime = Date.now();\n    const results: HookExecutionResult[] = [];\n    let currentData = data;\n\n    try {\n      // 查找匹配的Hook\n      const stageHooks = Array.from(this.hooks.entries())\n        .filter(([_, hook]) => hook.stage === stage)\n        .sort(([_, a], [__, b]) => a.priority - b.priority);\n\n      console.log(`[ServerV2HookIntegration] Executing ${stageHooks.length} hooks for stage: ${stage}`);\n\n      // 执行Hook\n      for (const [hookKey, hook] of stageHooks) {\n        const hookStartTime = Date.now();\n        const stats = this.hookStats.get(hookKey)!;\n\n        try {\n          stats.executions++;\n\n          // 执行Hook\n          const result = await this.executeSingleHook(hook, currentData, context);\n\n          if (result.success && result.data !== undefined) {\n            currentData = result.data;\n          }\n\n          const hookExecutionTime = Date.now() - hookStartTime;\n          stats.totalTime += hookExecutionTime;\n\n          results.push({\n            hookName: hook.name,\n            stage: hook.stage,\n            success: true,\n            executionTime: hookExecutionTime,\n            input: data,\n            output: result.data,\n            metadata: result.metadata\n          });\n\n          console.log(`[ServerV2HookIntegration] Hook ${hook.name} completed in ${hookExecutionTime}ms`);\n\n        } catch (error) {\n          stats.errors++;\n          const hookExecutionTime = Date.now() - hookStartTime;\n          stats.totalTime += hookExecutionTime;\n\n          const errorResult = {\n            hookName: hook.name,\n            stage: hook.stage,\n            success: false,\n            executionTime: hookExecutionTime,\n            error: (error as Error).message,\n            input: currentData,\n            output: null\n          };\n\n          results.push(errorResult);\n\n          console.error(`[ServerV2HookIntegration] Hook ${hook.name} failed:`, error);\n\n          // 根据配置决定是否继续执行\n          if (this.shouldStopOnHookError(hook, error as Error)) {\n            throw new Error(`Hook execution stopped at ${hook.name}: ${(error as Error).message}`);\n          }\n        }\n      }\n\n      const totalExecutionTime = Date.now() - startTime;\n\n      return {\n        data: currentData,\n        results,\n        executionTime: totalExecutionTime\n      };\n\n    } catch (error) {\n      const totalExecutionTime = Date.now() - startTime;\n\n      return {\n        data: currentData,\n        results,\n        executionTime: totalExecutionTime\n      };\n    }\n  }\n\n  /**\n   * 执行单个Hook\n   */\n  private async executeSingleHook(\n    hook: IBidirectionalHook,\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<HookResult> {\n    const timeoutPromise = new Promise<never>((_, reject) => {\n      setTimeout(() => reject(new Error(`Hook ${hook.name} timed out after ${this.config.hooks.timeout}ms`)), this.config.hooks.timeout);\n    });\n\n    const hookPromise = hook.execute(context, { data, metadata: {} });\n\n    return Promise.race([hookPromise, timeoutPromise]);\n  }\n\n  /**\n   * 执行Hook并记录快照\n   */\n  async executeHooksWithSnapshot(\n    phase: ServerV2SnapshotPhase,\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<{\n    data: UnknownObject;\n    results: HookExecutionResult[];\n    executionTime: number;\n  }> {\n    const stage = SERVER_V2_HOOK_STAGE_MAP[phase];\n\n    // 执行Hook\n    const hookResult = await this.executeHooks(stage, data, context);\n\n    // 记录快照\n    if (this.config.snapshot.enabled && this.config.snapshot.phases.includes(phase)) {\n      await this.writeSnapshot(phase, data, hookResult, context);\n    }\n\n    return hookResult;\n  }\n\n  /**\n   * 执行错误Hook并记录快照\n   */\n  async executeErrorHooksWithSnapshot(\n    data: UnknownObject,\n    context: ServerV2HookContext\n  ): Promise<void> {\n    const stage = 'ERROR_HANDLING' as UnifiedHookStage;\n\n    try {\n      // 执行错误处理Hook\n      const hookResult = await this.executeHooks(stage, data, context);\n\n      // 记录错误快照\n      if (this.config.snapshot.enabled) {\n        await this.writeErrorSnapshot(data, hookResult, context);\n      }\n    } catch (error) {\n      console.error('[ServerV2HookIntegration] Error in error hooks:', error);\n    }\n  }\n\n  /**\n   * 写入快照\n   */\n  private async writeSnapshot(\n    phase: ServerV2SnapshotPhase,\n    data: UnknownObject,\n    hookResult: { data: UnknownObject; results: HookExecutionResult[]; executionTime: number },\n    context: ServerV2HookContext\n  ): Promise<void> {\n    try {\n      const snapshotData = {\n        originalData: data,\n        processedData: hookResult.data,\n        hookResults: hookResult.results,\n        hookExecutionTime: hookResult.executionTime,\n        context: {\n          requestId: context.requestId,\n          endpoint: context.endpoint,\n          serverVersion: context.serverVersion\n        }\n      };\n\n      await writeServerV2Snapshot({\n        phase,\n        requestId: context.requestId,\n        data: snapshotData,\n        entryEndpoint: context.endpoint,\n        metadata: {\n          level: this.config.snapshot.level,\n          hookCount: hookResult.results.length,\n          successCount: hookResult.results.filter(r => r.success).length,\n          errorCount: hookResult.results.filter(r => !r.success).length\n        }\n      });\n\n    } catch (error) {\n      console.error(`[ServerV2HookIntegration] Failed to write snapshot for phase ${phase}:`, error);\n    }\n  }\n\n  /**\n   * 写入错误快照\n   */\n  private async writeErrorSnapshot(\n    data: UnknownObject,\n    hookResult: { data: UnknownObject; results: HookExecutionResult[]; executionTime: number },\n    context: ServerV2HookContext\n  ): Promise<void> {\n    try {\n      const snapshotData = {\n        originalData: data,\n        processedData: hookResult.data,\n        hookResults: hookResult.results,\n        hookExecutionTime: hookResult.executionTime,\n        context: {\n          requestId: context.requestId,\n          endpoint: context.endpoint,\n          serverVersion: context.serverVersion\n        }\n      };\n\n      await writeServerV2Snapshot({\n        phase: 'server-entry' as ServerV2SnapshotPhase, // 使用一个有效的phase\n        requestId: context.requestId,\n        data: snapshotData,\n        entryEndpoint: context.endpoint,\n        metadata: {\n          level: this.config.snapshot.level,\n          hookCount: hookResult.results.length,\n          successCount: hookResult.results.filter(r => r.success).length,\n          errorCount: hookResult.results.filter(r => !r.success).length,\n          isErrorSnapshot: true\n        }\n      });\n\n    } catch (error) {\n      console.error('[ServerV2HookIntegration] Failed to write error snapshot:', error);\n    }\n  }\n\n  /**\n   * 判断是否应该在Hook错误时停止执行\n   */\n  private shouldStopOnHookError(hook: IBidirectionalHook, error: Error): boolean {\n    // 对于debug hook，不停止执行\n    if (hook.isDebugHook) {\n      return false;\n    }\n\n    // 对于关键阶段，停止执行\n    const criticalStages: UnifiedHookStage[] = [\n      'REQUEST_VALIDATION',\n      'AUTHENTICATION',\n      'HTTP_REQUEST'\n    ];\n\n    return criticalStages.includes(hook.stage);\n  }\n\n  /**\n   * 获取Hook统计信息\n   */\n  getHookStats(): Record<string, { executions: number; errors: number; avgTime: number; successRate: number }> {\n    const stats: Record<string, any> = {};\n\n    for (const [hookKey, stat] of this.hookStats.entries()) {\n      const successRate = stat.executions > 0 ? ((stat.executions - stat.errors) / stat.executions) * 100 : 0;\n      const avgTime = stat.executions > 0 ? stat.totalTime / stat.executions : 0;\n\n      stats[hookKey] = {\n        executions: stat.executions,\n        errors: stat.errors,\n        avgTime: Math.round(avgTime * 100) / 100,\n        successRate: Math.round(successRate * 100) / 100\n      };\n    }\n\n    return stats;\n  }\n\n  /**\n   * 重置Hook统计信息\n   */\n  resetHookStats(): void {\n    for (const stats of this.hookStats.values()) {\n      stats.executions = 0;\n      stats.errors = 0;\n      stats.totalTime = 0;\n    }\n\n    console.log('[ServerV2HookIntegration] Hook statistics reset');\n  }\n\n  /**\n   * 获取配置信息\n   */\n  getConfig(): ServerV2HookIntegrationConfig {\n    return { ...this.config };\n  }\n\n  /**\n   * 更新配置\n   */\n  updateConfig(newConfig: Partial<ServerV2HookIntegrationConfig>): void {\n    this.config = { ...this.config, ...newConfig };\n    console.log('[ServerV2HookIntegration] Configuration updated:', {\n      enabled: this.config.enabled,\n      snapshotEnabled: this.config.snapshot.enabled,\n      hooksEnabled: this.config.hooks.enabled\n    });\n  }\n}"
          }
        },
        "utils": {
          "src/server-v2/utils/server-v2-snapshot-writer.ts": {
            "path": "src/server-v2/utils/server-v2-snapshot-writer.ts",
            "size": 5542,
            "lines": 201,
            "imports": [
              "fs",
              "path",
              "os",
              "../../types/common-types.js"
            ],
            "exports": [
              "ServerV2SnapshotOptions"
            ],
            "classes": [],
            "functions": [
              "mapEndpointToFolder",
              "maskSensitiveData",
              "generateSnapshotPath",
              "writeServerV2Snapshot",
              "writeBatchServerV2Snapshots",
              "cleanupExpiredSnapshots"
            ],
            "content": "/**\n * Server V2 Snapshot Writer\n *\n * 按照现有provider和llmswitch-core的snapshot规范实现\n * 支持端点映射和敏感数据遮蔽\n */\n\nimport { promises as fs } from 'fs';\nimport { join, dirname } from 'path';\nimport { homedir } from 'os';\nimport type { UnknownObject } from '../../types/common-types.js';\n\n/**\n * Server V2 Snapshot阶段定义\n */\nexport type ServerV2SnapshotPhase =\n  | 'server-entry'           // 请求进入服务器\n  | 'server-pre-process'     // 服务器预处理前\n  | 'server-post-process'    // 服务器处理后\n  | 'server-response'        // 服务器响应前\n  | 'server-error'           // 服务器错误\n  | 'server-final';          // 最终响应\n\n/**\n * Snapshot写入选项\n */\nexport interface ServerV2SnapshotOptions {\n  phase: ServerV2SnapshotPhase;\n  requestId: string;\n  data: UnknownObject;\n  entryEndpoint?: string;\n  error?: Error;\n  metadata?: {\n    [key: string]: unknown;\n  };\n}\n\n/**\n * 根据端点映射到文件夹\n */\nfunction mapEndpointToFolder(entryEndpoint?: string): string {\n  const ep = String(entryEndpoint || '').toLowerCase();\n  if (ep.includes('/v1/responses')) return 'openai-responses';\n  if (ep.includes('/v1/messages') || ep.includes('/anthropic')) return 'anthropic-messages';\n  if (ep.includes('/v2/chat/completions')) return 'server-v2-chat';\n  return 'openai-chat'; // 默认\n}\n\n/**\n * 敏感数据遮蔽\n */\nfunction maskSensitiveData(data: UnknownObject): UnknownObject {\n  if (typeof data !== 'object' || data === null) {\n    return data;\n  }\n\n  const result = { ...data };\n\n  // 遮蔽常见的敏感字段\n  const sensitiveFields = ['authorization', 'x-api-key', 'api-key', 'password', 'token'];\n\n  if (result.headers && typeof result.headers === 'object') {\n    const headers = { ...result.headers } as Record<string, unknown>;\n    for (const [key, value] of Object.entries(headers)) {\n      const lowerKey = key.toLowerCase();\n      if (sensitiveFields.some(field => lowerKey.includes(field))) {\n        const str = String(value ?? '');\n        const masked = str.length > 12 ? `${str.slice(0, 6)}****${str.slice(-6)}` : '****';\n        headers[key] = masked;\n      }\n    }\n    result.headers = headers;\n  }\n\n  return result;\n}\n\n/**\n * 生成快照文件路径\n */\nfunction generateSnapshotPath(\n  phase: ServerV2SnapshotPhase,\n  requestId: string,\n  entryEndpoint?: string\n): string {\n  const baseDir = join(homedir(), '.routecodex', 'codex-samples');\n  const endpointFolder = mapEndpointToFolder(entryEndpoint);\n  const fileName = `${requestId}_server-v2-${phase}.json`;\n\n  return join(baseDir, endpointFolder, fileName);\n}\n\n/**\n * 写入Server V2快照\n */\nexport async function writeServerV2Snapshot(options: ServerV2SnapshotOptions): Promise<void> {\n  try {\n    const { phase, requestId, data, entryEndpoint, error, metadata } = options;\n\n    // 创建快照数据结构\n    const snapshotData = {\n      timestamp: new Date().toISOString(),\n      phase,\n      requestId,\n      endpoint: entryEndpoint || 'unknown',\n      serverVersion: 'v2',\n      data: maskSensitiveData(data),\n      metadata: {\n        ...metadata,\n        phase,\n        writtenAt: Date.now(),\n        ...(error && {\n          error: {\n            name: error.name,\n            message: error.message,\n            stack: error.stack\n          }\n        })\n      }\n    };\n\n    // 错误信息已经在metadata中处理了\n\n    // 生成文件路径\n    const filePath = generateSnapshotPath(phase, requestId, entryEndpoint);\n\n    // 确保目录存在\n    await fs.mkdir(dirname(filePath), { recursive: true });\n\n    // 写入快照文件\n    await fs.writeFile(filePath, JSON.stringify(snapshotData, null, 2), 'utf-8');\n\n    console.log(`[ServerV2Snapshot] Snapshot written: ${filePath}`);\n\n  } catch (writeError) {\n    console.error('[ServerV2Snapshot] Failed to write snapshot:', writeError);\n    // 快照写入失败不应影响主流程\n  }\n}\n\n/**\n * 批量写入快照（用于多个阶段的快照）\n */\nexport async function writeBatchServerV2Snapshots(\n  requestId: string,\n  snapshots: Array<Omit<ServerV2SnapshotOptions, 'requestId'>>\n): Promise<void> {\n  const writePromises = snapshots.map(snapshot =>\n    writeServerV2Snapshot({\n      ...snapshot,\n      requestId\n    })\n  );\n\n  await Promise.allSettled(writePromises);\n}\n\n/**\n * 清理过期快照文件\n */\nexport async function cleanupExpiredSnapshots(\n  maxAge: number = 24 * 60 * 60 * 1000, // 默认24小时\n  baseDir?: string\n): Promise<void> {\n  try {\n    const snapshotDir = baseDir || join(homedir(), '.routecodex', 'codex-samples');\n    const files = await fs.readdir(snapshotDir, { recursive: true });\n\n    const now = Date.now();\n    const expiredFiles: string[] = [];\n\n    for (const file of files) {\n      const filePath = join(snapshotDir, file);\n      try {\n        const stats = await fs.stat(filePath);\n        if (now - stats.mtime.getTime() > maxAge) {\n          expiredFiles.push(filePath);\n        }\n      } catch {\n        // 忽略无法访问的文件\n      }\n    }\n\n    // 删除过期文件\n    for (const expiredFile of expiredFiles) {\n      try {\n        await fs.unlink(expiredFile);\n        console.log(`[ServerV2Snapshot] Cleaned up expired snapshot: ${expiredFile}`);\n      } catch {\n        // 忽略删除失败\n      }\n    }\n\n    if (expiredFiles.length > 0) {\n      console.log(`[ServerV2Snapshot] Cleaned up ${expiredFiles.length} expired snapshot files`);\n    }\n\n  } catch (error) {\n    console.error('[ServerV2Snapshot] Failed to cleanup snapshots:', error);\n  }\n}"
          }
        },
        "middleware": {}
      }
    },
    "comparison": {
      "fileComparison": {
        "v1": {
          "totalFiles": 37,
          "totalLines": 11679,
          "totalSize": 415486
        },
        "v2": {
          "totalFiles": 8,
          "totalLines": 2309,
          "totalSize": 64530
        },
        "improvements": {
          "fileChange": -29,
          "lineChange": -9370,
          "sizeChange": -350956
        }
      },
      "architecturalImprovements": [
        "Modular design with separate core, handlers, hooks, and utils directories",
        "Hook integration system for extensibility",
        "Enhanced error handling and logging",
        "Snapshot mechanism for debugging",
        "Configuration-driven architecture"
      ],
      "newFeatures": [
        "Server V2专用端点 (/v2/chat/completions)",
        "Enhanced response metadata",
        "Performance monitoring hooks",
        "Request/Response snapshot recording",
        "Server factory pattern for unified creation",
        "Version selector for runtime switching"
      ],
      "compatibilityFeatures": [
        "V1 compatible endpoints (/v1/chat/completions, /health, /status)",
        "V1 compatible response formats",
        "Same configuration structure",
        "Backward compatible API signatures",
        "Seamless migration path"
      ]
    },
    "keyFileComparison": {
      "routeCodexServer": {
        "v1": {
          "lines": 769,
          "size": 21616,
          "classes": [
            "RouteCodexServer"
          ],
          "functions": []
        },
        "v2": {
          "lines": 654,
          "size": 18139,
          "classes": [
            "RouteCodexServerV2"
          ],
          "functions": []
        },
        "improvements": {
          "lineReduction": 115,
          "sizeReduction": 3477,
          "complexityReduction": "Improved"
        }
      },
      "chatHandler": {
        "v1": {
          "lines": 399,
          "size": 17359,
          "functions": [
            "currentSys",
            "snapshotDir",
            "writeSnapshot",
            "ctSummary"
          ]
        },
        "v2": {
          "lines": 283,
          "size": 8368,
          "functions": []
        },
        "improvements": {
          "lineReduction": 116,
          "sizeReduction": 8991
        }
      }
    },
    "featureMatrix": {
      "Core Server": {
        "v1": true,
        "v2": true,
        "description": "Main server implementation"
      },
      "Chat Handler": {
        "v1": true,
        "v2": true,
        "description": "Chat completion request handling"
      },
      "Hook System": {
        "v1": false,
        "v2": true,
        "description": "Extensible hook system for customization"
      },
      "Snapshot Recording": {
        "v1": false,
        "v2": true,
        "description": "Request/response snapshot for debugging"
      },
      "Performance Monitoring": {
        "v1": false,
        "v2": true,
        "description": "Built-in performance monitoring"
      },
      "Server Factory": {
        "v1": false,
        "v2": false,
        "description": "Unified server creation interface"
      },
      "Version Selector": {
        "v1": false,
        "v2": false,
        "description": "Runtime version switching capability"
      },
      "V1 Compatible API": {
        "v1": true,
        "v2": true,
        "description": "Backward compatible API endpoints"
      }
    }
  },
  "recommendations": [
    "V2 shows significant architectural improvements with modular design",
    "Hook system provides excellent extensibility for future enhancements",
    "Snapshot mechanism will greatly improve debugging capabilities",
    "Performance monitoring hooks are valuable for optimization",
    "V1 compatibility ensures smooth migration path",
    "Server factory pattern enables unified deployment strategies"
  ]
}